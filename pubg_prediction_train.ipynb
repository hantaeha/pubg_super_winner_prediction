{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Flatten\n",
    "from keras.utils import np_utils\n",
    "from keras.utils.vis_utils import plot_model\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 0\n",
    "np.random.seed(seed)\n",
    "tf.random.set_seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pre = pd.read_csv('data/SuperRuleData220807.csv', header=0)\n",
    "df = df_pre.sample(frac=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>KILLSTANDARDIZED</th>\n",
       "      <th>PLACE STANDARDIZED</th>\n",
       "      <th>RANK</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.298894</td>\n",
       "      <td>1.495066</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.517655</td>\n",
       "      <td>0.815490</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.517655</td>\n",
       "      <td>-0.135915</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.751991</td>\n",
       "      <td>0.543660</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.533230</td>\n",
       "      <td>0.815490</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1035</th>\n",
       "      <td>-0.404703</td>\n",
       "      <td>-0.487114</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1036</th>\n",
       "      <td>-1.032898</td>\n",
       "      <td>-0.487114</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1037</th>\n",
       "      <td>-0.694639</td>\n",
       "      <td>-0.593879</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1038</th>\n",
       "      <td>-1.177866</td>\n",
       "      <td>-1.554762</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1039</th>\n",
       "      <td>-1.129543</td>\n",
       "      <td>-1.554762</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1040 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      KILLSTANDARDIZED  PLACE STANDARDIZED  RANK\n",
       "0             1.298894            1.495066     1\n",
       "1             1.517655            0.815490     2\n",
       "2             1.517655           -0.135915     3\n",
       "3             0.751991            0.543660     4\n",
       "4             0.533230            0.815490     5\n",
       "...                ...                 ...   ...\n",
       "1035         -0.404703           -0.487114    12\n",
       "1036         -1.032898           -0.487114    13\n",
       "1037         -0.694639           -0.593879    14\n",
       "1038         -1.177866           -1.554762    15\n",
       "1039         -1.129543           -1.554762    16\n",
       "\n",
       "[1040 rows x 3 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_pre"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 1040 entries, 631 to 684\n",
      "Data columns (total 3 columns):\n",
      " #   Column              Non-Null Count  Dtype  \n",
      "---  ------              --------------  -----  \n",
      " 0   KILLSTANDARDIZED    1040 non-null   float64\n",
      " 1   PLACE STANDARDIZED  1040 non-null   float64\n",
      " 2   RANK                1040 non-null   int64  \n",
      "dtypes: float64(2), int64(1)\n",
      "memory usage: 32.5 KB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = df.values\n",
    "X = dataset[:,0:2]\n",
    "Y = dataset[:,2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.03263437,  0.73950997],\n",
       "       [-0.32935982, -0.16681996],\n",
       "       [-0.95282066, -0.74535599],\n",
       "       [-1.87195721, -1.59766111],\n",
       "       [-0.03610948, -0.58176254],\n",
       "       [ 1.26457741,  0.79406303],\n",
       "       [ 0.4694232 ,  0.11635251],\n",
       "       [-0.49358403, -0.4871143 ],\n",
       "       [-0.60929845,  0.43605142],\n",
       "       [-1.41080106, -0.5134899 ]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 8.,  8., 13., 16.,  8.,  4.,  4., 11., 12., 15.])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "e = LabelEncoder()\n",
    "e.fit(Y)\n",
    "Y_trans = e.transform(Y)\n",
    "Y_encoded = np_utils.to_categorical(Y_trans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.]],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_encoded[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_valid, Y_train, Y_valid = train_test_split(X, Y_encoded, test_size=0.1, random_state=seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "936"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.66995595,  1.76689592],\n",
       "       [-1.23875296, -0.59387907],\n",
       "       [-0.18246146, -0.4871143 ]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2,)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.]],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_train[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(16,)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_train[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model = Sequential()\n",
    "#model.add(Dense(16, input_dim=2, activation='sigmoid'))\n",
    "#model.add(Dense(32, activation='relu'))\n",
    "#model.add(Dense(16, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metal device set to: Apple M1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-08-08 13:25:02.832009: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:305] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support.\n",
      "2022-08-08 13:25:02.832237: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:271] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 0 MB memory) -> physical PluggableDevice (device: 0, name: METAL, pci bus id: <undefined>)\n"
     ]
    }
   ],
   "source": [
    "model = Sequential([\n",
    "                    Flatten(input_shape=(2,)),\n",
    "                    Dense(16, activation='sigmoid', name=\"layer1\"),\n",
    "                    Dense(32, activation='relu', name=\"layer2\"),\n",
    "                    Dense(16, activation='softmax', name=\"layer3\")\n",
    "                    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " flatten (Flatten)           (None, 2)                 0         \n",
      "                                                                 \n",
      " layer1 (Dense)              (None, 16)                48        \n",
      "                                                                 \n",
      " layer2 (Dense)              (None, 32)                544       \n",
      "                                                                 \n",
      " layer3 (Dense)              (None, 16)                528       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,120\n",
      "Trainable params: 1,120\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAOQAAAHpCAIAAAAd3AewAAAABmJLR0QA/wD/AP+gvaeTAAAgAElEQVR4nO3deVwUV7o38Kc3BJFNREQxEhcQgnEJEhHUeBFQHHM1AYnYgMvVizEILgFzjYMLziTiB3Vi7ug1asOMC6ACicjEZYJjQCSDBpOA2AIJIEZAkKWBXuv947zT02mwRWy6+rTP96+u09VVp7p+FKeqTp3mMAwDCNGAy3YFEOorDCuiBoYVUQPDiqjB15y4ceNGSkoKW1VBSMumTZt8fHzUk785stbW1p49e9bgVTJ9RUVFRUVFbNeCMmfPnq2trdUs4fecKTMz01D1eVmEhoYCfrHPicPhaJVgmxVRA8OKqIFhRdTAsCJqYFgRNTCsiBoYVkQNDCuiBoYVUQPDiqiBYUXUwLAiamBYETUwrIgavXQR7KOGhgaRSHTz5k2JRPLKK69UVVUtWrQoNjZWj5V7ES0tLb6+vgkJCVFRUWzXpU/u3Llz584d9aSTk5O/v/+ArrG4uPjevXvqST6f/9577w3oGl9QP8Pa2dn5zjvvZGRkbN68+Y033vj6668BYP78+c/8oFQqHTRo0NMm9YjP59vb2w8ZMmQgFk7ot/Kvv/66mZnZjBkzWltb09LS3nrrLX0tWYu62t7e3lKp1N/fXy6X5+TkzJs3b4DWqC/9bAZkZ2f/8ssvI0eO5PF4RUVFX375ZR8/uG3bNpVK9bRJPbKysrp+/fq77747EAsn9F75iRMnjh8/nsPhLF++nMfj6XHJmjSrPWvWrFGjRtnb27/99tuDBw8eoDXqSz/Devv2bXNzc/La3Nx89uzZffnUDz/8cPjw4adN0mWAKm9ubs7j8bjcgTqX6FltMzMzMzOzAVqdfj13M+Dhw4f5+fkFBQUSieT06dMAsGzZsp5PIIjF4osXLz558sTb23vBggUAUFBQEB4eLpFIzpw5IxAIRo4cqTlJHvxoa2tLT08vLy8fO3bsihUryD/x+/fvi0SiXbt2VVZWZmRkDB8+fMWKFQKBQHc9u7u7MzMzHR0dAwMDdS+ksrLyq6++iouL+/bbb/Py8lxdXSMiIrhcbnp6ukqlEggEISEhAHD27Fm5XG5hYbF48WKtbSGVHwjGUO2euzInJ6ezsxMAOBwOaeb+9NNPpMEdGBhob2/f636srKwUiUQ7duzIy8srKyvbuHHjM3eiNkZDenq6VklPDQ0NOTk5fn5+o0aNysnJycnJYRimtbUVAJKTk8k8MTExs2bNampqunTpEofD+eSTTxiGuX79+vLlywHgwoULX3/9tdYkwzD37t1btGjR119//f3333t6eo4bN66lpUUkEjk6OgLAl19++c477yxcuBAAtm/frruS5eXlixcvBoBPP/2UYRgdC/nss8+GDBni5OR08uTJSZMmWVhYAMC7777LMExbW5uvr6+1tTVZZn19/aRJk0aMGNFzW3RXhmGYkJCQkJCQZ87GMIyvry+fzyevDVNtV1dXJyenp9Wn111ZXl7u5OREckxmUyqV/v7+hw4dUqlUve7H1NTUESNGAIBIJJo6dSoAFBQU6P4qACA9Pf03JZoTfQkrERYW5ubmpp7UCquNjU1SUhJ57eHhMWPGDPJ6586dAKBSqXqdDAgIyMrKIq/z8vLUOyY+Pp78NZO35s6d6+rq+swaPnjwQB1W3QsJCwuztLT861//yjBMfX09efaX7MsPPvhAvdcZhvmv//ovstd7Vl63/oXVMNXWHdan7cqTJ0+qV8cwjEwm8/LyUigUzNP347Zt20hYGYa5e/fuM7+6nmHt/6UrHXJzc93d3QGguLiYYZiurq5nfuThw4eXL1+eMmXKzZs3AaCjo8PLy4v8r7G0tASA4OBgMqenpyeZRzet6wA6FmJpaWltbU0OOU5OTn/84x/feuuty5cvBwYGarUdB64p+TSsV/tpuzIsLCwxMXHfvn2klZWVlbV48WIej6djP5LD/7JlywDAzc2tH5UZkLD6+vpmZWWdP38+KCjIxcWFHOR0E4vFABAfHz9s2DCtt7S+a0tLS4VC8cwF6t5hWgvRbHNPnz4dALSeWGcL69V+2q7k8XgJCQlr1qwpLi729vY+duxYamoq6NyPPU9snteAHCri4+OPHz9+9OhRoVDYxyuR5IT01q1bmoXt7e0DUb1n1mTQoEGvvPKK4Vf9IvRebXXsnrYrIyMjR40atWfPnoqKCltbW9IkHdD9qP+wlpSUJCcnr1+/Xn1ti/ntELBKpbLnpJubG4/HS0xMlMlkpLyxsZE0jAygu7tb/bqwsFAqlXp7ewOAtbW1VCpVv8UwTK+VZ4u+qs30GKNXpVIdPXpU9640MzPbsmULuSIRHR1NCgd0P/YzrC0tLeSkiujo6AAAiUQCAOTacnZ2tkKhuHLlSmlpaUtLi1gsrq6udnBwAICSkpLr1693d3drTlpYWERHRxcVFc2ZM+fUqVMikWj58uWkfdPc3AwA6taSQqGQy+WaO6NXmlV65kJaW1tramrI67/97W9eXl7kbsKYMWOkUunly5cZhklPTy8sLGxtbW1tbVUqlVrb0r+vsaf29naFQkEqb5hqP3z4sKmpSfP7lEqlGzZscHFx0bEryZxr1qyxt7evrq6eO3cuKbGzs3vafpTL5QDw+PHj/n87mmdbfbka8Pjx45SUFPIfITY2Nj8/v76+ft26dQDg4eGRnZ3NMAy54Ofo6Hj48OGkpCQul7tlyxaGYaqqqhwdHe3s7L744ouekxKJJDIyktTK2tqanFFmZ2e7uLiQdVVVVZ05c+bVV18FgA8//PDRo0dPq2RNTY26Snl5eboXsmrVKktLy7fffvvzzz9fu3atn59fdXU1WY5EIvH09AQAR0fH1NTUtWvX2tnZbdmypampSavyuvXlakBpaekHH3xAGqnLly+/dOnSQFe7qKiInJ8BgLOz8/Tp0729vV9//XUrKysOh1NXV6djV6rFx8enpKRolvS6H8+ePUtOqkJDQ0tLS5/5jTF6vHT1TA0NDTKZjLxubm5Wl8tkss7OzqdNMgzT2NhYUlKiVTigVq1aNXLkSKlUevv27aqqKq13VSrVnTt3JBIJwzD37t3TXfmn6fulq74zQLWJp+1KIjg4uGcho4/92DOsA3I1AADIvxvCzs5O/VogEGjet9CaBIBhw4b1PJE0ADMzsylTpvQs53A4kyZNIq8nTJig+VbPyhueAar9tF0JAIWFhaNHj9YqJAZiPw5UWCnS2dmpbtpShMVqFxcXb9q06bXXXisrK7tw4YLB1ktrWGtra1euXPm0d6OioiIiIp65ELlcfvTo0WvXrrW3t2/fvv2///u/nZ2d9VrNAWEM1RaLxRYWFgcOHLCxsTHYSjmMxsWIjIyMsLAwhoYfG2IYRn1xpCc+nz9wXez6Acdn7QcOh5Oenr506VJ1Ca1HVg6HM0C9tpHRwmewEDUwrIgaGFZEDQwrogaGFVEDw4qogWFF1MCwImpgWBE1MKyIGhhWRA0MK6JGLx1ZBm4wnJcW+f12/GJf0G/COnr0aDJAEnoR5eXlAEDGhiBmzJjBXnVoFRISMnr0aM0SDhW9V+lCumBmZGSwXRFTg21WRA0MK6IGhhVRA8OKqIFhRdTAsCJqYFgRNTCsiBoYVkQNDCuiBoYVUQPDiqiBYUXUwLAiamBYETUwrIgaGFZEDQwrogaGFVEDw4qogWFF1MCwImpgWBE1MKyIGhhWRA0MK6IGhhVRA8OKqIFhRdTAsCJqYFgRNTCsiBoYVkQNHPlaD06ePHns2DGVSkUmKyoqAMDNzY1Mcrnc1atXL1++nLX6mQoMqx6UlpZOmTJFxwzff//95MmTDVYfU4Vh1Y+JEyeSA2pP48ePF4vFBq6PScI2q35EREQIBIKe5QKBYOXKlYavj0nCI6t+VFVVjR8/vtcvUywWjx8/3vBVMj14ZNWPsWPHTp06lcPhaBZyOJw33ngDk6ovGFa9iYyM5PF4miU8Hi8yMpKt+pgebAboTUNDg5OTk/oCFgBwudwHDx6MGDGCxVqZEjyy6s3w4cNnz56tPrjyeLw5c+ZgUvUIw6pPEREROibRC8JmgD61tbUNGzZMLpcDgEAgaGhosLW1ZbtSpgOPrPpkbW29YMECPp/P5/ODg4MxqfqFYdUzoVCoVCqVSiV2BtA7PtsVgBs3btTW1rJdC72Ry+VmZmYMw0ilUlP6FffRo0f7+PiwXAmGbSEhISx/BagPQkJC2E4Kw/6RFQBCQkIyMzPZroUehIaGAsDq1as5HE5QUBDb1dEbsl2sM4qwmph58+axXQXThGHVPz4fv9UBgVcDEDUwrIgaGFZEDQwrogaGFVEDw4qogWFF1MCwImpgWBE1MKyIGhhWRA0MK6IGZV0uGhoaRCLRzZs3JRLJK6+8UlVVtWjRotjYWLbr9RyuX7/+yy+/9PrW66+/7uHhkZOT8+c//5m67TIAmo6snZ2d77zzjlAozMjI+PXXX48ePXr16lXydJ5uUqlUx6SB+fn5jRw5MiIi4uOPPx4yZMiQIUPMzc0fP368e/fuU6dO1dXV1dfX07hdBkBTWLOzs3/55ZeRI0fyeLyioqIvv/yyjx/ctm2b5tgTWpMGxuFw/uM//sPCwsLGxmbx4sWLFy8OCQmJjY3Nz8+XyWQuLi7h4eF9XJRRbZcB0BTW27dvm5ubk9fm5uazZ8/uy6d++OGHw4cPP22SLeoNUXNyclq/fj30uTuscW7XgKKjzfrw4cP8/PyCggKJRHL69GkAWLZsmdYoaAAgFosvXrz45MkTb2/vBQsWAEBBQUF4eLhEIjlz5oxAIBg5cqTmJHlao62tLT09vby8fOzYsStWrBgyZAgA3L9/XyQS7dq1q7KyMiMjY/jw4StWrOh1UEu9KC4u9vLyGjduHACY0nbpFx1HVj6fb2lpyePxuFyupaWlpaVlz3k2bNiwevVqoVA4c+bMhQsXfvrppwDAMMysWbMAwMbGxsbGRmsSAMRisVAoHDNmTFRU1JEjR6ZMmfLkyZPU1FQ/P789e/bk5uZu3br1xo0ba9eu3b179wBtnUKh2L17t1Kp7PVderdL/9h9XpFhmJCQkD4+ORkWFubm5qaebG1tBYDk5GQyaWNjk5SURF57eHjMmDGDvN65cycAqFSqXicDAgKysrLI67y8PADYvn07wzDx8fEAkJOTQ96aO3euq6urHrfFzs7O0tJy9uzZs2fPHjZsGADIZDIT2K4BRUczoC9yc3Pd3d0BoLi4mGGYrq6uZ37k4cOHly9fnjJlys2bNwGgo6PDy8urs7MTAMjBOzg4mMzp6elJ5tGjcePGXbt2DQBkMpmOUbGo266BYzph9fX1zcrKOn/+fFBQkIuLy4MHD575ETLUf3x8PDm2aeJyf9NAsrS0VCgUeqytJjMzs/fff19rjWr0bpfe0dFm7Yv4+Pjjx48fPXpUKBQOGjSoLx8xMzMDgFu3bmkWtre3D0j9dJozZ47WQMRqVG+XfplIWEtKSpKTk9evX6++JMT8dnRErdMXMunm5sbj8RITE2UyGSlvbGw8efKkQaqsXcNe0bhdA4emZkBLSws5+SA6OjoAQCKRAMDgwYMBIDs7e968efn5+aWlpXK5XCwW8/l8BwcHACgpKZHJZNOnT9eajI6O/vzzz+fMmRMTEyOTyU6dOkXGhmlubgaArq4uKysrAFAoFHK5XCqV9vHYpptKpZJIJE+ePOn1XXq3a8CxenrHMH0703z8+HFKSgr5QsnNnvr6+nXr1gGAh4dHdnY2wzARERFcLtfR0fHw4cNJSUlcLnfLli0Mw1RVVTk6OtrZ2X3xxRc9JyUSiXrYf2tra3IGnZ2d7eLiQtZVVVV15syZV199FQA+/PDDR48eveC25OfnL126FAA4HM7mzZvJaZMavdtlAOwPJkyuYOtlrKvGxkZbW1tyibulpcXOzo6Uy+VyhUJhYWHR6yQANDU11dTUuLu7axb2gx63RZOpbtfzoqkZ8EzkXyGh3qMAIBAING/SaE0CwLBhw3qeOBsPU92u52UiJ1joZYBhRdTAsCJqYFgRNTCsiBoYVkQNDCuiBoYVUQPDiqiBYUXUwLAiamBYETUwrIgaGFZEDQwrogaGFVHDKDpf19XVZWRksF0LPairqwMA09gWTXV1dc7OzmzXwjiewWL7O0DPhs9gmSbyPKDpHV9Zh21WRA0MK6IGhhVRA8OKqIFhRdTAsCJqYFgRNTCsiBoYVkQNDCuiBoYVUQPDiqiBYUXUwLAiamBYETUwrIgaGFZEDQwrogaGFVEDw4qogWFF1MCwImpgWBE1MKyIGhhWRA0MK6IGhhVRA8OKqIFhRdTAsCJqYFgRNTCsiBoYVkQNo/hNAdpdu3atqKhIPXn37l0A+PTTT9UlM2bMmDNnDgs1My04TLseXL58OTAwUCAQcLlcACBfKYfDAQCVSiWXyy9duhQQEMByLemHYdUDpVLp6Oj4+PHjXt+1s7NraGjg8/Gf2IvCNqse8Hi85cuXm5mZ9XzLzMwsMjISk6oXGFb9WLZsmUwm61kuk8mWLVtm+PqYJGwG6M2YMWNqamq0Cp2dnWtqakj7Fb0gPLLqjVAoFAgEmiVmZmZRUVGYVH3BI6velJeXe3h4aBX+8MMPnp6erNTH9GBY9cnDw6O8vFw96e7uXlZWxmJ9TAw2A/QpMjJS3RIQCARRUVHs1sfE4JFVn2pqalxcXNQ3BaqqqlxcXNiulOnAI6s+vfLKK15eXlwul8PhTJ8+HZOqXxhWPYuMjORyuTweLyIigu26mBpsBuhZY2Ojk5MTADx48MDR0ZHt6pgWhg0hISFsbzfqv5CQEFZiw9o96xkzZmzcuJGttetLWFhYXFycj4+PZuG1a9c4HM7s2bPZqtWA2r9/P1urZi2szs7OS5cuZWvt+hIWFubj46O1IfPnzwcAa2trlio1sDIzM9laNfYG0j9TjSnr8GoAogaGFVEDw4qogWFF1MATLHYUFBRUV1erJ/l8vo2NzdChQydNmjR48GAWK2bM8MjKjpkzZzo4OERGRsbExIjF4u7u7tu3b3/yySf29vbBwcHkYW6kBY+s7OBwOEFBQXZ2dg4ODjt37lSXX716VSgUTp06NT8//80332SxhkYIj6xs6vlArL+//7Fjx7q7u999912pVMpKrYwWHlmNTnBwsL+//9WrVzMzM4VCIQC0tbWlp6eXl5ePHTt2xYoVQ4YMAYD79++LRKJdu3ZVVlZmZGQMHz58xYoV6q7f3377bV5e3ujRo7lc7tq1a0lhr8uhCB5ZjRHpbPDNN98AgFgsFgqFY8aMiYqKOnLkyJQpU548eZKamurn57dnz57c3NytW7feuHFj7dq1u3fvJh9PSEioqanZtm2bra3t5s2bSWGvy2FrA/uJle4zISEhbPXc0S8ASE9P7/fHR4wY4ebm1rM8LS0NAAICAhiGCQgIyMrKIuV5eXkAsH37doZh4uPjASAnJ4e8NXfuXFdXV4ZhZDKZvb19RUUFKY+NjSUvnrac58XivsNmgDGSSCQA4ODg8PDhw8uXL0+ZMuXmzZsA0NHR4eXl1dnZCQCWlpYAEBwcTD7i6elJ5hEIBFZWVvPmzTty5MiCBQu2bdsGADqWQxEMqzGqqKgAAA8PD7FYDADx8fHDhg3TmocMAqdmaWmpUCjI60OHDkVERAQHB/v4+IhEIgcHBx3LoQi2WY2OTCa7cOECn89fsmQJuVxw69YtzRna29t1L2HhwoX379+Pi4srKSnx8vIqLy/v33KMDYbV6CQnJ5OoeXh4uLm58Xi8xMRE9UBajY2NJ0+e1PFxiURy9OjRoUOH7t+/Pz8/v6Oj4/Tp0/1YjhHCsLJGLpc3NjZqlkil0o0bN+7cufOjjz5KSkoCADs7u+jo6KKiojlz5pw6dUokEi1fvpyM9Nbc3AwAXV1d5LMKhUIul0ulUpVKlZiY2N3dDQA+Pj4TJkxwcHDQsRyKYJuVHdevXz948KBSqaysrJw2bZqLiwuXy5VKpc7OzkVFRdOmTVPPuXfv3vb29rS0tKKiImtr69TUVBsbm5ycnKysLADYvn17bGxscXHxuXPnGIbZvn17dHT0kydPvL2916xZ09zc7OvrGx0d/bTlsLb9/cLO062hoaHA6gMS+sLhcNLT0w3wfE5TU1NNTY27u7uFhYXuORmG6erqUiqVYrHY1dVV68p/35fzNCzuOzyy0mHYsGF9PJHncDik35bm4bkfyzFCxttmVSgU586dmzdv3sGDBw253kuXLuXm5hpyjaiPjDesdXV19fX1V69elcvlhlnjlStXgoKCgoKC/vnPfxpmjei5GG9YXVxcwsPDDblGPz+/I0eOGHKN6LkYb1gBwMC/G2Fubj5q1ChDrhE9F6M+weo5wLlYLL548SK5NLNgwQIAyMnJIfe4ORzOe++9BwA//fTTnTt3ACAwMNDe3r7XfnGVlZUikWjHjh15eXllZWUbN24knet4PJ6BtxH1nVEfWbVs2LBh9erVQqFw5syZCxcuJD/h5+bmtnnz5vDwcC8vLzKbu7v7sWPHmpubhw4d2mu/uLS0ND8/v6SkpL/+9a+///3vExISvvvuO/JZ8ueBvwJgnGgKa1paWlBQkL29fUBAgLu7e3Z2NgBMnDhx3759AFBVVUVmUyqVra2t0dHRHA5n/fr1q1atCgwMnDx5cnJycmVlZUpKSmRk5OrVq8nMt27dunv3rtZgVcg4GXUzQEtubq67uzsAFBcXk0vfpDwsLCwxMXHfvn2BgYEAkJWVtXjxYh6Pp6NfHLkkTu43urm5vUitbty48WKbRZm6ujpnZ2dWVk1TWH19fbOyss6fPx8UFOTi4vLgwQNSzuPxEhIS1qxZU1xc7O3tfezYsdTUVADQ0S9Oj//oDxw4cODAAX0tjQpsjVhKUzMgPj7++PHjR48eFQqFgwYN0nwrMjJy1KhRe/bsqaiosLW1HTFiBPzrcbyB7hf3Ik8K0IjFsXWpCWtJSUlycvL69evNzc1JCaPRq8HMzGzLli1fffVVXFwc6bcBAP3oF0eWyeBo4EbJqMPa0dEB/3rGg9zvzs7OVigUV65cKS0tbWlpEYvF6nFN1qxZY29vX11dPXfuXFKio18cuSvW83esSazJGpHRYetfyTMfOquvr1+3bh0AeHh4ZGdnMwwTERHB5XIdHR0PHz6clJTE5XK3bNmi+ZH4+PiUlBTNEolEEhkZSbbU2tqaPDF39uxZclIVGhpaWlqqnrmwsPD9998HgPHjx3/++edyufyZGwIvZTOArQcGjTesvWpoaJDJZOR1c3Oz1rvBwcE9CxmGaWxsLCkp6ezs7McadcOwGhJNVwMAwMHBQf3azs5O863CwsLRo0drFRJU94tDapSFtafi4uJNmza99tprZWVlFy5cYLs6aAAZ9QlWH4nF4qqqqgMHDlD0nEZLS4uHhwe5Hsz6Gn/55Zdt27YZ/+8hUn9k9fb2fvToEdu1eG58Pt/e3t6Qo03pWGNVVVV+fn5dXZ3BKtM/1IeVUlZWVtevXzeSNc6dO9fX15fclDZmptAMQC9OPfygMcMjqyH0HICyu7s7MzPT0dGRdL4BgM7Ozr/85S8NDQ3u7u7+/v7W1tbk17V//vnnEydObN269dGjRyKRyNHRcdmyZba2tpWVlZmZmWZmZitXrtS8BiKVSq9du3bt2rWRI0fOnz9/3LhxpLznGuVyeVZW1u3bt9966y2VSmXYr6Q/8Mg64HoOQHn37t1ly5ZFRkZ+//33ZJ6mpqbXX3/dwsJi69at33333dChQ8ePHz979uwzZ87MmjVr165dubm5v//97+/fvx8TE7N69ep//OMfH3/88Y8//piQkEDGcCW6u7vnz5/f0tLy4YcfMgwzderU8+fP97rG1tbWBQsWlJeXf/jhh48ePUpJSTH4F/P8WLm6+/IMefm0AShJl7FPP/2UTMbFxVlbW5N7ZrW1tQDwP//zP+QtMoi7emhLco8tLS2NTH788ccA0NraSibDw8NXrlypXntISIiFhUVtbW3PNb7//vuLFy9Wz/m73/2Ox+P1ZZNZ3Hd4ZB1Y6gEoyZCoZABKANA6KxeLxeSfPgA4OzuPHz/+22+/1ZxT/bPFkydPBgA/Pz8yOXHiRAAgQezs7MzMzJw6dap6sevWrevq6jpx4oTWGhsaGo4ePapuDwDA66+/rs/NHhgY1gF36NChjo6O4ODgmTNntrS0kEKtASv9/PyePHlSXFwMAFKptL6+Xj1EBZlT3QFXq28k6QZJ+uUUFhbK5XLNpywnTJgAAPfu3dNaY2lpqVwuJx0pCSqe5MGwDrieA1D2nGfjxo0hISHx8fFXrlzZsmXLzJkzd+3a9bwrUiqVAFBYWKguITeZXV1dteYknXofPnz4vKtgF4Z1YPU6AGXP2TgczsiRI/fv369SqdavX3/58mUrK6vnXdfUqVMHDRpUUFCgLiGjFM6aNUtrTtJ4IC0TNeO/IIBhHVi9DkAJv+2qCwB79+69du1abW2tQCBobW0tKytTD2OtNSf5j6850qX63eHDh8fExFRXV5NfzgCA7Ozs0NDQOXPmaC3Hw8Nj/vz5Fy5cEIlEACCTyb7//nuGYWpra9XrNUasnNa9PFcD2traLCwsJk2a9Kc//WnHjh0rV66UyWQ1NTXqrrp5eXkMw3z11VfqJyCIV1555dKlS5cuXfL09ASA6OjoioqKixcvTp8+HQCEQuGdO3f+/ve/kzOtkJCQsrIyhmGUSuWmTZscHBwSEhKioqKWLl3a1dXFMEzPNf7666/kiOvq6vr2228LhcIhQ4Z88MEHdXV1ujeZxX2HQ16+kGcOecnoHIBS7ezZswqFIiAg4PHjxxKJpLW19ccffzx37pz6GPlcurq67t696+7urvUH0FNlZaVSqZwwYcLPP/9sb29vbW39zIXjkJcmS/cAlMT9+/fXr1//4MED0teEFHp4eJCLA/1gYWGheX35jcsAABuiSURBVAFLB/X9rVdffbV/6zIkDCv7amtrGxoaIiIi1q1bR0JTUVHxxRdf7Nmzh+2qGRcMK/vmzp379ddfX7hwITo6urq6esKECUFBQf/3f//Xl3/KLxUMq1EIDAwk95MYhqHi+jwr8NKVccGk6oBhRdTAsCJqYFgRNVg7wSoqKiKXl2m3f/9+E7i70XdFRUUzZsxgZdXshNVkBu/tdUi9H374AQAmTZpk8OoYwowZM9jafezcbjVt5O5rRkYG2xUxNdhmRdTAsCJqYFgRNTCsiBoYVkQNDCuiBoYVUQPDiqiBYUXUwLAiamBYETUwrIgaGFZEDQwrogaGFVEDw4qogWFF1MCwImpgWBE1MKyIGhhWRA0MK6IGhhVRA8OKqIFhRdTAsCJqYFgRNTCsiBoYVkQNDCuiBoYVUQPDiqiBYUXUwJGv9SAtLS0lJUWpVJLJpqYmABg2bBiZ5PF4mzZtioyMZK1+pgLDqgf37t1zc3PTMUNFRYWrq6vB6mOqsBmgB66urpMnT+71xwE5HM7kyZMxqXqBYdWPyMhIHo/Xs5zP50dFRRm+PiYJmwH6UV9fP3r0aJVKpVXO4XBqa2tHjRrFSq1MDB5Z9WPkyJEzZ87kcn/zfXK5XF9fX0yqvmBY9SYiIkKrhMPh4EUAPcJmgN60tLQ4OjrK5XJ1CZ/P//XXX+3t7VmslSnBI6ve2NnZBQQEqE+zeDxeUFAQJlWPMKz6JBQK1edYDMMIhUJ262NisBmgT52dnfb29t3d3QBgbm7e1NRkaWnJdqVMBx5Z9Wnw4MFLliwRCAQCgWDJkiWYVP3CsOpZeHi4XC6Xy+Xh4eFs18XU8NmuwL/V1dUVFhayXYsXpVQqBw8ezDBMW1ubCfyK+8yZM52dndmuxb8wRiM9PZ3tLwNpS09PZzsX/2ZER1aCof+E76233uJwON988w3bFXlRvXbNYZHRhdUEqHuyIv3CsOqfsR2QTAZeDUDUwLAiamBYETUwrIgaeILFjoKCgurqavUkn8+3sbEZOnTopEmTBg8ezGLFjBkeWdkxc+ZMBweHyMjImJgYsVjc3d19+/btTz75xN7ePjg4+O7du2xX0BjhkZUdHA4nKCjIzs7OwcFh586d6vKrV68KhcKpU6fm5+e/+eabLNbQCOGRlU1mZmZaJf7+/seOHevu7n733XelUikrtTJaeGQ1OsHBwf7+/levXs3MzCTdt9va2tLT08vLy8eOHbtixYohQ4YAwP3790Ui0a5duyorKzMyMoYPH75ixQqBQEAW8u233+bl5Y0ePZrL5a5du5YU9rociuCR1Rj5+PgAAOldIBaLhULhmDFjoqKijhw5MmXKlCdPnqSmpvr5+e3Zsyc3N3fr1q03btxYu3bt7t27yccTEhJqamq2bdtma2u7efNmUtjrctjawH5iuyfNv5FeV2zXQg9CQkJCQkL6MueIESPc3Nx6lqelpQFAQEAAwzABAQFZWVmkPC8vDwC2b9/OMEx8fDwA5OTkkLfmzp3r6urKMIxMJrO3t6+oqCDlsbGx5MXTlqMDYK8r9EwSiQQAHBwcHj58ePny5SlTpty8eRMAOjo6vLy8Ojs7AYA8hhAcHEw+4unpSeYRCARWVlbz5s07cuTIggULtm3bBgA6lkMRDKsxqqioAAAPDw+xWAwA8fHxPXtyaQ2oYWlpqVAoyOtDhw5FREQEBwf7+PiIRCIHBwcdy6EItlmNjkwmu3DhAp/PX7JkCblccOvWLc0Z2tvbdS9h4cKF9+/fj4uLKykp8fLyKi8v799yjA2G1egkJyeTqHl4eLi5ufF4vMTERJlMRt5tbGw8efKkjo9LJJKjR48OHTp0//79+fn5HR0dp0+f7sdyjBCGlTVyubyxsVGzRCqVbty4cefOnR999FFSUhIA2NnZRUdHFxUVzZkz59SpUyKRaPny5cuWLQOA5uZmAOjq6iKfVSgUcrlcKpWqVKrExETyOLiPj8+ECRMcHBx0LIci2GZlx/Xr1w8ePKhUKisrK6dNm+bi4sLlcqVSqbOzc1FR0bRp09Rz7t27t729PS0traioyNraOjU11cbGJicnJysrCwC2b98eGxtbXFx87tw5hmG2b98eHR395MkTb2/vNWvWNDc3+/r6RkdHP205rG1//7B9OeLfXsJLV33X2NhYUlLS2dn5zDlVKpVEImlrayspKWlvb+/3chi8dIX6Z9iwYX08kedwOKTflubhuR/LMUKUtVkVCsW5c+fmzZt38OBBw6zxl19++cMf/hAXFycSiTRHCESGR9mRta6urr6+/urVq/PnzzfA6srKyry9ve3s7B49eiSXyz/77LNr165Rd0vdZFB2ZHVxcTHksDzHjh27cuVKbW1tdXV1WFjYrVu39uzZY7C1Iy2UhRUA+HwD/Td48uSJn5/fjBkzAGDUqFGffvoph8MhtysRKyhrBkBvT+WLxeKLFy+S6zULFiwAgJycHHLjm8PhvPfeewDw008/3blzBwACAwPt7e177SxXWVkpEol27NiRl5dXVla2cePGJUuWqNcyZsyY1157bcKECQbbUqSN7csR/9bHS1etra0AkJycTCZjYmJmzZrV1NR06dIlDofzySefMAxTXl7u5OQEAGKxmMymVCr9/f0PHTqkUqnu3bu3aNGir7/++vvvv/f09Bw3blxLS0tqauqIESMAQCQSTZ06FQAKCgo016tUKi0tLcnlTN0G4tIVK8DILl3R1wzQkpaWRkZDDwgIcHd3z87OBoCJEyfu27cPAKqqqshsSqWytbU1Ojqaw+GsX79+1apVgYGBkydPTk5OrqysTElJiYyMXL16NZn51q1bd+/eJZ1K1b788ktPT0/NYy0yMPqaAVpyc3Pd3d0BoLi4mGEY9e3HsLCwxMTEffv2BQYGAkBWVtbixYt5PJ6OznIWFhYAQG5Cav28pVwu/+Mf/5iWltbHoYGKiopCQ0P1uZ3IBMLq6+ublZV1/vz5oKAgFxeXBw8ekHIej5eQkLBmzZri4mJvb+9jx46lpqYCgI7OcjqCGBcXl5iYqPsHWtFAoz6s8fHx5eXlmZmZ5ubm586d03wrMjJyx44de/bs2bt3r62tLWmSqjvLkSMu0d7ebmVl9bRVHDx4cPr06epuzn0xY8aMzMzM594YI2NsI8zR3WYtKSlJTk5ev369ubk5KWE0hnc1MzPbsmXLV199FRcXRzpzAMDzdpY7fvw4h8NZsWKFevn4UD9b6AtrR0cH/OvBD3ITPDs7W6FQXLlypbS0tKWlRSwWqwc7WbNmjb29fXV19dy5c0mJjs5y5G7q48eP1es6fPjwF198YW1tLRKJTpw48dlnn/3ud7/T6teHDIflqxEa+nLpqr6+ft26dQDg4eGRnZ3NMExERASXy3V0dDx8+HBSUhKXy92yZYvmR+Lj41NSUjRLJBKJ+lcqra2tyWN0Z8+eJU3S0NDQ0tJShmFOnDjR8+t69dVXVSqV7kripasBQllYe9XQ0CCTycjr5uZmrXeDg4N7FjLP2VnuuWBYBwj1J1gA4ODgoH5tZ2en+VZhYeHo0aO1CgmqO8u9nEwhrD0VFxdv2rTptddeKysru3DhAtvVGRAKhSInJ+fPf/7zokWLYmNj2a6OIdB3gtVHYrG4qqrqwIED9D280Tfq3pIvTy9b0zyyent7P3r0iO1aDCzSW3LDhg1sV8RwTPbI+jIwWG9JI/FybS0VevZUFAgEvfZp1LzDlJ6erlKpBAJBSEgIAJw9e1Yul1tYWCxevJi1LdE7ti9H/Bs+3cowTK89FXvt08j8trdkW1ubr6+vtbU1WU59ff2kSZNGjBjxIhsCRnbpCpsBxqXXnoq99mnU+qCVlRUJN+Hk5GR6A2djM8DoaPVU7PsAgFpDtWlNmgAMq9HR6utkGgMA6oWp/fGZHtMYAFAvMKzGru99Gq2trTV/M4NhGKVSaaBaGgSG1eho9VTU0adRs7ckAIwZM0YqlV6+fJlhmPT09MLCwtbW1tbWVtOJLNuXI/4NL10xvfVUZJ7Sp7Fnb0mJROLp6QkAjo6Oqampa9eutbOz27JlS1NTU/8qA0Z26cqIwoFh1a0vfRpVKtWdO3ckEgnDMPfu3XvBDpDGFla8GkCNvvRp5HA4kyZNIq9NbzwObLMiamBYETUwrIgaGFZEDQwrogaGFVEDw4qogWFF1DC6mwLGNhhYv5nMhhgPDqMxkhm76urqCgsL2a6FHuzfvx8ANm7cyHZF9GDmzJnOzs5s1+L/M6KwmoylS5cCQEZGBtsVMTXYZkXUwLAiamBYETUwrIgaGFZEDQwrogaGFVEDw4qogWFF1MCwImpgWBE1MKyIGhhWRA0MK6IGhhVRA8OKqIFhRdTAsCJqYFgRNTCsiBoYVkQNDCuiBoYVUQPDiqiBYUXUwLAiamBYETUwrIgaGFZEDQwrogaGFVEDw4qoYXTDtNOoqampra1NPUl+U72qqkpdYm1t/cyfXUXPhCNf68GJEydWrVqlY4bjx4+vXLnSYPUxVRhWPWhtbXVwcJDL5b2+KxAIGhsbbWxsDFwr04NtVj2wsbEJDg7m83tpU/H5/IULF2JS9QLDqh9CoVCpVPYsV6lUQqHQ8PUxSdgM0I/u7u5hw4aRUytNgwcPbmpqsrCwYKVWJgaPrPphbm7+zjvvCAQCzUKBQBASEoJJ1RcMq96Eh4drnWPJ5fLw8HC26mN6sBmgNwqFwtHRsbm5WV1ia2vb2NjY64kX6gc8suoNn89ftmyZuiUgEAiEQiEmVY8wrPq0bNkydUtALpcvW7aM3fqYGGwG6BPDMKNHj37w4AEAODk5PXjwAH8bW4/wyKpPHA4nIiLCzMzMzMwsKioKk6pfeGTVszt37kyePJm8mDRpEtvVMSnsNP9TUlJu3LjByqoNYMiQIQCwa9cutisyUHx8fDZt2mT49bLTDLhx40ZRURErq9avs2fP1tXVaRWOGTPGxcWFjeoYQlFREVsHGtYurMyYMSMzM5OttesLh8PZuHHj0qVLNQtJT9axY8eyVKmBFRoaytaq8Sqg/plqTFmHVwMQNTCsiBoYVkQNDCuiBp5gsaOgoKC6ulo9yefzbWxshg4dOmnSpMGDB7NYMWOGR1Z2zJw508HBITIyMiYmRiwWd3d33759+5NPPrG3tw8ODr579y7bFTRGeGRlB4fDCQoKsrOzc3Bw2Llzp7r86tWrQqFw6tSp+fn5b775Jos1NEJ4ZGWTmZmZVom/v/+xY8e6u7vfffddqVTKSq2MFh5ZjU5wcLC/v//Vq1czMzPJk7FtbW3p6enl5eVjx45dsWIF6Xtw//59kUi0a9euysrKjIyM4cOHr1ixQt31+9tvv83Lyxs9ejSXy127di0p7HU5FMEjqzHy8fEBgG+++QYAxGKxUCgcM2ZMVFTUkSNHpkyZ8uTJk9TUVD8/vz179uTm5m7duvXGjRtr167dvXs3+XhCQkJNTc22bdtsbW03b95MCntdDlsb2E8MG0JCQkJCQlhZtX4BQHp6er8/PmLECDc3t57laWlpABAQEMAwTEBAQFZWFinPy8sDgO3btzMMEx8fDwA5OTnkrblz57q6ujIMI5PJ7O3tKyoqSHlsbCx58bTlPC8W9x02A4wRGX/AwcHh4cOHly9fnjJlys2bNwGgo6PDy8urs7MTACwtLQEgODiYfMTT05PMIxAIrKys5s2bd+TIkQULFmzbtg0AdCyHIhhWY1RRUQEAHh4eYrEYAOLj43sOQsjl/qYJZ2lpqVAoyOtDhw5FREQEBwf7+PiIRCIHBwcdy6EItlmNjkwmu3DhAp/PX7JkCblccOvWLc0Z2tvbdS9h4cKF9+/fj4uLKykp8fLyKi8v799yjA2G1egkJyeTqHl4eLi5ufF4vMTERJlMRt5tbGw8efKkjo9LJJKjR48OHTp0//79+fn5HR0dp0+f7sdyjBCGlTVyubyxsVGzRCqVbty4cefOnR999FFSUhIA2NnZRUdHFxUVzZkz59SpUyKRaPny5eQJbzKaRldXF/msQqGQy+VSqVSlUiUmJnZ3dwOAj4/PhAkTHBwcdCyHIthmZcf169cPHjyoVCorKyunTZvm4uLC5XKlUqmzs3NRUdG0adPUc+7du7e9vT0tLa2oqMja2jo1NdXGxiYnJycrKwsAtm/fHhsbW1xcfO7cOYZhtm/fHh0d/eTJE29v7zVr1jQ3N/v6+kZHRz9tOaxtf7+w83QreTTCNB5rSU9P13qsZSA0NTXV1NS4u7s/c5g3hmG6urqUSqVYLHZ1ddW68t/35TwNi/sOj6x0GDZsWB9P5DkcDum3pXl47sdyjBC2WRE1jDesCoXi3Llz8+bNO3jwoGHWKJFIzpw5s3nz5tOnT7PSOkK6GW9Y6+rq6uvrr169+rQfltCvX3/9ddq0aX/5y1+OHTsWHh4eGxtrgJWi52K8YXVxcTHkSLyHDx/+5z//mZub+/Dhw9dff/3YsWOaP22FjIHxhhUADDm46UcffWRlZQUAFhYWkZGRHA6nZ2dTxC6jvhrQcxQ+sVh88eJFch1xwYIFAJCTk0M6ZHA4nPfeew8Afvrppzt37gBAYGCgvb19r504KysrRSLRjh078vLyysrKNm7cOGjQIPVaGhsb4+LizM3NDbalqE9Y6evVx25mra2tAJCcnEwmY2JiZs2a1dTUdOnSJQ6H88knnzAMU15e7uTkBABisZjMplQq/f39Dx06pFKp7t27t2jRoq+//vr777/39PQcN25cS0tLamrqiBEjAEAkEk2dOhUACgoK1Cv97rvvlixZolKp+rIh8GJdBGnEYhdBo24GaElLSwsKCrK3tw8ICHB3d8/OzgaAiRMn7tu3DzR+K1WpVLa2tkZHR3M4nPXr169atSowMHDy5MnJycmVlZUpKSmRkZGrV68mM9+6devu3buks3NHR8f7778/e/bsrKysTZs2qW+jIyNh1M0ALbm5ue7u7gBQXFzMMIz6tnhYWFhiYuK+ffsCAwMBICsra/HixTweT0cnTnL/htwcd3NzI8sZMmTI559/vmrVqg0bNhw4cOCNN97oy++thYWFhYWFDcgGG6uQkBBW1ktTWH19fbOyss6fPx8UFOTi4kJGQwcAHo+XkJCwZs2a4uJib2/vY8eOpaamAoCOTpxPG5Oaw+F4eXnl5eWNGzfuwoULfQlrXFwcOTC/JPbv38/WqmkKa3x8fHl5eWZmprm5+blz5zTfioyM3LFjx549e/bu3Wtra0uapOpOnOSIS7S3t5Ozfh1sbGzmzJnTx2aAj4+PAfoGGA8We3RQ02YtKSlJTk5ev369+iSd0bjJZGZmtmXLlq+++iouLo50MgKAF+nE+ejRozlz5uh1C9CLMuqwdnR0wL8eSCKdM7KzsxUKxZUrV0pLS1taWsRisXoQnjVr1tjb21dXV8+dO5eU6OjESe6KPX78mMypUChOnTqlHsM6Pz+/s7Nz3bp1Bt1a9EysXIPoy+WP+vp6EhcPD4/s7GyGYSIiIrhcrqOj4+HDh5OSkrhc7pYtWzQ/Eh8fn5KSolkikUgiIyPJllpbW5PHO8+ePUtOqkJDQ0tLSxmGefTo0dChQwUCwX/+538uXrw4Jiams7OzLxsCeOnKgIw3rL1qaGiQyWTkdXNzs9a7wcHBPQsZhmlsbCwpKdGdP5VKJRaLa2pqnqs+GFZDoukECwAcHBzUr+3s7DTfKiwsHD16tFYh0ZdOnBwOZ/z48XqpJBoglIW1p+Li4k2bNr322mtlZWUXLlxguzp609DQIBKJbt68KZVKY2JigoKC2K4R+4z6BKuPxGJxVVXVgQMHqHuo6Gk6OzvfeecdoVCYkZHB5/NDQ0Ope2x6IFB/ZPX29n706BHbtdCz7OzsX375ZeTIkQBw5syZW7du7dy5c+/evVoDW7xsXuqNN1q3b99WX042Nze3srI6fPgwu1UyBtQfWanQ6wCUUqn02rVr165dGzly5Pz588eNGwcADx8+zM/PLygokEgkp0+fBoBXXnklPDycPHIjEAhCQ0N//vnnEydObN269dGjRyKRyNHRcdmyZba2tpWVlZmZmWZmZitXrtQ80ezZr/Lq1asNDQ3k3eDg4Hv37t2/fx/+1anSwF9O3+GRdcD1OgBld3f3/PnzW1paPvzwQ4Zhpk6dev78eQDg8/mWlpY8Ho/L5VpaWlpaWjIMM2vWLACwsbGxsbE5c+bMrFmzdu3alZub+/vf//7+/fsxMTGrV6/+xz/+8fHHH//4448JCQmaXRo2bNiwevVqoVA4c+bMhQsXfvrppwAwbdq0y5cvh4eHX7x40cbGxsnJafXq1dbW1kOHDmXjG+ozVi6YvTxDXj5tAMrw8PCVK1eqZwsJCbGwsKitrSWTYWFhmkNhknHc1V1syaR6sMv3338fANLS0sjkxx9/DACtra1k0sbGJikpibz28PCYMWOGumJ+fn5WVlY1NTWxsbHnz5/v4yZjf1aTpR6AkgyJSgag7OzszMzMJP2+iXXr1nV1dZ04caIvyyQPO8yePZtMkl+M9/PzI5MTJ04EAHWXtNzcXHIjUKtfpUAgIKPALlq0yNzcfMmSJS++sQMNwzrgDh061NHRERwcPHPmzJaWFgAoLCyUy+WaT5hNmDABAO7du9eXBZJrAupejpoP5MC/+pqpHwn29fW9du1aRETEvXv3XFxcGI3eP6+++uof/vCH0tJSDw+PF9lAg8GwDrieA1AqlUoAKCwsVM9DbrC5urrqfe3x8fHHjx8/evSoUCjUirVKpfr222/9/f03bNhQW1ur91XrHYZ1YPU6AOXUqVMHDRpUUFCgno0MJ0hOpJ6GRPy56O5XmZSUFBUVdfLkSYFAEBUVxRj9uB4Y1oHV6wCUw4cPj4mJqa6uJj9xAQDZ2dmhoaHqHrQtLS3kYUmC9IgoKSm5fv16d3e3Zs9J+Nd/fM2xL6EP/Sr/9re/PX78eMGCBY6Ojnv37v3mm2/27t1riG/kRbByWvfyXA1oa2uzsLCYNGnSn/70px07dqxcuZL0GlMqlZs2bXJwcEhISIiKilq6dGlXVxfDMI8fP05JSSH/r2NjY/Pz8xmGqaqqcnR0tLOz++KLLy5duuTp6QkA0dHRFRUVFy9enD59OgAIhcI7d+78/e9/J2daISEhZWVlzFP6VZ49e9bKymrVqlUKhYJhmP/93/8FAB6Pt3Xr1mf2jWRx3+GQly/kmUNeMjoHoOzq6rp79667u/szxyiQy+UKhaJ/41Q2Njba2tqSn8hqaWnptWNa3+GQlyZL9wCUFhYWmhewdBAIBOofZHteOvpV0gXbrIgaGFZEDQwrogaGFVEDw4qogWFF1MCwImqwdp21rq4uIyODrbXr0Y0bN9iugkHV1dU5Ozuzs25W7puxNWYi0ouX63YrQv2AbVZEDQwrogaGFVEDw4qo8f8AwTdFJ9ILRh4AAAAASUVORK5CYII=\n",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "plot_model(model, show_layer_activations=True, to_file='model/model.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_path = \"model/checkpoint.h5\"\n",
    "\n",
    "checkpoint = ModelCheckpoint(filepath=checkpoint_path,\n",
    "                             save_weights_only=True,\n",
    "                             save_best_only=True,\n",
    "                             monitor='val_loss',\n",
    "                             verbose=2\n",
    "                            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='categorical_crossentropy',\n",
    "             optimizer='adam',\n",
    "             metrics=['acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-08-08 13:25:21.723287: W tensorflow/core/platform/profile_utils/cpu_utils.cc:128] Failed to get CPU frequency: 0 Hz\n",
      "2022-08-08 13:25:21.849437: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:112] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/500\n",
      "461/468 [============================>.] - ETA: 0s - loss: 2.6811 - acc: 0.1095"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-08-08 13:25:24.515172: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:112] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00001: val_loss improved from inf to 2.49736, saving model to model/checkpoint.h5\n",
      "468/468 [==============================] - 3s 6ms/step - loss: 2.6790 - acc: 0.1090 - val_loss: 2.4974 - val_acc: 0.2019\n",
      "Epoch 2/500\n",
      "462/468 [============================>.] - ETA: 0s - loss: 2.2822 - acc: 0.2013\n",
      "Epoch 00002: val_loss improved from 2.49736 to 2.15273, saving model to model/checkpoint.h5\n",
      "468/468 [==============================] - 3s 6ms/step - loss: 2.2800 - acc: 0.1998 - val_loss: 2.1527 - val_acc: 0.2115\n",
      "Epoch 3/500\n",
      "468/468 [==============================] - ETA: 0s - loss: 1.9718 - acc: 0.2511\n",
      "Epoch 00003: val_loss improved from 2.15273 to 1.90896, saving model to model/checkpoint.h5\n",
      "468/468 [==============================] - 3s 6ms/step - loss: 1.9718 - acc: 0.2511 - val_loss: 1.9090 - val_acc: 0.2019\n",
      "Epoch 4/500\n",
      "468/468 [==============================] - ETA: 0s - loss: 1.8011 - acc: 0.2949\n",
      "Epoch 00004: val_loss improved from 1.90896 to 1.76769, saving model to model/checkpoint.h5\n",
      "468/468 [==============================] - 3s 6ms/step - loss: 1.8011 - acc: 0.2949 - val_loss: 1.7677 - val_acc: 0.2788\n",
      "Epoch 5/500\n",
      "466/468 [============================>.] - ETA: 0s - loss: 1.7010 - acc: 0.3208\n",
      "Epoch 00005: val_loss improved from 1.76769 to 1.70829, saving model to model/checkpoint.h5\n",
      "468/468 [==============================] - 3s 6ms/step - loss: 1.7021 - acc: 0.3194 - val_loss: 1.7083 - val_acc: 0.3077\n",
      "Epoch 6/500\n",
      "468/468 [==============================] - ETA: 0s - loss: 1.6341 - acc: 0.3237\n",
      "Epoch 00006: val_loss improved from 1.70829 to 1.65782, saving model to model/checkpoint.h5\n",
      "468/468 [==============================] - 3s 6ms/step - loss: 1.6341 - acc: 0.3237 - val_loss: 1.6578 - val_acc: 0.3173\n",
      "Epoch 7/500\n",
      "467/468 [============================>.] - ETA: 0s - loss: 1.5915 - acc: 0.3276\n",
      "Epoch 00007: val_loss improved from 1.65782 to 1.61187, saving model to model/checkpoint.h5\n",
      "468/468 [==============================] - 3s 6ms/step - loss: 1.5920 - acc: 0.3269 - val_loss: 1.6119 - val_acc: 0.2788\n",
      "Epoch 8/500\n",
      "468/468 [==============================] - ETA: 0s - loss: 1.5595 - acc: 0.3291\n",
      "Epoch 00008: val_loss improved from 1.61187 to 1.58563, saving model to model/checkpoint.h5\n",
      "468/468 [==============================] - 3s 6ms/step - loss: 1.5595 - acc: 0.3291 - val_loss: 1.5856 - val_acc: 0.2788\n",
      "Epoch 9/500\n",
      "464/468 [============================>.] - ETA: 0s - loss: 1.5387 - acc: 0.3319\n",
      "Epoch 00009: val_loss did not improve from 1.58563\n",
      "468/468 [==============================] - 3s 6ms/step - loss: 1.5380 - acc: 0.3323 - val_loss: 1.6004 - val_acc: 0.3077\n",
      "Epoch 10/500\n",
      "465/468 [============================>.] - ETA: 0s - loss: 1.5168 - acc: 0.3387\n",
      "Epoch 00010: val_loss improved from 1.58563 to 1.56915, saving model to model/checkpoint.h5\n",
      "468/468 [==============================] - 3s 6ms/step - loss: 1.5188 - acc: 0.3376 - val_loss: 1.5691 - val_acc: 0.3462\n",
      "Epoch 11/500\n",
      "466/468 [============================>.] - ETA: 0s - loss: 1.5025 - acc: 0.3766\n",
      "Epoch 00011: val_loss improved from 1.56915 to 1.54012, saving model to model/checkpoint.h5\n",
      "468/468 [==============================] - 3s 6ms/step - loss: 1.5006 - acc: 0.3771 - val_loss: 1.5401 - val_acc: 0.3462\n",
      "Epoch 12/500\n",
      "465/468 [============================>.] - ETA: 0s - loss: 1.4885 - acc: 0.3591\n",
      "Epoch 00012: val_loss did not improve from 1.54012\n",
      "468/468 [==============================] - 3s 6ms/step - loss: 1.4884 - acc: 0.3600 - val_loss: 1.5845 - val_acc: 0.2981\n",
      "Epoch 13/500\n",
      "461/468 [============================>.] - ETA: 0s - loss: 1.4836 - acc: 0.3601\n",
      "Epoch 00013: val_loss did not improve from 1.54012\n",
      "468/468 [==============================] - 3s 6ms/step - loss: 1.4862 - acc: 0.3579 - val_loss: 1.5517 - val_acc: 0.3462\n",
      "Epoch 14/500\n",
      "464/468 [============================>.] - ETA: 0s - loss: 1.4772 - acc: 0.3513\n",
      "Epoch 00014: val_loss did not improve from 1.54012\n",
      "468/468 [==============================] - 3s 6ms/step - loss: 1.4787 - acc: 0.3526 - val_loss: 1.5518 - val_acc: 0.3654\n",
      "Epoch 15/500\n",
      "464/468 [============================>.] - ETA: 0s - loss: 1.4699 - acc: 0.3502\n",
      "Epoch 00015: val_loss improved from 1.54012 to 1.50012, saving model to model/checkpoint.h5\n",
      "468/468 [==============================] - 3s 6ms/step - loss: 1.4697 - acc: 0.3494 - val_loss: 1.5001 - val_acc: 0.3750\n",
      "Epoch 16/500\n",
      "467/468 [============================>.] - ETA: 0s - loss: 1.4529 - acc: 0.3640\n",
      "Epoch 00016: val_loss did not improve from 1.50012\n",
      "468/468 [==============================] - 3s 6ms/step - loss: 1.4546 - acc: 0.3632 - val_loss: 1.5411 - val_acc: 0.2885\n",
      "Epoch 17/500\n",
      "459/468 [============================>.] - ETA: 0s - loss: 1.4650 - acc: 0.3627\n",
      "Epoch 00017: val_loss did not improve from 1.50012\n",
      "468/468 [==============================] - 3s 6ms/step - loss: 1.4619 - acc: 0.3632 - val_loss: 1.5522 - val_acc: 0.3462\n",
      "Epoch 18/500\n",
      "462/468 [============================>.] - ETA: 0s - loss: 1.4644 - acc: 0.3582\n",
      "Epoch 00018: val_loss did not improve from 1.50012\n",
      "468/468 [==============================] - 3s 6ms/step - loss: 1.4697 - acc: 0.3536 - val_loss: 1.5299 - val_acc: 0.3654\n",
      "Epoch 19/500\n",
      "461/468 [============================>.] - ETA: 0s - loss: 1.4559 - acc: 0.3449\n",
      "Epoch 00019: val_loss did not improve from 1.50012\n",
      "468/468 [==============================] - 3s 6ms/step - loss: 1.4549 - acc: 0.3462 - val_loss: 1.5004 - val_acc: 0.3173\n",
      "Epoch 20/500\n",
      "462/468 [============================>.] - ETA: 0s - loss: 1.4561 - acc: 0.3658\n",
      "Epoch 00020: val_loss did not improve from 1.50012\n",
      "468/468 [==============================] - 3s 6ms/step - loss: 1.4575 - acc: 0.3665 - val_loss: 1.5302 - val_acc: 0.2981\n",
      "Epoch 21/500\n",
      "459/468 [============================>.] - ETA: 0s - loss: 1.4438 - acc: 0.3627\n",
      "Epoch 00021: val_loss did not improve from 1.50012\n",
      "468/468 [==============================] - 3s 6ms/step - loss: 1.4433 - acc: 0.3632 - val_loss: 1.5710 - val_acc: 0.2885\n",
      "Epoch 22/500\n",
      "465/468 [============================>.] - ETA: 0s - loss: 1.4440 - acc: 0.3688\n",
      "Epoch 00022: val_loss improved from 1.50012 to 1.47635, saving model to model/checkpoint.h5\n",
      "468/468 [==============================] - 3s 6ms/step - loss: 1.4494 - acc: 0.3686 - val_loss: 1.4763 - val_acc: 0.3750\n",
      "Epoch 23/500\n",
      "459/468 [============================>.] - ETA: 0s - loss: 1.4455 - acc: 0.3758\n",
      "Epoch 00023: val_loss did not improve from 1.47635\n",
      "468/468 [==============================] - 3s 6ms/step - loss: 1.4466 - acc: 0.3761 - val_loss: 1.5618 - val_acc: 0.3365\n",
      "Epoch 24/500\n",
      "468/468 [==============================] - ETA: 0s - loss: 1.4494 - acc: 0.3590\n",
      "Epoch 00024: val_loss did not improve from 1.47635\n",
      "468/468 [==============================] - 3s 6ms/step - loss: 1.4494 - acc: 0.3590 - val_loss: 1.5238 - val_acc: 0.3750\n",
      "Epoch 25/500\n",
      "460/468 [============================>.] - ETA: 0s - loss: 1.4393 - acc: 0.3739\n",
      "Epoch 00025: val_loss improved from 1.47635 to 1.46691, saving model to model/checkpoint.h5\n",
      "468/468 [==============================] - 3s 6ms/step - loss: 1.4404 - acc: 0.3750 - val_loss: 1.4669 - val_acc: 0.3846\n",
      "Epoch 26/500\n",
      "459/468 [============================>.] - ETA: 0s - loss: 1.4491 - acc: 0.3606\n",
      "Epoch 00026: val_loss did not improve from 1.46691\n",
      "468/468 [==============================] - 3s 6ms/step - loss: 1.4476 - acc: 0.3600 - val_loss: 1.5086 - val_acc: 0.3173\n",
      "Epoch 27/500\n",
      "468/468 [==============================] - ETA: 0s - loss: 1.4524 - acc: 0.3344\n",
      "Epoch 00027: val_loss did not improve from 1.46691\n",
      "468/468 [==============================] - 3s 6ms/step - loss: 1.4524 - acc: 0.3344 - val_loss: 1.5585 - val_acc: 0.3558\n",
      "Epoch 28/500\n",
      "459/468 [============================>.] - ETA: 0s - loss: 1.4370 - acc: 0.3704\n",
      "Epoch 00028: val_loss did not improve from 1.46691\n",
      "468/468 [==============================] - 3s 6ms/step - loss: 1.4421 - acc: 0.3697 - val_loss: 1.4877 - val_acc: 0.3750\n",
      "Epoch 29/500\n",
      "467/468 [============================>.] - ETA: 0s - loss: 1.4395 - acc: 0.3608\n",
      "Epoch 00029: val_loss did not improve from 1.46691\n",
      "468/468 [==============================] - 3s 6ms/step - loss: 1.4391 - acc: 0.3611 - val_loss: 1.5715 - val_acc: 0.3750\n",
      "Epoch 30/500\n",
      "459/468 [============================>.] - ETA: 0s - loss: 1.4397 - acc: 0.3736\n",
      "Epoch 00030: val_loss did not improve from 1.46691\n",
      "468/468 [==============================] - 3s 6ms/step - loss: 1.4407 - acc: 0.3729 - val_loss: 1.4740 - val_acc: 0.3846\n",
      "Epoch 31/500\n",
      "465/468 [============================>.] - ETA: 0s - loss: 1.4374 - acc: 0.3677\n",
      "Epoch 00031: val_loss did not improve from 1.46691\n",
      "468/468 [==============================] - 3s 6ms/step - loss: 1.4358 - acc: 0.3675 - val_loss: 1.5027 - val_acc: 0.3173\n",
      "Epoch 32/500\n",
      "463/468 [============================>.] - ETA: 0s - loss: 1.4348 - acc: 0.3596\n",
      "Epoch 00032: val_loss did not improve from 1.46691\n",
      "468/468 [==============================] - 3s 6ms/step - loss: 1.4328 - acc: 0.3611 - val_loss: 1.5033 - val_acc: 0.2788\n",
      "Epoch 33/500\n",
      "467/468 [============================>.] - ETA: 0s - loss: 1.4436 - acc: 0.3683\n",
      "Epoch 00033: val_loss did not improve from 1.46691\n",
      "468/468 [==============================] - 3s 6ms/step - loss: 1.4428 - acc: 0.3686 - val_loss: 1.4829 - val_acc: 0.3462\n",
      "Epoch 34/500\n",
      "468/468 [==============================] - ETA: 0s - loss: 1.4310 - acc: 0.3761\n",
      "Epoch 00034: val_loss did not improve from 1.46691\n",
      "468/468 [==============================] - 3s 6ms/step - loss: 1.4310 - acc: 0.3761 - val_loss: 1.4863 - val_acc: 0.3558\n",
      "Epoch 35/500\n",
      "464/468 [============================>.] - ETA: 0s - loss: 1.4417 - acc: 0.3534\n",
      "Epoch 00035: val_loss did not improve from 1.46691\n",
      "468/468 [==============================] - 3s 6ms/step - loss: 1.4384 - acc: 0.3526 - val_loss: 1.4873 - val_acc: 0.4038\n",
      "Epoch 36/500\n",
      "466/468 [============================>.] - ETA: 0s - loss: 1.4284 - acc: 0.3605\n",
      "Epoch 00036: val_loss did not improve from 1.46691\n",
      "468/468 [==============================] - 3s 6ms/step - loss: 1.4326 - acc: 0.3600 - val_loss: 1.5224 - val_acc: 0.3942\n",
      "Epoch 37/500\n",
      "459/468 [============================>.] - ETA: 0s - loss: 1.4365 - acc: 0.3475\n",
      "Epoch 00037: val_loss did not improve from 1.46691\n",
      "468/468 [==============================] - 3s 6ms/step - loss: 1.4357 - acc: 0.3462 - val_loss: 1.4741 - val_acc: 0.3654\n",
      "Epoch 38/500\n",
      "466/468 [============================>.] - ETA: 0s - loss: 1.4359 - acc: 0.3745\n",
      "Epoch 00038: val_loss did not improve from 1.46691\n",
      "468/468 [==============================] - 3s 6ms/step - loss: 1.4345 - acc: 0.3739 - val_loss: 1.5396 - val_acc: 0.3558\n",
      "Epoch 39/500\n",
      "464/468 [============================>.] - ETA: 0s - loss: 1.4331 - acc: 0.3556\n",
      "Epoch 00039: val_loss did not improve from 1.46691\n",
      "468/468 [==============================] - 3s 6ms/step - loss: 1.4367 - acc: 0.3536 - val_loss: 1.4814 - val_acc: 0.3750\n",
      "Epoch 40/500\n",
      "464/468 [============================>.] - ETA: 0s - loss: 1.4362 - acc: 0.3685\n",
      "Epoch 00040: val_loss did not improve from 1.46691\n",
      "468/468 [==============================] - 3s 6ms/step - loss: 1.4338 - acc: 0.3707 - val_loss: 1.4706 - val_acc: 0.4135\n",
      "Epoch 41/500\n",
      "465/468 [============================>.] - ETA: 0s - loss: 1.4294 - acc: 0.3688\n",
      "Epoch 00041: val_loss did not improve from 1.46691\n",
      "468/468 [==============================] - 3s 6ms/step - loss: 1.4336 - acc: 0.3665 - val_loss: 1.5088 - val_acc: 0.3750\n",
      "Epoch 42/500\n",
      "464/468 [============================>.] - ETA: 0s - loss: 1.4299 - acc: 0.3728\n",
      "Epoch 00042: val_loss improved from 1.46691 to 1.45231, saving model to model/checkpoint.h5\n",
      "468/468 [==============================] - 3s 6ms/step - loss: 1.4301 - acc: 0.3729 - val_loss: 1.4523 - val_acc: 0.3558\n",
      "Epoch 43/500\n",
      "466/468 [============================>.] - ETA: 0s - loss: 1.4333 - acc: 0.3766\n",
      "Epoch 00043: val_loss did not improve from 1.45231\n",
      "468/468 [==============================] - 3s 6ms/step - loss: 1.4334 - acc: 0.3750 - val_loss: 1.4571 - val_acc: 0.3462\n",
      "Epoch 44/500\n",
      "462/468 [============================>.] - ETA: 0s - loss: 1.4309 - acc: 0.3626\n",
      "Epoch 00044: val_loss did not improve from 1.45231\n",
      "468/468 [==============================] - 3s 6ms/step - loss: 1.4272 - acc: 0.3632 - val_loss: 1.4834 - val_acc: 0.3462\n",
      "Epoch 45/500\n",
      "460/468 [============================>.] - ETA: 0s - loss: 1.4207 - acc: 0.3891\n",
      "Epoch 00045: val_loss did not improve from 1.45231\n",
      "468/468 [==============================] - 3s 6ms/step - loss: 1.4225 - acc: 0.3878 - val_loss: 1.4621 - val_acc: 0.3654\n",
      "Epoch 46/500\n",
      "459/468 [============================>.] - ETA: 0s - loss: 1.4265 - acc: 0.3715\n",
      "Epoch 00046: val_loss did not improve from 1.45231\n",
      "468/468 [==============================] - 3s 6ms/step - loss: 1.4263 - acc: 0.3718 - val_loss: 1.4964 - val_acc: 0.3750\n",
      "Epoch 47/500\n",
      "460/468 [============================>.] - ETA: 0s - loss: 1.4179 - acc: 0.3663\n",
      "Epoch 00047: val_loss did not improve from 1.45231\n",
      "468/468 [==============================] - 3s 6ms/step - loss: 1.4236 - acc: 0.3643 - val_loss: 1.5053 - val_acc: 0.3269\n",
      "Epoch 48/500\n",
      "460/468 [============================>.] - ETA: 0s - loss: 1.4306 - acc: 0.3598\n",
      "Epoch 00048: val_loss did not improve from 1.45231\n",
      "468/468 [==============================] - 3s 6ms/step - loss: 1.4325 - acc: 0.3600 - val_loss: 1.5019 - val_acc: 0.3365\n",
      "Epoch 49/500\n",
      "465/468 [============================>.] - ETA: 0s - loss: 1.4288 - acc: 0.3796\n",
      "Epoch 00049: val_loss did not improve from 1.45231\n",
      "468/468 [==============================] - 3s 6ms/step - loss: 1.4294 - acc: 0.3793 - val_loss: 1.4602 - val_acc: 0.4038\n",
      "Epoch 50/500\n",
      "463/468 [============================>.] - ETA: 0s - loss: 1.4330 - acc: 0.3661\n",
      "Epoch 00050: val_loss did not improve from 1.45231\n",
      "468/468 [==============================] - 3s 6ms/step - loss: 1.4324 - acc: 0.3675 - val_loss: 1.4928 - val_acc: 0.3942\n",
      "Epoch 51/500\n",
      "468/468 [==============================] - ETA: 0s - loss: 1.4204 - acc: 0.3739\n",
      "Epoch 00051: val_loss did not improve from 1.45231\n",
      "468/468 [==============================] - 3s 6ms/step - loss: 1.4204 - acc: 0.3739 - val_loss: 1.5503 - val_acc: 0.4038\n",
      "Epoch 52/500\n",
      "460/468 [============================>.] - ETA: 0s - loss: 1.4282 - acc: 0.3837\n",
      "Epoch 00052: val_loss did not improve from 1.45231\n",
      "468/468 [==============================] - 3s 6ms/step - loss: 1.4281 - acc: 0.3846 - val_loss: 1.4879 - val_acc: 0.3942\n",
      "Epoch 53/500\n",
      "459/468 [============================>.] - ETA: 0s - loss: 1.4185 - acc: 0.3519\n",
      "Epoch 00053: val_loss did not improve from 1.45231\n",
      "468/468 [==============================] - 3s 6ms/step - loss: 1.4204 - acc: 0.3536 - val_loss: 1.5225 - val_acc: 0.3269\n",
      "Epoch 54/500\n",
      "462/468 [============================>.] - ETA: 0s - loss: 1.4211 - acc: 0.3918\n",
      "Epoch 00054: val_loss did not improve from 1.45231\n",
      "468/468 [==============================] - 3s 6ms/step - loss: 1.4229 - acc: 0.3910 - val_loss: 1.4871 - val_acc: 0.3269\n",
      "Epoch 55/500\n",
      "461/468 [============================>.] - ETA: 0s - loss: 1.4219 - acc: 0.3796\n",
      "Epoch 00055: val_loss did not improve from 1.45231\n",
      "468/468 [==============================] - 3s 6ms/step - loss: 1.4240 - acc: 0.3782 - val_loss: 1.5797 - val_acc: 0.3558\n",
      "Epoch 56/500\n",
      "461/468 [============================>.] - ETA: 0s - loss: 1.4209 - acc: 0.3677\n",
      "Epoch 00056: val_loss did not improve from 1.45231\n",
      "468/468 [==============================] - 3s 6ms/step - loss: 1.4171 - acc: 0.3707 - val_loss: 1.5195 - val_acc: 0.3558\n",
      "Epoch 57/500\n",
      "459/468 [============================>.] - ETA: 0s - loss: 1.4234 - acc: 0.3802\n",
      "Epoch 00057: val_loss did not improve from 1.45231\n",
      "468/468 [==============================] - 3s 6ms/step - loss: 1.4260 - acc: 0.3803 - val_loss: 1.4874 - val_acc: 0.3462\n",
      "Epoch 58/500\n",
      "465/468 [============================>.] - ETA: 0s - loss: 1.4216 - acc: 0.3828\n",
      "Epoch 00058: val_loss did not improve from 1.45231\n",
      "468/468 [==============================] - 3s 6ms/step - loss: 1.4217 - acc: 0.3825 - val_loss: 1.5437 - val_acc: 0.3173\n",
      "Epoch 59/500\n",
      "465/468 [============================>.] - ETA: 0s - loss: 1.4203 - acc: 0.3892\n",
      "Epoch 00059: val_loss did not improve from 1.45231\n",
      "468/468 [==============================] - 3s 6ms/step - loss: 1.4245 - acc: 0.3878 - val_loss: 1.4867 - val_acc: 0.3173\n",
      "Epoch 60/500\n",
      "463/468 [============================>.] - ETA: 0s - loss: 1.4305 - acc: 0.4017\n",
      "Epoch 00060: val_loss did not improve from 1.45231\n",
      "468/468 [==============================] - 3s 6ms/step - loss: 1.4287 - acc: 0.4028 - val_loss: 1.5275 - val_acc: 0.3846\n",
      "Epoch 61/500\n",
      "463/468 [============================>.] - ETA: 0s - loss: 1.4227 - acc: 0.3855\n",
      "Epoch 00061: val_loss did not improve from 1.45231\n",
      "468/468 [==============================] - 3s 6ms/step - loss: 1.4239 - acc: 0.3835 - val_loss: 1.4893 - val_acc: 0.3365\n",
      "Epoch 62/500\n",
      "463/468 [============================>.] - ETA: 0s - loss: 1.4184 - acc: 0.3985\n",
      "Epoch 00062: val_loss did not improve from 1.45231\n",
      "468/468 [==============================] - 3s 6ms/step - loss: 1.4162 - acc: 0.3996 - val_loss: 1.5107 - val_acc: 0.4327\n",
      "Epoch 63/500\n",
      "468/468 [==============================] - ETA: 0s - loss: 1.4225 - acc: 0.3771\n",
      "Epoch 00063: val_loss did not improve from 1.45231\n",
      "468/468 [==============================] - 3s 6ms/step - loss: 1.4225 - acc: 0.3771 - val_loss: 1.5000 - val_acc: 0.3942\n",
      "Epoch 64/500\n",
      "464/468 [============================>.] - ETA: 0s - loss: 1.4194 - acc: 0.3815\n",
      "Epoch 00064: val_loss did not improve from 1.45231\n",
      "468/468 [==============================] - 3s 6ms/step - loss: 1.4185 - acc: 0.3814 - val_loss: 1.5156 - val_acc: 0.3750\n",
      "Epoch 65/500\n",
      "464/468 [============================>.] - ETA: 0s - loss: 1.4217 - acc: 0.3933\n",
      "Epoch 00065: val_loss did not improve from 1.45231\n",
      "468/468 [==============================] - 3s 6ms/step - loss: 1.4242 - acc: 0.3921 - val_loss: 1.5180 - val_acc: 0.3462\n",
      "Epoch 66/500\n",
      "462/468 [============================>.] - ETA: 0s - loss: 1.4188 - acc: 0.3755\n",
      "Epoch 00066: val_loss did not improve from 1.45231\n",
      "468/468 [==============================] - 3s 6ms/step - loss: 1.4214 - acc: 0.3729 - val_loss: 1.5127 - val_acc: 0.3173\n",
      "Epoch 67/500\n",
      "459/468 [============================>.] - ETA: 0s - loss: 1.4241 - acc: 0.3824\n",
      "Epoch 00067: val_loss did not improve from 1.45231\n",
      "468/468 [==============================] - 3s 6ms/step - loss: 1.4204 - acc: 0.3835 - val_loss: 1.4739 - val_acc: 0.3942\n",
      "Epoch 68/500\n",
      "467/468 [============================>.] - ETA: 0s - loss: 1.4190 - acc: 0.3833\n",
      "Epoch 00068: val_loss did not improve from 1.45231\n",
      "468/468 [==============================] - 3s 6ms/step - loss: 1.4187 - acc: 0.3825 - val_loss: 1.5402 - val_acc: 0.3654\n",
      "Epoch 69/500\n",
      "466/468 [============================>.] - ETA: 0s - loss: 1.4250 - acc: 0.3702\n",
      "Epoch 00069: val_loss did not improve from 1.45231\n",
      "468/468 [==============================] - 3s 6ms/step - loss: 1.4236 - acc: 0.3718 - val_loss: 1.4598 - val_acc: 0.3846\n",
      "Epoch 70/500\n",
      "467/468 [============================>.] - ETA: 0s - loss: 1.4166 - acc: 0.3801\n",
      "Epoch 00070: val_loss did not improve from 1.45231\n",
      "468/468 [==============================] - 3s 6ms/step - loss: 1.4165 - acc: 0.3793 - val_loss: 1.4987 - val_acc: 0.3942\n",
      "Epoch 71/500\n",
      "466/468 [============================>.] - ETA: 0s - loss: 1.4189 - acc: 0.3863\n",
      "Epoch 00071: val_loss did not improve from 1.45231\n",
      "468/468 [==============================] - 3s 6ms/step - loss: 1.4186 - acc: 0.3857 - val_loss: 1.5319 - val_acc: 0.4135\n",
      "Epoch 72/500\n",
      "461/468 [============================>.] - ETA: 0s - loss: 1.4101 - acc: 0.4056\n",
      "Epoch 00072: val_loss did not improve from 1.45231\n",
      "468/468 [==============================] - 3s 6ms/step - loss: 1.4144 - acc: 0.4006 - val_loss: 1.5173 - val_acc: 0.3558\n",
      "Epoch 73/500\n",
      "465/468 [============================>.] - ETA: 0s - loss: 1.4195 - acc: 0.3763\n",
      "Epoch 00073: val_loss did not improve from 1.45231\n",
      "468/468 [==============================] - 3s 6ms/step - loss: 1.4208 - acc: 0.3750 - val_loss: 1.5787 - val_acc: 0.3846\n",
      "Epoch 74/500\n",
      "463/468 [============================>.] - ETA: 0s - loss: 1.4224 - acc: 0.3661\n",
      "Epoch 00074: val_loss did not improve from 1.45231\n",
      "468/468 [==============================] - 3s 6ms/step - loss: 1.4204 - acc: 0.3675 - val_loss: 1.5607 - val_acc: 0.3846\n",
      "Epoch 75/500\n",
      "462/468 [============================>.] - ETA: 0s - loss: 1.4166 - acc: 0.3831\n",
      "Epoch 00075: val_loss did not improve from 1.45231\n",
      "468/468 [==============================] - 3s 6ms/step - loss: 1.4174 - acc: 0.3814 - val_loss: 1.5095 - val_acc: 0.3462\n",
      "Epoch 76/500\n",
      "466/468 [============================>.] - ETA: 0s - loss: 1.4131 - acc: 0.3712\n",
      "Epoch 00076: val_loss did not improve from 1.45231\n",
      "468/468 [==============================] - 3s 6ms/step - loss: 1.4138 - acc: 0.3697 - val_loss: 1.4925 - val_acc: 0.3558\n",
      "Epoch 77/500\n",
      "462/468 [============================>.] - ETA: 0s - loss: 1.4107 - acc: 0.3788\n",
      "Epoch 00077: val_loss did not improve from 1.45231\n",
      "468/468 [==============================] - 3s 6ms/step - loss: 1.4106 - acc: 0.3782 - val_loss: 1.5955 - val_acc: 0.4038\n",
      "Epoch 78/500\n",
      "465/468 [============================>.] - ETA: 0s - loss: 1.4181 - acc: 0.3753\n",
      "Epoch 00078: val_loss did not improve from 1.45231\n",
      "468/468 [==============================] - 3s 6ms/step - loss: 1.4172 - acc: 0.3739 - val_loss: 1.5574 - val_acc: 0.3173\n",
      "Epoch 79/500\n",
      "467/468 [============================>.] - ETA: 0s - loss: 1.4149 - acc: 0.3801\n",
      "Epoch 00079: val_loss did not improve from 1.45231\n",
      "468/468 [==============================] - 3s 6ms/step - loss: 1.4164 - acc: 0.3793 - val_loss: 1.5252 - val_acc: 0.3269\n",
      "Epoch 80/500\n",
      "463/468 [============================>.] - ETA: 0s - loss: 1.4114 - acc: 0.3931\n",
      "Epoch 00080: val_loss did not improve from 1.45231\n",
      "468/468 [==============================] - 3s 6ms/step - loss: 1.4096 - acc: 0.3942 - val_loss: 1.5424 - val_acc: 0.3558\n",
      "Epoch 81/500\n",
      "465/468 [============================>.] - ETA: 0s - loss: 1.4167 - acc: 0.3731\n",
      "Epoch 00081: val_loss did not improve from 1.45231\n",
      "468/468 [==============================] - 3s 6ms/step - loss: 1.4185 - acc: 0.3729 - val_loss: 1.5038 - val_acc: 0.3462\n",
      "Epoch 82/500\n",
      "462/468 [============================>.] - ETA: 0s - loss: 1.4128 - acc: 0.3799\n",
      "Epoch 00082: val_loss did not improve from 1.45231\n",
      "468/468 [==============================] - 3s 6ms/step - loss: 1.4142 - acc: 0.3782 - val_loss: 1.4943 - val_acc: 0.4038\n",
      "Epoch 83/500\n",
      "462/468 [============================>.] - ETA: 0s - loss: 1.4159 - acc: 0.3885\n",
      "Epoch 00083: val_loss did not improve from 1.45231\n",
      "468/468 [==============================] - 3s 6ms/step - loss: 1.4143 - acc: 0.3878 - val_loss: 1.5010 - val_acc: 0.3462\n",
      "Epoch 84/500\n",
      "461/468 [============================>.] - ETA: 0s - loss: 1.4182 - acc: 0.3818\n",
      "Epoch 00084: val_loss did not improve from 1.45231\n",
      "468/468 [==============================] - 3s 6ms/step - loss: 1.4183 - acc: 0.3814 - val_loss: 1.5034 - val_acc: 0.3750\n",
      "Epoch 85/500\n",
      "465/468 [============================>.] - ETA: 0s - loss: 1.4136 - acc: 0.3796\n",
      "Epoch 00085: val_loss did not improve from 1.45231\n",
      "468/468 [==============================] - 3s 6ms/step - loss: 1.4122 - acc: 0.3793 - val_loss: 1.4887 - val_acc: 0.3462\n",
      "Epoch 86/500\n",
      "460/468 [============================>.] - ETA: 0s - loss: 1.4020 - acc: 0.3989\n",
      "Epoch 00086: val_loss did not improve from 1.45231\n",
      "468/468 [==============================] - 3s 6ms/step - loss: 1.4041 - acc: 0.3985 - val_loss: 1.5249 - val_acc: 0.3558\n",
      "Epoch 87/500\n",
      "467/468 [============================>.] - ETA: 0s - loss: 1.4147 - acc: 0.3865\n",
      "Epoch 00087: val_loss did not improve from 1.45231\n",
      "468/468 [==============================] - 3s 6ms/step - loss: 1.4121 - acc: 0.3878 - val_loss: 1.5300 - val_acc: 0.3942\n",
      "Epoch 88/500\n",
      "467/468 [============================>.] - ETA: 0s - loss: 1.4157 - acc: 0.3779\n",
      "Epoch 00088: val_loss did not improve from 1.45231\n",
      "468/468 [==============================] - 3s 6ms/step - loss: 1.4155 - acc: 0.3771 - val_loss: 1.4802 - val_acc: 0.4327\n",
      "Epoch 89/500\n",
      "463/468 [============================>.] - ETA: 0s - loss: 1.4087 - acc: 0.3974\n",
      "Epoch 00089: val_loss did not improve from 1.45231\n",
      "468/468 [==============================] - 3s 6ms/step - loss: 1.4115 - acc: 0.3953 - val_loss: 1.4780 - val_acc: 0.3846\n",
      "Epoch 90/500\n",
      "463/468 [============================>.] - ETA: 0s - loss: 1.4101 - acc: 0.3844\n",
      "Epoch 00090: val_loss did not improve from 1.45231\n",
      "468/468 [==============================] - 3s 6ms/step - loss: 1.4097 - acc: 0.3835 - val_loss: 1.4681 - val_acc: 0.3942\n",
      "Epoch 91/500\n",
      "467/468 [============================>.] - ETA: 0s - loss: 1.4105 - acc: 0.3833\n",
      "Epoch 00091: val_loss did not improve from 1.45231\n",
      "468/468 [==============================] - 3s 6ms/step - loss: 1.4097 - acc: 0.3835 - val_loss: 1.4902 - val_acc: 0.4231\n",
      "Epoch 92/500\n",
      "466/468 [============================>.] - ETA: 0s - loss: 1.4139 - acc: 0.3830\n",
      "Epoch 00092: val_loss did not improve from 1.45231\n",
      "468/468 [==============================] - 3s 6ms/step - loss: 1.4117 - acc: 0.3835 - val_loss: 1.4848 - val_acc: 0.3750\n",
      "Epoch 93/500\n",
      "463/468 [============================>.] - ETA: 0s - loss: 1.4114 - acc: 0.3672\n",
      "Epoch 00093: val_loss did not improve from 1.45231\n",
      "468/468 [==============================] - 3s 6ms/step - loss: 1.4113 - acc: 0.3665 - val_loss: 1.5489 - val_acc: 0.3365\n",
      "Epoch 94/500\n",
      "467/468 [============================>.] - ETA: 0s - loss: 1.4113 - acc: 0.3940\n",
      "Epoch 00094: val_loss did not improve from 1.45231\n",
      "468/468 [==============================] - 3s 6ms/step - loss: 1.4107 - acc: 0.3942 - val_loss: 1.5704 - val_acc: 0.3750\n",
      "Epoch 95/500\n",
      "461/468 [============================>.] - ETA: 0s - loss: 1.4027 - acc: 0.4013\n",
      "Epoch 00095: val_loss did not improve from 1.45231\n",
      "468/468 [==============================] - 3s 6ms/step - loss: 1.4088 - acc: 0.3964 - val_loss: 1.5182 - val_acc: 0.3365\n",
      "Epoch 96/500\n",
      "464/468 [============================>.] - ETA: 0s - loss: 1.4015 - acc: 0.3901\n",
      "Epoch 00096: val_loss did not improve from 1.45231\n",
      "468/468 [==============================] - 3s 6ms/step - loss: 1.4044 - acc: 0.3900 - val_loss: 1.4820 - val_acc: 0.4038\n",
      "Epoch 97/500\n",
      "466/468 [============================>.] - ETA: 0s - loss: 1.4112 - acc: 0.3873\n",
      "Epoch 00097: val_loss did not improve from 1.45231\n",
      "468/468 [==============================] - 3s 6ms/step - loss: 1.4121 - acc: 0.3868 - val_loss: 1.4946 - val_acc: 0.4038\n",
      "Epoch 98/500\n",
      "466/468 [============================>.] - ETA: 0s - loss: 1.4047 - acc: 0.3959\n",
      "Epoch 00098: val_loss did not improve from 1.45231\n",
      "468/468 [==============================] - 3s 6ms/step - loss: 1.4067 - acc: 0.3953 - val_loss: 1.4892 - val_acc: 0.3654\n",
      "Epoch 99/500\n",
      "467/468 [============================>.] - ETA: 0s - loss: 1.4066 - acc: 0.3812\n",
      "Epoch 00099: val_loss did not improve from 1.45231\n",
      "468/468 [==============================] - 3s 6ms/step - loss: 1.4068 - acc: 0.3803 - val_loss: 1.5530 - val_acc: 0.3558\n",
      "Epoch 100/500\n",
      "459/468 [============================>.] - ETA: 0s - loss: 1.4128 - acc: 0.3987\n",
      "Epoch 00100: val_loss did not improve from 1.45231\n",
      "468/468 [==============================] - 3s 6ms/step - loss: 1.4115 - acc: 0.3964 - val_loss: 1.4803 - val_acc: 0.3942\n",
      "Epoch 101/500\n",
      "466/468 [============================>.] - ETA: 0s - loss: 1.4059 - acc: 0.3906\n",
      "Epoch 00101: val_loss did not improve from 1.45231\n",
      "468/468 [==============================] - 3s 6ms/step - loss: 1.4064 - acc: 0.3900 - val_loss: 1.5205 - val_acc: 0.3750\n",
      "Epoch 102/500\n",
      "465/468 [============================>.] - ETA: 0s - loss: 1.4073 - acc: 0.3774\n",
      "Epoch 00102: val_loss did not improve from 1.45231\n",
      "468/468 [==============================] - 3s 6ms/step - loss: 1.4066 - acc: 0.3771 - val_loss: 1.5041 - val_acc: 0.3846\n",
      "Epoch 103/500\n",
      "462/468 [============================>.] - ETA: 0s - loss: 1.4111 - acc: 0.3755\n",
      "Epoch 00103: val_loss did not improve from 1.45231\n",
      "468/468 [==============================] - 3s 6ms/step - loss: 1.4076 - acc: 0.3771 - val_loss: 1.4707 - val_acc: 0.4135\n",
      "Epoch 104/500\n",
      "459/468 [============================>.] - ETA: 0s - loss: 1.4065 - acc: 0.3758\n",
      "Epoch 00104: val_loss did not improve from 1.45231\n",
      "468/468 [==============================] - 3s 6ms/step - loss: 1.4048 - acc: 0.3761 - val_loss: 1.5558 - val_acc: 0.3846\n",
      "Epoch 105/500\n",
      "467/468 [============================>.] - ETA: 0s - loss: 1.4046 - acc: 0.3929\n",
      "Epoch 00105: val_loss did not improve from 1.45231\n",
      "468/468 [==============================] - 3s 6ms/step - loss: 1.4054 - acc: 0.3932 - val_loss: 1.5295 - val_acc: 0.4327\n",
      "Epoch 106/500\n",
      "466/468 [============================>.] - ETA: 0s - loss: 1.4010 - acc: 0.3959\n",
      "Epoch 00106: val_loss did not improve from 1.45231\n",
      "468/468 [==============================] - 3s 6ms/step - loss: 1.4044 - acc: 0.3964 - val_loss: 1.5421 - val_acc: 0.3269\n",
      "Epoch 107/500\n",
      "463/468 [============================>.] - ETA: 0s - loss: 1.3963 - acc: 0.3801\n",
      "Epoch 00107: val_loss did not improve from 1.45231\n",
      "468/468 [==============================] - 3s 6ms/step - loss: 1.4031 - acc: 0.3782 - val_loss: 1.4590 - val_acc: 0.3462\n",
      "Epoch 108/500\n",
      "463/468 [============================>.] - ETA: 0s - loss: 1.4145 - acc: 0.3737\n",
      "Epoch 00108: val_loss did not improve from 1.45231\n",
      "468/468 [==============================] - 3s 6ms/step - loss: 1.4110 - acc: 0.3739 - val_loss: 1.5050 - val_acc: 0.4327\n",
      "Epoch 109/500\n",
      "463/468 [============================>.] - ETA: 0s - loss: 1.4084 - acc: 0.3855\n",
      "Epoch 00109: val_loss did not improve from 1.45231\n",
      "468/468 [==============================] - 3s 6ms/step - loss: 1.4065 - acc: 0.3846 - val_loss: 1.4686 - val_acc: 0.3750\n",
      "Epoch 110/500\n",
      "463/468 [============================>.] - ETA: 0s - loss: 1.4089 - acc: 0.3877\n",
      "Epoch 00110: val_loss did not improve from 1.45231\n",
      "468/468 [==============================] - 3s 6ms/step - loss: 1.4085 - acc: 0.3910 - val_loss: 1.5130 - val_acc: 0.3846\n",
      "Epoch 111/500\n",
      "466/468 [============================>.] - ETA: 0s - loss: 1.3993 - acc: 0.4088\n",
      "Epoch 00111: val_loss did not improve from 1.45231\n",
      "468/468 [==============================] - 3s 6ms/step - loss: 1.4043 - acc: 0.4071 - val_loss: 1.5057 - val_acc: 0.3942\n",
      "Epoch 112/500\n",
      "468/468 [==============================] - ETA: 0s - loss: 1.4018 - acc: 0.4081\n",
      "Epoch 00112: val_loss did not improve from 1.45231\n",
      "468/468 [==============================] - 3s 6ms/step - loss: 1.4018 - acc: 0.4081 - val_loss: 1.5463 - val_acc: 0.3462\n",
      "Epoch 113/500\n",
      "466/468 [============================>.] - ETA: 0s - loss: 1.4050 - acc: 0.3916\n",
      "Epoch 00113: val_loss did not improve from 1.45231\n",
      "468/468 [==============================] - 3s 6ms/step - loss: 1.4040 - acc: 0.3921 - val_loss: 1.4592 - val_acc: 0.3846\n",
      "Epoch 114/500\n",
      "463/468 [============================>.] - ETA: 0s - loss: 1.3962 - acc: 0.4082\n",
      "Epoch 00114: val_loss did not improve from 1.45231\n",
      "468/468 [==============================] - 3s 6ms/step - loss: 1.4006 - acc: 0.4049 - val_loss: 1.5096 - val_acc: 0.3365\n",
      "Epoch 115/500\n",
      "464/468 [============================>.] - ETA: 0s - loss: 1.4056 - acc: 0.3998\n",
      "Epoch 00115: val_loss did not improve from 1.45231\n",
      "468/468 [==============================] - 3s 6ms/step - loss: 1.4046 - acc: 0.4006 - val_loss: 1.5426 - val_acc: 0.3558\n",
      "Epoch 116/500\n",
      "467/468 [============================>.] - ETA: 0s - loss: 1.4022 - acc: 0.3972\n",
      "Epoch 00116: val_loss did not improve from 1.45231\n",
      "468/468 [==============================] - 3s 6ms/step - loss: 1.4046 - acc: 0.3964 - val_loss: 1.4733 - val_acc: 0.4231\n",
      "Epoch 117/500\n",
      "466/468 [============================>.] - ETA: 0s - loss: 1.4026 - acc: 0.4067\n",
      "Epoch 00117: val_loss did not improve from 1.45231\n",
      "468/468 [==============================] - 3s 6ms/step - loss: 1.4036 - acc: 0.4071 - val_loss: 1.5736 - val_acc: 0.4231\n",
      "Epoch 118/500\n",
      "465/468 [============================>.] - ETA: 0s - loss: 1.3985 - acc: 0.3796\n",
      "Epoch 00118: val_loss did not improve from 1.45231\n",
      "468/468 [==============================] - 3s 6ms/step - loss: 1.3969 - acc: 0.3803 - val_loss: 1.5545 - val_acc: 0.3269\n",
      "Epoch 119/500\n",
      "463/468 [============================>.] - ETA: 0s - loss: 1.4014 - acc: 0.3888\n",
      "Epoch 00119: val_loss did not improve from 1.45231\n",
      "468/468 [==============================] - 3s 6ms/step - loss: 1.4009 - acc: 0.3878 - val_loss: 1.5430 - val_acc: 0.3077\n",
      "Epoch 120/500\n",
      "466/468 [============================>.] - ETA: 0s - loss: 1.3972 - acc: 0.3938\n",
      "Epoch 00120: val_loss did not improve from 1.45231\n",
      "468/468 [==============================] - 3s 6ms/step - loss: 1.4015 - acc: 0.3932 - val_loss: 1.5764 - val_acc: 0.3750\n",
      "Epoch 121/500\n",
      "461/468 [============================>.] - ETA: 0s - loss: 1.3938 - acc: 0.3883\n",
      "Epoch 00121: val_loss did not improve from 1.45231\n",
      "468/468 [==============================] - 3s 6ms/step - loss: 1.3967 - acc: 0.3878 - val_loss: 1.5292 - val_acc: 0.4038\n",
      "Epoch 122/500\n",
      "464/468 [============================>.] - ETA: 0s - loss: 1.3897 - acc: 0.4095\n",
      "Epoch 00122: val_loss did not improve from 1.45231\n",
      "468/468 [==============================] - 3s 6ms/step - loss: 1.3934 - acc: 0.4071 - val_loss: 1.5471 - val_acc: 0.3462\n",
      "Epoch 123/500\n",
      "463/468 [============================>.] - ETA: 0s - loss: 1.3973 - acc: 0.3888\n",
      "Epoch 00123: val_loss did not improve from 1.45231\n",
      "468/468 [==============================] - 3s 6ms/step - loss: 1.3985 - acc: 0.3889 - val_loss: 1.5116 - val_acc: 0.4038\n",
      "Epoch 124/500\n",
      "464/468 [============================>.] - ETA: 0s - loss: 1.3940 - acc: 0.4041\n",
      "Epoch 00124: val_loss did not improve from 1.45231\n",
      "468/468 [==============================] - 3s 6ms/step - loss: 1.3919 - acc: 0.4049 - val_loss: 1.5239 - val_acc: 0.4038\n",
      "Epoch 125/500\n",
      "462/468 [============================>.] - ETA: 0s - loss: 1.3988 - acc: 0.3929\n",
      "Epoch 00125: val_loss did not improve from 1.45231\n",
      "468/468 [==============================] - 3s 6ms/step - loss: 1.3972 - acc: 0.3942 - val_loss: 1.5340 - val_acc: 0.3750\n",
      "Epoch 126/500\n",
      "467/468 [============================>.] - ETA: 0s - loss: 1.3996 - acc: 0.3908\n",
      "Epoch 00126: val_loss did not improve from 1.45231\n",
      "468/468 [==============================] - 3s 6ms/step - loss: 1.3986 - acc: 0.3921 - val_loss: 1.5441 - val_acc: 0.3750\n",
      "Epoch 127/500\n",
      "467/468 [============================>.] - ETA: 0s - loss: 1.4021 - acc: 0.4047\n",
      "Epoch 00127: val_loss did not improve from 1.45231\n",
      "468/468 [==============================] - 3s 6ms/step - loss: 1.4017 - acc: 0.4049 - val_loss: 1.5090 - val_acc: 0.3462\n",
      "Epoch 128/500\n",
      "464/468 [============================>.] - ETA: 0s - loss: 1.3930 - acc: 0.4041\n",
      "Epoch 00128: val_loss did not improve from 1.45231\n",
      "468/468 [==============================] - 3s 6ms/step - loss: 1.3945 - acc: 0.4038 - val_loss: 1.5112 - val_acc: 0.3750\n",
      "Epoch 129/500\n",
      "459/468 [============================>.] - ETA: 0s - loss: 1.3945 - acc: 0.3867\n",
      "Epoch 00129: val_loss improved from 1.45231 to 1.43601, saving model to model/checkpoint.h5\n",
      "468/468 [==============================] - 3s 6ms/step - loss: 1.3973 - acc: 0.3878 - val_loss: 1.4360 - val_acc: 0.4231\n",
      "Epoch 130/500\n",
      "463/468 [============================>.] - ETA: 0s - loss: 1.4023 - acc: 0.4060\n",
      "Epoch 00130: val_loss did not improve from 1.43601\n",
      "468/468 [==============================] - 3s 6ms/step - loss: 1.3995 - acc: 0.4092 - val_loss: 1.5005 - val_acc: 0.3462\n",
      "Epoch 131/500\n",
      "468/468 [==============================] - ETA: 0s - loss: 1.3952 - acc: 0.4060\n",
      "Epoch 00131: val_loss did not improve from 1.43601\n",
      "468/468 [==============================] - 3s 6ms/step - loss: 1.3952 - acc: 0.4060 - val_loss: 1.5076 - val_acc: 0.3654\n",
      "Epoch 132/500\n",
      "464/468 [============================>.] - ETA: 0s - loss: 1.3852 - acc: 0.4073\n",
      "Epoch 00132: val_loss did not improve from 1.43601\n",
      "468/468 [==============================] - 3s 6ms/step - loss: 1.3879 - acc: 0.4081 - val_loss: 1.5373 - val_acc: 0.4038\n",
      "Epoch 133/500\n",
      "465/468 [============================>.] - ETA: 0s - loss: 1.3947 - acc: 0.3968\n",
      "Epoch 00133: val_loss did not improve from 1.43601\n",
      "468/468 [==============================] - 3s 6ms/step - loss: 1.3915 - acc: 0.3996 - val_loss: 1.5547 - val_acc: 0.3269\n",
      "Epoch 134/500\n",
      "462/468 [============================>.] - ETA: 0s - loss: 1.3899 - acc: 0.4210\n",
      "Epoch 00134: val_loss did not improve from 1.43601\n",
      "468/468 [==============================] - 3s 6ms/step - loss: 1.3911 - acc: 0.4209 - val_loss: 1.5034 - val_acc: 0.3750\n",
      "Epoch 135/500\n",
      "463/468 [============================>.] - ETA: 0s - loss: 1.3894 - acc: 0.4158\n",
      "Epoch 00135: val_loss did not improve from 1.43601\n",
      "468/468 [==============================] - 3s 6ms/step - loss: 1.3887 - acc: 0.4145 - val_loss: 1.5252 - val_acc: 0.3942\n",
      "Epoch 136/500\n",
      "461/468 [============================>.] - ETA: 0s - loss: 1.3850 - acc: 0.3991\n",
      "Epoch 00136: val_loss did not improve from 1.43601\n",
      "468/468 [==============================] - 3s 6ms/step - loss: 1.3886 - acc: 0.3996 - val_loss: 1.4671 - val_acc: 0.3846\n",
      "Epoch 137/500\n",
      "466/468 [============================>.] - ETA: 0s - loss: 1.3881 - acc: 0.3981\n",
      "Epoch 00137: val_loss did not improve from 1.43601\n",
      "468/468 [==============================] - 3s 6ms/step - loss: 1.3906 - acc: 0.3974 - val_loss: 1.4846 - val_acc: 0.4135\n",
      "Epoch 138/500\n",
      "468/468 [==============================] - ETA: 0s - loss: 1.3884 - acc: 0.4156\n",
      "Epoch 00138: val_loss did not improve from 1.43601\n",
      "468/468 [==============================] - 3s 6ms/step - loss: 1.3884 - acc: 0.4156 - val_loss: 1.5158 - val_acc: 0.3558\n",
      "Epoch 139/500\n",
      "468/468 [==============================] - ETA: 0s - loss: 1.3866 - acc: 0.3985\n",
      "Epoch 00139: val_loss did not improve from 1.43601\n",
      "468/468 [==============================] - 3s 6ms/step - loss: 1.3866 - acc: 0.3985 - val_loss: 1.4951 - val_acc: 0.4038\n",
      "Epoch 140/500\n",
      "459/468 [============================>.] - ETA: 0s - loss: 1.3889 - acc: 0.4041\n",
      "Epoch 00140: val_loss did not improve from 1.43601\n",
      "468/468 [==============================] - 3s 6ms/step - loss: 1.3866 - acc: 0.4060 - val_loss: 1.5356 - val_acc: 0.3942\n",
      "Epoch 141/500\n",
      "464/468 [============================>.] - ETA: 0s - loss: 1.3831 - acc: 0.4235\n",
      "Epoch 00141: val_loss did not improve from 1.43601\n",
      "468/468 [==============================] - 3s 6ms/step - loss: 1.3832 - acc: 0.4241 - val_loss: 1.5678 - val_acc: 0.3846\n",
      "Epoch 142/500\n",
      "465/468 [============================>.] - ETA: 0s - loss: 1.3879 - acc: 0.4108\n",
      "Epoch 00142: val_loss did not improve from 1.43601\n",
      "468/468 [==============================] - 3s 6ms/step - loss: 1.3847 - acc: 0.4124 - val_loss: 1.5056 - val_acc: 0.3462\n",
      "Epoch 143/500\n",
      "462/468 [============================>.] - ETA: 0s - loss: 1.3765 - acc: 0.4307\n",
      "Epoch 00143: val_loss did not improve from 1.43601\n",
      "468/468 [==============================] - 3s 6ms/step - loss: 1.3775 - acc: 0.4316 - val_loss: 1.4526 - val_acc: 0.4423\n",
      "Epoch 144/500\n",
      "468/468 [==============================] - ETA: 0s - loss: 1.3835 - acc: 0.3985\n",
      "Epoch 00144: val_loss did not improve from 1.43601\n",
      "468/468 [==============================] - 3s 6ms/step - loss: 1.3835 - acc: 0.3985 - val_loss: 1.5516 - val_acc: 0.3365\n",
      "Epoch 145/500\n",
      "461/468 [============================>.] - ETA: 0s - loss: 1.3756 - acc: 0.4111\n",
      "Epoch 00145: val_loss did not improve from 1.43601\n",
      "468/468 [==============================] - 3s 6ms/step - loss: 1.3794 - acc: 0.4113 - val_loss: 1.4807 - val_acc: 0.3942\n",
      "Epoch 146/500\n",
      "467/468 [============================>.] - ETA: 0s - loss: 1.3902 - acc: 0.3726\n",
      "Epoch 00146: val_loss did not improve from 1.43601\n",
      "468/468 [==============================] - 3s 6ms/step - loss: 1.3896 - acc: 0.3739 - val_loss: 1.5423 - val_acc: 0.3462\n",
      "Epoch 147/500\n",
      "460/468 [============================>.] - ETA: 0s - loss: 1.3860 - acc: 0.3924\n",
      "Epoch 00147: val_loss did not improve from 1.43601\n",
      "468/468 [==============================] - 3s 6ms/step - loss: 1.3868 - acc: 0.3921 - val_loss: 1.5144 - val_acc: 0.4038\n",
      "Epoch 148/500\n",
      "463/468 [============================>.] - ETA: 0s - loss: 1.3837 - acc: 0.3942\n",
      "Epoch 00148: val_loss did not improve from 1.43601\n",
      "468/468 [==============================] - 3s 6ms/step - loss: 1.3851 - acc: 0.3953 - val_loss: 1.4772 - val_acc: 0.4038\n",
      "Epoch 149/500\n",
      "459/468 [============================>.] - ETA: 0s - loss: 1.3755 - acc: 0.4161\n",
      "Epoch 00149: val_loss did not improve from 1.43601\n",
      "468/468 [==============================] - 3s 6ms/step - loss: 1.3814 - acc: 0.4167 - val_loss: 1.4996 - val_acc: 0.3558\n",
      "Epoch 150/500\n",
      "468/468 [==============================] - ETA: 0s - loss: 1.3870 - acc: 0.3985\n",
      "Epoch 00150: val_loss did not improve from 1.43601\n",
      "468/468 [==============================] - 3s 6ms/step - loss: 1.3870 - acc: 0.3985 - val_loss: 1.5367 - val_acc: 0.3942\n",
      "Epoch 151/500\n",
      "460/468 [============================>.] - ETA: 0s - loss: 1.3840 - acc: 0.3989\n",
      "Epoch 00151: val_loss did not improve from 1.43601\n",
      "468/468 [==============================] - 3s 6ms/step - loss: 1.3826 - acc: 0.3996 - val_loss: 1.4917 - val_acc: 0.4038\n",
      "Epoch 152/500\n",
      "467/468 [============================>.] - ETA: 0s - loss: 1.3819 - acc: 0.4165\n",
      "Epoch 00152: val_loss did not improve from 1.43601\n",
      "468/468 [==============================] - 3s 6ms/step - loss: 1.3802 - acc: 0.4177 - val_loss: 1.5535 - val_acc: 0.3942\n",
      "Epoch 153/500\n",
      "462/468 [============================>.] - ETA: 0s - loss: 1.3768 - acc: 0.4015\n",
      "Epoch 00153: val_loss did not improve from 1.43601\n",
      "468/468 [==============================] - 3s 6ms/step - loss: 1.3800 - acc: 0.4017 - val_loss: 1.4948 - val_acc: 0.3365\n",
      "Epoch 154/500\n",
      "460/468 [============================>.] - ETA: 0s - loss: 1.3793 - acc: 0.3924\n",
      "Epoch 00154: val_loss did not improve from 1.43601\n",
      "468/468 [==============================] - 3s 6ms/step - loss: 1.3815 - acc: 0.3921 - val_loss: 1.5424 - val_acc: 0.4038\n",
      "Epoch 155/500\n",
      "465/468 [============================>.] - ETA: 0s - loss: 1.3854 - acc: 0.4032\n",
      "Epoch 00155: val_loss did not improve from 1.43601\n",
      "468/468 [==============================] - 3s 6ms/step - loss: 1.3818 - acc: 0.4038 - val_loss: 1.5076 - val_acc: 0.3942\n",
      "Epoch 156/500\n",
      "460/468 [============================>.] - ETA: 0s - loss: 1.3775 - acc: 0.3967\n",
      "Epoch 00156: val_loss did not improve from 1.43601\n",
      "468/468 [==============================] - 3s 6ms/step - loss: 1.3792 - acc: 0.3932 - val_loss: 1.5303 - val_acc: 0.3462\n",
      "Epoch 157/500\n",
      "460/468 [============================>.] - ETA: 0s - loss: 1.3772 - acc: 0.4054\n",
      "Epoch 00157: val_loss did not improve from 1.43601\n",
      "468/468 [==============================] - 3s 6ms/step - loss: 1.3852 - acc: 0.4006 - val_loss: 1.5193 - val_acc: 0.3654\n",
      "Epoch 158/500\n",
      "461/468 [============================>.] - ETA: 0s - loss: 1.3754 - acc: 0.4132\n",
      "Epoch 00158: val_loss did not improve from 1.43601\n",
      "468/468 [==============================] - 3s 6ms/step - loss: 1.3760 - acc: 0.4124 - val_loss: 1.4669 - val_acc: 0.3269\n",
      "Epoch 159/500\n",
      "460/468 [============================>.] - ETA: 0s - loss: 1.3790 - acc: 0.4130\n",
      "Epoch 00159: val_loss did not improve from 1.43601\n",
      "468/468 [==============================] - 3s 6ms/step - loss: 1.3809 - acc: 0.4071 - val_loss: 1.5435 - val_acc: 0.3558\n",
      "Epoch 160/500\n",
      "467/468 [============================>.] - ETA: 0s - loss: 1.3772 - acc: 0.4176\n",
      "Epoch 00160: val_loss did not improve from 1.43601\n",
      "468/468 [==============================] - 3s 6ms/step - loss: 1.3765 - acc: 0.4177 - val_loss: 1.5391 - val_acc: 0.3654\n",
      "Epoch 161/500\n",
      "461/468 [============================>.] - ETA: 0s - loss: 1.3758 - acc: 0.4111\n",
      "Epoch 00161: val_loss did not improve from 1.43601\n",
      "468/468 [==============================] - 3s 6ms/step - loss: 1.3775 - acc: 0.4092 - val_loss: 1.5428 - val_acc: 0.3365\n",
      "Epoch 162/500\n",
      "460/468 [============================>.] - ETA: 0s - loss: 1.3772 - acc: 0.4174\n",
      "Epoch 00162: val_loss did not improve from 1.43601\n",
      "468/468 [==============================] - 3s 6ms/step - loss: 1.3764 - acc: 0.4199 - val_loss: 1.5280 - val_acc: 0.4135\n",
      "Epoch 163/500\n",
      "466/468 [============================>.] - ETA: 0s - loss: 1.3754 - acc: 0.4109\n",
      "Epoch 00163: val_loss did not improve from 1.43601\n",
      "468/468 [==============================] - 3s 6ms/step - loss: 1.3770 - acc: 0.4103 - val_loss: 1.5590 - val_acc: 0.3365\n",
      "Epoch 164/500\n",
      "462/468 [============================>.] - ETA: 0s - loss: 1.3738 - acc: 0.4037\n",
      "Epoch 00164: val_loss did not improve from 1.43601\n",
      "468/468 [==============================] - 3s 6ms/step - loss: 1.3731 - acc: 0.4028 - val_loss: 1.4779 - val_acc: 0.3846\n",
      "Epoch 165/500\n",
      "466/468 [============================>.] - ETA: 0s - loss: 1.3719 - acc: 0.4270\n",
      "Epoch 00165: val_loss did not improve from 1.43601\n",
      "468/468 [==============================] - 3s 6ms/step - loss: 1.3705 - acc: 0.4274 - val_loss: 1.4818 - val_acc: 0.3462\n",
      "Epoch 166/500\n",
      "460/468 [============================>.] - ETA: 0s - loss: 1.3823 - acc: 0.3989\n",
      "Epoch 00166: val_loss did not improve from 1.43601\n",
      "468/468 [==============================] - 3s 6ms/step - loss: 1.3782 - acc: 0.4017 - val_loss: 1.5214 - val_acc: 0.3846\n",
      "Epoch 167/500\n",
      "467/468 [============================>.] - ETA: 0s - loss: 1.3719 - acc: 0.4004\n",
      "Epoch 00167: val_loss did not improve from 1.43601\n",
      "468/468 [==============================] - 3s 6ms/step - loss: 1.3723 - acc: 0.3996 - val_loss: 1.5440 - val_acc: 0.3750\n",
      "Epoch 168/500\n",
      "466/468 [============================>.] - ETA: 0s - loss: 1.3766 - acc: 0.4056\n",
      "Epoch 00168: val_loss did not improve from 1.43601\n",
      "468/468 [==============================] - 3s 6ms/step - loss: 1.3775 - acc: 0.4049 - val_loss: 1.4765 - val_acc: 0.3942\n",
      "Epoch 169/500\n",
      "460/468 [============================>.] - ETA: 0s - loss: 1.3686 - acc: 0.4326\n",
      "Epoch 00169: val_loss did not improve from 1.43601\n",
      "468/468 [==============================] - 3s 6ms/step - loss: 1.3689 - acc: 0.4338 - val_loss: 1.5698 - val_acc: 0.3654\n",
      "Epoch 170/500\n",
      "466/468 [============================>.] - ETA: 0s - loss: 1.3726 - acc: 0.4131\n",
      "Epoch 00170: val_loss did not improve from 1.43601\n",
      "468/468 [==============================] - 3s 6ms/step - loss: 1.3729 - acc: 0.4124 - val_loss: 1.5083 - val_acc: 0.3942\n",
      "Epoch 171/500\n",
      "466/468 [============================>.] - ETA: 0s - loss: 1.3771 - acc: 0.4195\n",
      "Epoch 00171: val_loss did not improve from 1.43601\n",
      "468/468 [==============================] - 3s 6ms/step - loss: 1.3764 - acc: 0.4209 - val_loss: 1.5138 - val_acc: 0.3846\n",
      "Epoch 172/500\n",
      "461/468 [============================>.] - ETA: 0s - loss: 1.3668 - acc: 0.4230\n",
      "Epoch 00172: val_loss did not improve from 1.43601\n",
      "468/468 [==============================] - 3s 6ms/step - loss: 1.3745 - acc: 0.4188 - val_loss: 1.5192 - val_acc: 0.3654\n",
      "Epoch 173/500\n",
      "466/468 [============================>.] - ETA: 0s - loss: 1.3739 - acc: 0.3959\n",
      "Epoch 00173: val_loss did not improve from 1.43601\n",
      "468/468 [==============================] - 3s 6ms/step - loss: 1.3748 - acc: 0.3964 - val_loss: 1.5424 - val_acc: 0.3462\n",
      "Epoch 174/500\n",
      "468/468 [==============================] - ETA: 0s - loss: 1.3810 - acc: 0.4028\n",
      "Epoch 00174: val_loss did not improve from 1.43601\n",
      "468/468 [==============================] - 3s 6ms/step - loss: 1.3810 - acc: 0.4028 - val_loss: 1.4993 - val_acc: 0.3750\n",
      "Epoch 175/500\n",
      "459/468 [============================>.] - ETA: 0s - loss: 1.3781 - acc: 0.4183\n",
      "Epoch 00175: val_loss did not improve from 1.43601\n",
      "468/468 [==============================] - 3s 6ms/step - loss: 1.3773 - acc: 0.4199 - val_loss: 1.5498 - val_acc: 0.3654\n",
      "Epoch 176/500\n",
      "468/468 [==============================] - ETA: 0s - loss: 1.3706 - acc: 0.4252\n",
      "Epoch 00176: val_loss did not improve from 1.43601\n",
      "468/468 [==============================] - 3s 6ms/step - loss: 1.3706 - acc: 0.4252 - val_loss: 1.5555 - val_acc: 0.3365\n",
      "Epoch 177/500\n",
      "461/468 [============================>.] - ETA: 0s - loss: 1.3722 - acc: 0.4241\n",
      "Epoch 00177: val_loss did not improve from 1.43601\n",
      "468/468 [==============================] - 3s 6ms/step - loss: 1.3703 - acc: 0.4241 - val_loss: 1.5356 - val_acc: 0.3750\n",
      "Epoch 178/500\n",
      "468/468 [==============================] - ETA: 0s - loss: 1.3791 - acc: 0.4113\n",
      "Epoch 00178: val_loss did not improve from 1.43601\n",
      "468/468 [==============================] - 3s 6ms/step - loss: 1.3791 - acc: 0.4113 - val_loss: 1.5222 - val_acc: 0.3846\n",
      "Epoch 179/500\n",
      "464/468 [============================>.] - ETA: 0s - loss: 1.3692 - acc: 0.4127\n",
      "Epoch 00179: val_loss did not improve from 1.43601\n",
      "468/468 [==============================] - 3s 6ms/step - loss: 1.3672 - acc: 0.4145 - val_loss: 1.4791 - val_acc: 0.4135\n",
      "Epoch 180/500\n",
      "468/468 [==============================] - ETA: 0s - loss: 1.3694 - acc: 0.4145\n",
      "Epoch 00180: val_loss did not improve from 1.43601\n",
      "468/468 [==============================] - 3s 6ms/step - loss: 1.3694 - acc: 0.4145 - val_loss: 1.5291 - val_acc: 0.3462\n",
      "Epoch 181/500\n",
      "467/468 [============================>.] - ETA: 0s - loss: 1.3745 - acc: 0.4186\n",
      "Epoch 00181: val_loss did not improve from 1.43601\n",
      "468/468 [==============================] - 3s 6ms/step - loss: 1.3738 - acc: 0.4188 - val_loss: 1.5326 - val_acc: 0.3750\n",
      "Epoch 182/500\n",
      "464/468 [============================>.] - ETA: 0s - loss: 1.3697 - acc: 0.4159\n",
      "Epoch 00182: val_loss did not improve from 1.43601\n",
      "468/468 [==============================] - 3s 6ms/step - loss: 1.3685 - acc: 0.4156 - val_loss: 1.4969 - val_acc: 0.3942\n",
      "Epoch 183/500\n",
      "468/468 [==============================] - ETA: 0s - loss: 1.3659 - acc: 0.4199\n",
      "Epoch 00183: val_loss did not improve from 1.43601\n",
      "468/468 [==============================] - 3s 6ms/step - loss: 1.3659 - acc: 0.4199 - val_loss: 1.4500 - val_acc: 0.4231\n",
      "Epoch 184/500\n",
      "463/468 [============================>.] - ETA: 0s - loss: 1.3643 - acc: 0.4158\n",
      "Epoch 00184: val_loss did not improve from 1.43601\n",
      "468/468 [==============================] - 3s 6ms/step - loss: 1.3677 - acc: 0.4135 - val_loss: 1.5429 - val_acc: 0.4038\n",
      "Epoch 185/500\n",
      "467/468 [============================>.] - ETA: 0s - loss: 1.3668 - acc: 0.4090\n",
      "Epoch 00185: val_loss did not improve from 1.43601\n",
      "468/468 [==============================] - 3s 6ms/step - loss: 1.3658 - acc: 0.4092 - val_loss: 1.5369 - val_acc: 0.3942\n",
      "Epoch 186/500\n",
      "461/468 [============================>.] - ETA: 0s - loss: 1.3680 - acc: 0.4121\n",
      "Epoch 00186: val_loss did not improve from 1.43601\n",
      "468/468 [==============================] - 3s 6ms/step - loss: 1.3719 - acc: 0.4124 - val_loss: 1.5547 - val_acc: 0.3750\n",
      "Epoch 187/500\n",
      "461/468 [============================>.] - ETA: 0s - loss: 1.3698 - acc: 0.4154\n",
      "Epoch 00187: val_loss did not improve from 1.43601\n",
      "468/468 [==============================] - 3s 6ms/step - loss: 1.3710 - acc: 0.4124 - val_loss: 1.5451 - val_acc: 0.4038\n",
      "Epoch 188/500\n",
      "466/468 [============================>.] - ETA: 0s - loss: 1.3735 - acc: 0.4024\n",
      "Epoch 00188: val_loss did not improve from 1.43601\n",
      "468/468 [==============================] - 3s 6ms/step - loss: 1.3746 - acc: 0.4028 - val_loss: 1.5256 - val_acc: 0.3462\n",
      "Epoch 189/500\n",
      "468/468 [==============================] - ETA: 0s - loss: 1.3722 - acc: 0.3942\n",
      "Epoch 00189: val_loss did not improve from 1.43601\n",
      "468/468 [==============================] - 3s 6ms/step - loss: 1.3722 - acc: 0.3942 - val_loss: 1.5080 - val_acc: 0.3750\n",
      "Epoch 190/500\n",
      "463/468 [============================>.] - ETA: 0s - loss: 1.3624 - acc: 0.4104\n",
      "Epoch 00190: val_loss did not improve from 1.43601\n",
      "468/468 [==============================] - 3s 6ms/step - loss: 1.3632 - acc: 0.4103 - val_loss: 1.5204 - val_acc: 0.3846\n",
      "Epoch 191/500\n",
      "461/468 [============================>.] - ETA: 0s - loss: 1.3671 - acc: 0.4230\n",
      "Epoch 00191: val_loss did not improve from 1.43601\n",
      "468/468 [==============================] - 3s 6ms/step - loss: 1.3707 - acc: 0.4252 - val_loss: 1.5161 - val_acc: 0.3558\n",
      "Epoch 192/500\n",
      "460/468 [============================>.] - ETA: 0s - loss: 1.3731 - acc: 0.4120\n",
      "Epoch 00192: val_loss did not improve from 1.43601\n",
      "468/468 [==============================] - 3s 6ms/step - loss: 1.3682 - acc: 0.4145 - val_loss: 1.5147 - val_acc: 0.3558\n",
      "Epoch 193/500\n",
      "463/468 [============================>.] - ETA: 0s - loss: 1.3706 - acc: 0.4060\n",
      "Epoch 00193: val_loss did not improve from 1.43601\n",
      "468/468 [==============================] - 3s 6ms/step - loss: 1.3678 - acc: 0.4092 - val_loss: 1.4983 - val_acc: 0.3654\n",
      "Epoch 194/500\n",
      "468/468 [==============================] - ETA: 0s - loss: 1.3647 - acc: 0.4327\n",
      "Epoch 00194: val_loss did not improve from 1.43601\n",
      "468/468 [==============================] - 3s 6ms/step - loss: 1.3647 - acc: 0.4327 - val_loss: 1.5160 - val_acc: 0.4135\n",
      "Epoch 195/500\n",
      "465/468 [============================>.] - ETA: 0s - loss: 1.3666 - acc: 0.4269\n",
      "Epoch 00195: val_loss did not improve from 1.43601\n",
      "468/468 [==============================] - 3s 6ms/step - loss: 1.3664 - acc: 0.4274 - val_loss: 1.5676 - val_acc: 0.3750\n",
      "Epoch 196/500\n",
      "461/468 [============================>.] - ETA: 0s - loss: 1.3691 - acc: 0.4219\n",
      "Epoch 00196: val_loss did not improve from 1.43601\n",
      "468/468 [==============================] - 3s 6ms/step - loss: 1.3689 - acc: 0.4231 - val_loss: 1.4931 - val_acc: 0.3365\n",
      "Epoch 197/500\n",
      "465/468 [============================>.] - ETA: 0s - loss: 1.3600 - acc: 0.4419\n",
      "Epoch 00197: val_loss did not improve from 1.43601\n",
      "468/468 [==============================] - 3s 6ms/step - loss: 1.3566 - acc: 0.4444 - val_loss: 1.4661 - val_acc: 0.4327\n",
      "Epoch 198/500\n",
      "465/468 [============================>.] - ETA: 0s - loss: 1.3658 - acc: 0.4140\n",
      "Epoch 00198: val_loss did not improve from 1.43601\n",
      "468/468 [==============================] - 3s 7ms/step - loss: 1.3646 - acc: 0.4156 - val_loss: 1.4965 - val_acc: 0.4231\n",
      "Epoch 199/500\n",
      "467/468 [============================>.] - ETA: 0s - loss: 1.3630 - acc: 0.4272\n",
      "Epoch 00199: val_loss did not improve from 1.43601\n",
      "468/468 [==============================] - 3s 6ms/step - loss: 1.3607 - acc: 0.4284 - val_loss: 1.4725 - val_acc: 0.4135\n",
      "Epoch 200/500\n",
      "462/468 [============================>.] - ETA: 0s - loss: 1.3674 - acc: 0.4091\n",
      "Epoch 00200: val_loss did not improve from 1.43601\n",
      "468/468 [==============================] - 3s 6ms/step - loss: 1.3639 - acc: 0.4113 - val_loss: 1.5387 - val_acc: 0.3846\n",
      "Epoch 201/500\n",
      "467/468 [============================>.] - ETA: 0s - loss: 1.3603 - acc: 0.4293\n",
      "Epoch 00201: val_loss did not improve from 1.43601\n",
      "468/468 [==============================] - 3s 6ms/step - loss: 1.3602 - acc: 0.4295 - val_loss: 1.5505 - val_acc: 0.4135\n",
      "Epoch 202/500\n",
      "459/468 [============================>.] - ETA: 0s - loss: 1.3608 - acc: 0.4216\n",
      "Epoch 00202: val_loss did not improve from 1.43601\n",
      "468/468 [==============================] - 3s 6ms/step - loss: 1.3659 - acc: 0.4177 - val_loss: 1.5294 - val_acc: 0.3846\n",
      "Epoch 203/500\n",
      "466/468 [============================>.] - ETA: 0s - loss: 1.3656 - acc: 0.4067\n",
      "Epoch 00203: val_loss did not improve from 1.43601\n",
      "468/468 [==============================] - 3s 6ms/step - loss: 1.3657 - acc: 0.4071 - val_loss: 1.5050 - val_acc: 0.3558\n",
      "Epoch 204/500\n",
      "465/468 [============================>.] - ETA: 0s - loss: 1.3599 - acc: 0.4409\n",
      "Epoch 00204: val_loss did not improve from 1.43601\n",
      "468/468 [==============================] - 3s 6ms/step - loss: 1.3596 - acc: 0.4412 - val_loss: 1.5301 - val_acc: 0.3750\n",
      "Epoch 205/500\n",
      "467/468 [============================>.] - ETA: 0s - loss: 1.3636 - acc: 0.4197\n",
      "Epoch 00205: val_loss did not improve from 1.43601\n",
      "468/468 [==============================] - 3s 6ms/step - loss: 1.3614 - acc: 0.4209 - val_loss: 1.5282 - val_acc: 0.3942\n",
      "Epoch 206/500\n",
      "459/468 [============================>.] - ETA: 0s - loss: 1.3605 - acc: 0.4270\n",
      "Epoch 00206: val_loss did not improve from 1.43601\n",
      "468/468 [==============================] - 3s 7ms/step - loss: 1.3612 - acc: 0.4241 - val_loss: 1.5042 - val_acc: 0.3654\n",
      "Epoch 207/500\n",
      "467/468 [============================>.] - ETA: 0s - loss: 1.3602 - acc: 0.4240\n",
      "Epoch 00207: val_loss did not improve from 1.43601\n",
      "468/468 [==============================] - 3s 6ms/step - loss: 1.3600 - acc: 0.4231 - val_loss: 1.5358 - val_acc: 0.4327\n",
      "Epoch 208/500\n",
      "467/468 [============================>.] - ETA: 0s - loss: 1.3620 - acc: 0.4165\n",
      "Epoch 00208: val_loss did not improve from 1.43601\n",
      "468/468 [==============================] - 3s 6ms/step - loss: 1.3624 - acc: 0.4167 - val_loss: 1.5508 - val_acc: 0.3942\n",
      "Epoch 209/500\n",
      "459/468 [============================>.] - ETA: 0s - loss: 1.3670 - acc: 0.4237\n",
      "Epoch 00209: val_loss did not improve from 1.43601\n",
      "468/468 [==============================] - 3s 7ms/step - loss: 1.3656 - acc: 0.4231 - val_loss: 1.4827 - val_acc: 0.4327\n",
      "Epoch 210/500\n",
      "460/468 [============================>.] - ETA: 0s - loss: 1.3588 - acc: 0.4152\n",
      "Epoch 00210: val_loss did not improve from 1.43601\n",
      "468/468 [==============================] - 3s 6ms/step - loss: 1.3556 - acc: 0.4156 - val_loss: 1.5976 - val_acc: 0.3077\n",
      "Epoch 211/500\n",
      "465/468 [============================>.] - ETA: 0s - loss: 1.3626 - acc: 0.4301\n",
      "Epoch 00211: val_loss did not improve from 1.43601\n",
      "468/468 [==============================] - 3s 6ms/step - loss: 1.3619 - acc: 0.4295 - val_loss: 1.4920 - val_acc: 0.4231\n",
      "Epoch 212/500\n",
      "461/468 [============================>.] - ETA: 0s - loss: 1.3604 - acc: 0.4371\n",
      "Epoch 00212: val_loss did not improve from 1.43601\n",
      "468/468 [==============================] - 3s 6ms/step - loss: 1.3578 - acc: 0.4380 - val_loss: 1.5696 - val_acc: 0.3942\n",
      "Epoch 213/500\n",
      "459/468 [============================>.] - ETA: 0s - loss: 1.3611 - acc: 0.4270\n",
      "Epoch 00213: val_loss did not improve from 1.43601\n",
      "468/468 [==============================] - 3s 6ms/step - loss: 1.3587 - acc: 0.4263 - val_loss: 1.4967 - val_acc: 0.3750\n",
      "Epoch 214/500\n",
      "464/468 [============================>.] - ETA: 0s - loss: 1.3660 - acc: 0.4213\n",
      "Epoch 00214: val_loss did not improve from 1.43601\n",
      "468/468 [==============================] - 3s 6ms/step - loss: 1.3640 - acc: 0.4199 - val_loss: 1.5181 - val_acc: 0.4038\n",
      "Epoch 215/500\n",
      "468/468 [==============================] - ETA: 0s - loss: 1.3573 - acc: 0.4295\n",
      "Epoch 00215: val_loss did not improve from 1.43601\n",
      "468/468 [==============================] - 3s 6ms/step - loss: 1.3573 - acc: 0.4295 - val_loss: 1.5928 - val_acc: 0.3654\n",
      "Epoch 216/500\n",
      "463/468 [============================>.] - ETA: 0s - loss: 1.3657 - acc: 0.4158\n",
      "Epoch 00216: val_loss did not improve from 1.43601\n",
      "468/468 [==============================] - 3s 6ms/step - loss: 1.3602 - acc: 0.4188 - val_loss: 1.5279 - val_acc: 0.3942\n",
      "Epoch 217/500\n",
      "459/468 [============================>.] - ETA: 0s - loss: 1.3620 - acc: 0.4281\n",
      "Epoch 00217: val_loss did not improve from 1.43601\n",
      "468/468 [==============================] - 3s 6ms/step - loss: 1.3637 - acc: 0.4263 - val_loss: 1.4955 - val_acc: 0.4038\n",
      "Epoch 218/500\n",
      "459/468 [============================>.] - ETA: 0s - loss: 1.3507 - acc: 0.4292\n",
      "Epoch 00218: val_loss did not improve from 1.43601\n",
      "468/468 [==============================] - 3s 6ms/step - loss: 1.3553 - acc: 0.4252 - val_loss: 1.5503 - val_acc: 0.3942\n",
      "Epoch 219/500\n",
      "460/468 [============================>.] - ETA: 0s - loss: 1.3643 - acc: 0.4293\n",
      "Epoch 00219: val_loss did not improve from 1.43601\n",
      "468/468 [==============================] - 3s 6ms/step - loss: 1.3600 - acc: 0.4284 - val_loss: 1.5434 - val_acc: 0.3462\n",
      "Epoch 220/500\n",
      "468/468 [==============================] - ETA: 0s - loss: 1.3603 - acc: 0.4338\n",
      "Epoch 00220: val_loss did not improve from 1.43601\n",
      "468/468 [==============================] - 3s 6ms/step - loss: 1.3603 - acc: 0.4338 - val_loss: 1.5197 - val_acc: 0.3558\n",
      "Epoch 221/500\n",
      "459/468 [============================>.] - ETA: 0s - loss: 1.3564 - acc: 0.4205\n",
      "Epoch 00221: val_loss did not improve from 1.43601\n",
      "468/468 [==============================] - 3s 6ms/step - loss: 1.3599 - acc: 0.4188 - val_loss: 1.5317 - val_acc: 0.3942\n",
      "Epoch 222/500\n",
      "459/468 [============================>.] - ETA: 0s - loss: 1.3633 - acc: 0.4227\n",
      "Epoch 00222: val_loss did not improve from 1.43601\n",
      "468/468 [==============================] - 3s 6ms/step - loss: 1.3645 - acc: 0.4220 - val_loss: 1.4828 - val_acc: 0.4231\n",
      "Epoch 223/500\n",
      "463/468 [============================>.] - ETA: 0s - loss: 1.3622 - acc: 0.4168\n",
      "Epoch 00223: val_loss did not improve from 1.43601\n",
      "468/468 [==============================] - 3s 6ms/step - loss: 1.3617 - acc: 0.4167 - val_loss: 1.5399 - val_acc: 0.3654\n",
      "Epoch 224/500\n",
      "460/468 [============================>.] - ETA: 0s - loss: 1.3582 - acc: 0.4217\n",
      "Epoch 00224: val_loss did not improve from 1.43601\n",
      "468/468 [==============================] - 3s 6ms/step - loss: 1.3591 - acc: 0.4209 - val_loss: 1.5294 - val_acc: 0.4519\n",
      "Epoch 225/500\n",
      "459/468 [============================>.] - ETA: 0s - loss: 1.3565 - acc: 0.4270\n",
      "Epoch 00225: val_loss did not improve from 1.43601\n",
      "468/468 [==============================] - 3s 6ms/step - loss: 1.3567 - acc: 0.4263 - val_loss: 1.5027 - val_acc: 0.4231\n",
      "Epoch 226/500\n",
      "463/468 [============================>.] - ETA: 0s - loss: 1.3511 - acc: 0.4374\n",
      "Epoch 00226: val_loss did not improve from 1.43601\n",
      "468/468 [==============================] - 3s 6ms/step - loss: 1.3482 - acc: 0.4380 - val_loss: 1.5659 - val_acc: 0.4038\n",
      "Epoch 227/500\n",
      "462/468 [============================>.] - ETA: 0s - loss: 1.3660 - acc: 0.4340\n",
      "Epoch 00227: val_loss did not improve from 1.43601\n",
      "468/468 [==============================] - 3s 6ms/step - loss: 1.3643 - acc: 0.4348 - val_loss: 1.4985 - val_acc: 0.4038\n",
      "Epoch 228/500\n",
      "461/468 [============================>.] - ETA: 0s - loss: 1.3529 - acc: 0.4208\n",
      "Epoch 00228: val_loss did not improve from 1.43601\n",
      "468/468 [==============================] - 3s 6ms/step - loss: 1.3517 - acc: 0.4220 - val_loss: 1.5196 - val_acc: 0.3654\n",
      "Epoch 229/500\n",
      "467/468 [============================>.] - ETA: 0s - loss: 1.3549 - acc: 0.4197\n",
      "Epoch 00229: val_loss did not improve from 1.43601\n",
      "468/468 [==============================] - 3s 6ms/step - loss: 1.3537 - acc: 0.4199 - val_loss: 1.5034 - val_acc: 0.3846\n",
      "Epoch 230/500\n",
      "466/468 [============================>.] - ETA: 0s - loss: 1.3567 - acc: 0.4345\n",
      "Epoch 00230: val_loss did not improve from 1.43601\n",
      "468/468 [==============================] - 3s 6ms/step - loss: 1.3551 - acc: 0.4348 - val_loss: 1.5368 - val_acc: 0.3846\n",
      "Epoch 231/500\n",
      "459/468 [============================>.] - ETA: 0s - loss: 1.3632 - acc: 0.4336\n",
      "Epoch 00231: val_loss did not improve from 1.43601\n",
      "468/468 [==============================] - 3s 6ms/step - loss: 1.3587 - acc: 0.4359 - val_loss: 1.4707 - val_acc: 0.4231\n",
      "Epoch 232/500\n",
      "459/468 [============================>.] - ETA: 0s - loss: 1.3588 - acc: 0.4357\n",
      "Epoch 00232: val_loss did not improve from 1.43601\n",
      "468/468 [==============================] - 3s 6ms/step - loss: 1.3591 - acc: 0.4348 - val_loss: 1.4782 - val_acc: 0.4231\n",
      "Epoch 233/500\n",
      "463/468 [============================>.] - ETA: 0s - loss: 1.3532 - acc: 0.4341\n",
      "Epoch 00233: val_loss did not improve from 1.43601\n",
      "468/468 [==============================] - 3s 6ms/step - loss: 1.3535 - acc: 0.4327 - val_loss: 1.5542 - val_acc: 0.3269\n",
      "Epoch 234/500\n",
      "460/468 [============================>.] - ETA: 0s - loss: 1.3588 - acc: 0.4217\n",
      "Epoch 00234: val_loss did not improve from 1.43601\n",
      "468/468 [==============================] - 3s 7ms/step - loss: 1.3553 - acc: 0.4241 - val_loss: 1.5469 - val_acc: 0.3750\n",
      "Epoch 235/500\n",
      "467/468 [============================>.] - ETA: 0s - loss: 1.3507 - acc: 0.4293\n",
      "Epoch 00235: val_loss did not improve from 1.43601\n",
      "468/468 [==============================] - 3s 6ms/step - loss: 1.3523 - acc: 0.4295 - val_loss: 1.4620 - val_acc: 0.4327\n",
      "Epoch 236/500\n",
      "463/468 [============================>.] - ETA: 0s - loss: 1.3481 - acc: 0.4352\n",
      "Epoch 00236: val_loss did not improve from 1.43601\n",
      "468/468 [==============================] - 3s 6ms/step - loss: 1.3525 - acc: 0.4327 - val_loss: 1.5051 - val_acc: 0.4231\n",
      "Epoch 237/500\n",
      "467/468 [============================>.] - ETA: 0s - loss: 1.3537 - acc: 0.4465\n",
      "Epoch 00237: val_loss did not improve from 1.43601\n",
      "468/468 [==============================] - 3s 6ms/step - loss: 1.3521 - acc: 0.4476 - val_loss: 1.4886 - val_acc: 0.3654\n",
      "Epoch 238/500\n",
      "465/468 [============================>.] - ETA: 0s - loss: 1.3496 - acc: 0.4376\n",
      "Epoch 00238: val_loss did not improve from 1.43601\n",
      "468/468 [==============================] - 3s 6ms/step - loss: 1.3493 - acc: 0.4370 - val_loss: 1.5699 - val_acc: 0.3750\n",
      "Epoch 239/500\n",
      "462/468 [============================>.] - ETA: 0s - loss: 1.3634 - acc: 0.4123\n",
      "Epoch 00239: val_loss did not improve from 1.43601\n",
      "468/468 [==============================] - 3s 6ms/step - loss: 1.3585 - acc: 0.4177 - val_loss: 1.5096 - val_acc: 0.4135\n",
      "Epoch 240/500\n",
      "464/468 [============================>.] - ETA: 0s - loss: 1.3526 - acc: 0.4397\n",
      "Epoch 00240: val_loss did not improve from 1.43601\n",
      "468/468 [==============================] - 3s 6ms/step - loss: 1.3508 - acc: 0.4423 - val_loss: 1.5136 - val_acc: 0.3942\n",
      "Epoch 241/500\n",
      "463/468 [============================>.] - ETA: 0s - loss: 1.3552 - acc: 0.4374\n",
      "Epoch 00241: val_loss did not improve from 1.43601\n",
      "468/468 [==============================] - 3s 6ms/step - loss: 1.3592 - acc: 0.4348 - val_loss: 1.5012 - val_acc: 0.3654\n",
      "Epoch 242/500\n",
      "467/468 [============================>.] - ETA: 0s - loss: 1.3524 - acc: 0.4293\n",
      "Epoch 00242: val_loss did not improve from 1.43601\n",
      "468/468 [==============================] - 3s 6ms/step - loss: 1.3533 - acc: 0.4284 - val_loss: 1.5291 - val_acc: 0.4038\n",
      "Epoch 243/500\n",
      "462/468 [============================>.] - ETA: 0s - loss: 1.3513 - acc: 0.4470\n",
      "Epoch 00243: val_loss did not improve from 1.43601\n",
      "468/468 [==============================] - 3s 6ms/step - loss: 1.3547 - acc: 0.4455 - val_loss: 1.5323 - val_acc: 0.4327\n",
      "Epoch 244/500\n",
      "462/468 [============================>.] - ETA: 0s - loss: 1.3529 - acc: 0.4307\n",
      "Epoch 00244: val_loss did not improve from 1.43601\n",
      "468/468 [==============================] - 3s 6ms/step - loss: 1.3508 - acc: 0.4338 - val_loss: 1.5151 - val_acc: 0.3750\n",
      "Epoch 245/500\n",
      "465/468 [============================>.] - ETA: 0s - loss: 1.3478 - acc: 0.4355\n",
      "Epoch 00245: val_loss did not improve from 1.43601\n",
      "468/468 [==============================] - 3s 6ms/step - loss: 1.3489 - acc: 0.4338 - val_loss: 1.5528 - val_acc: 0.3558\n",
      "Epoch 246/500\n",
      "461/468 [============================>.] - ETA: 0s - loss: 1.3506 - acc: 0.4284\n",
      "Epoch 00246: val_loss did not improve from 1.43601\n",
      "468/468 [==============================] - 3s 6ms/step - loss: 1.3471 - acc: 0.4316 - val_loss: 1.5150 - val_acc: 0.4038\n",
      "Epoch 247/500\n",
      "466/468 [============================>.] - ETA: 0s - loss: 1.3499 - acc: 0.4238\n",
      "Epoch 00247: val_loss did not improve from 1.43601\n",
      "468/468 [==============================] - 3s 6ms/step - loss: 1.3504 - acc: 0.4220 - val_loss: 1.5127 - val_acc: 0.4038\n",
      "Epoch 248/500\n",
      "467/468 [============================>.] - ETA: 0s - loss: 1.3545 - acc: 0.4240\n",
      "Epoch 00248: val_loss did not improve from 1.43601\n",
      "468/468 [==============================] - 3s 6ms/step - loss: 1.3539 - acc: 0.4241 - val_loss: 1.5579 - val_acc: 0.4231\n",
      "Epoch 249/500\n",
      "467/468 [============================>.] - ETA: 0s - loss: 1.3593 - acc: 0.4197\n",
      "Epoch 00249: val_loss did not improve from 1.43601\n",
      "468/468 [==============================] - 3s 6ms/step - loss: 1.3594 - acc: 0.4188 - val_loss: 1.5149 - val_acc: 0.4231\n",
      "Epoch 250/500\n",
      "461/468 [============================>.] - ETA: 0s - loss: 1.3440 - acc: 0.4328\n",
      "Epoch 00250: val_loss did not improve from 1.43601\n",
      "468/468 [==============================] - 3s 6ms/step - loss: 1.3485 - acc: 0.4306 - val_loss: 1.4723 - val_acc: 0.4231\n",
      "Epoch 251/500\n",
      "467/468 [============================>.] - ETA: 0s - loss: 1.3546 - acc: 0.4229\n",
      "Epoch 00251: val_loss did not improve from 1.43601\n",
      "468/468 [==============================] - 3s 6ms/step - loss: 1.3578 - acc: 0.4231 - val_loss: 1.4877 - val_acc: 0.4135\n",
      "Epoch 252/500\n",
      "461/468 [============================>.] - ETA: 0s - loss: 1.3528 - acc: 0.4338\n",
      "Epoch 00252: val_loss did not improve from 1.43601\n",
      "468/468 [==============================] - 3s 6ms/step - loss: 1.3548 - acc: 0.4327 - val_loss: 1.4896 - val_acc: 0.3942\n",
      "Epoch 253/500\n",
      "461/468 [============================>.] - ETA: 0s - loss: 1.3446 - acc: 0.4295\n",
      "Epoch 00253: val_loss did not improve from 1.43601\n",
      "468/468 [==============================] - 3s 6ms/step - loss: 1.3493 - acc: 0.4274 - val_loss: 1.5347 - val_acc: 0.4038\n",
      "Epoch 254/500\n",
      "466/468 [============================>.] - ETA: 0s - loss: 1.3494 - acc: 0.4324\n",
      "Epoch 00254: val_loss did not improve from 1.43601\n",
      "468/468 [==============================] - 3s 6ms/step - loss: 1.3509 - acc: 0.4316 - val_loss: 1.5181 - val_acc: 0.4038\n",
      "Epoch 255/500\n",
      "460/468 [============================>.] - ETA: 0s - loss: 1.3468 - acc: 0.4446\n",
      "Epoch 00255: val_loss did not improve from 1.43601\n",
      "468/468 [==============================] - 3s 6ms/step - loss: 1.3479 - acc: 0.4444 - val_loss: 1.5351 - val_acc: 0.4327\n",
      "Epoch 256/500\n",
      "459/468 [============================>.] - ETA: 0s - loss: 1.3548 - acc: 0.4379\n",
      "Epoch 00256: val_loss did not improve from 1.43601\n",
      "468/468 [==============================] - 3s 6ms/step - loss: 1.3471 - acc: 0.4444 - val_loss: 1.4902 - val_acc: 0.4327\n",
      "Epoch 257/500\n",
      "464/468 [============================>.] - ETA: 0s - loss: 1.3564 - acc: 0.4138\n",
      "Epoch 00257: val_loss did not improve from 1.43601\n",
      "468/468 [==============================] - 3s 6ms/step - loss: 1.3525 - acc: 0.4167 - val_loss: 1.4961 - val_acc: 0.4327\n",
      "Epoch 258/500\n",
      "464/468 [============================>.] - ETA: 0s - loss: 1.3531 - acc: 0.4418\n",
      "Epoch 00258: val_loss did not improve from 1.43601\n",
      "468/468 [==============================] - 3s 6ms/step - loss: 1.3510 - acc: 0.4434 - val_loss: 1.5559 - val_acc: 0.3750\n",
      "Epoch 259/500\n",
      "467/468 [============================>.] - ETA: 0s - loss: 1.3489 - acc: 0.4518\n",
      "Epoch 00259: val_loss did not improve from 1.43601\n",
      "468/468 [==============================] - 3s 6ms/step - loss: 1.3513 - acc: 0.4509 - val_loss: 1.4916 - val_acc: 0.4135\n",
      "Epoch 260/500\n",
      "467/468 [============================>.] - ETA: 0s - loss: 1.3491 - acc: 0.4454\n",
      "Epoch 00260: val_loss did not improve from 1.43601\n",
      "468/468 [==============================] - 3s 6ms/step - loss: 1.3469 - acc: 0.4466 - val_loss: 1.4954 - val_acc: 0.4423\n",
      "Epoch 261/500\n",
      "461/468 [============================>.] - ETA: 0s - loss: 1.3470 - acc: 0.4262\n",
      "Epoch 00261: val_loss did not improve from 1.43601\n",
      "468/468 [==============================] - 3s 6ms/step - loss: 1.3473 - acc: 0.4263 - val_loss: 1.5215 - val_acc: 0.3654\n",
      "Epoch 262/500\n",
      "464/468 [============================>.] - ETA: 0s - loss: 1.3494 - acc: 0.4321\n",
      "Epoch 00262: val_loss did not improve from 1.43601\n",
      "468/468 [==============================] - 3s 6ms/step - loss: 1.3495 - acc: 0.4338 - val_loss: 1.5165 - val_acc: 0.4038\n",
      "Epoch 263/500\n",
      "464/468 [============================>.] - ETA: 0s - loss: 1.3528 - acc: 0.3998\n",
      "Epoch 00263: val_loss did not improve from 1.43601\n",
      "468/468 [==============================] - 3s 6ms/step - loss: 1.3503 - acc: 0.4006 - val_loss: 1.5350 - val_acc: 0.4038\n",
      "Epoch 264/500\n",
      "463/468 [============================>.] - ETA: 0s - loss: 1.3527 - acc: 0.4341\n",
      "Epoch 00264: val_loss did not improve from 1.43601\n",
      "468/468 [==============================] - 3s 6ms/step - loss: 1.3479 - acc: 0.4391 - val_loss: 1.5407 - val_acc: 0.3654\n",
      "Epoch 265/500\n",
      "463/468 [============================>.] - ETA: 0s - loss: 1.3420 - acc: 0.4428\n",
      "Epoch 00265: val_loss did not improve from 1.43601\n",
      "468/468 [==============================] - 3s 6ms/step - loss: 1.3482 - acc: 0.4412 - val_loss: 1.4565 - val_acc: 0.4231\n",
      "Epoch 266/500\n",
      "467/468 [============================>.] - ETA: 0s - loss: 1.3441 - acc: 0.4443\n",
      "Epoch 00266: val_loss did not improve from 1.43601\n",
      "468/468 [==============================] - 3s 6ms/step - loss: 1.3447 - acc: 0.4444 - val_loss: 1.5146 - val_acc: 0.4038\n",
      "Epoch 267/500\n",
      "460/468 [============================>.] - ETA: 0s - loss: 1.3544 - acc: 0.4239\n",
      "Epoch 00267: val_loss did not improve from 1.43601\n",
      "468/468 [==============================] - 3s 6ms/step - loss: 1.3542 - acc: 0.4241 - val_loss: 1.5169 - val_acc: 0.3462\n",
      "Epoch 268/500\n",
      "468/468 [==============================] - ETA: 0s - loss: 1.3481 - acc: 0.4231\n",
      "Epoch 00268: val_loss did not improve from 1.43601\n",
      "468/468 [==============================] - 3s 6ms/step - loss: 1.3481 - acc: 0.4231 - val_loss: 1.4954 - val_acc: 0.3942\n",
      "Epoch 269/500\n",
      "459/468 [============================>.] - ETA: 0s - loss: 1.3553 - acc: 0.4227\n",
      "Epoch 00269: val_loss did not improve from 1.43601\n",
      "468/468 [==============================] - 3s 6ms/step - loss: 1.3510 - acc: 0.4263 - val_loss: 1.4936 - val_acc: 0.4423\n",
      "Epoch 270/500\n",
      "463/468 [============================>.] - ETA: 0s - loss: 1.3480 - acc: 0.4330\n",
      "Epoch 00270: val_loss did not improve from 1.43601\n",
      "468/468 [==============================] - 3s 6ms/step - loss: 1.3436 - acc: 0.4338 - val_loss: 1.5576 - val_acc: 0.3942\n",
      "Epoch 271/500\n",
      "459/468 [============================>.] - ETA: 0s - loss: 1.3523 - acc: 0.4325\n",
      "Epoch 00271: val_loss did not improve from 1.43601\n",
      "468/468 [==============================] - 3s 6ms/step - loss: 1.3479 - acc: 0.4348 - val_loss: 1.5072 - val_acc: 0.4135\n",
      "Epoch 272/500\n",
      "468/468 [==============================] - ETA: 0s - loss: 1.3448 - acc: 0.4167\n",
      "Epoch 00272: val_loss did not improve from 1.43601\n",
      "468/468 [==============================] - 3s 6ms/step - loss: 1.3448 - acc: 0.4167 - val_loss: 1.5732 - val_acc: 0.4135\n",
      "Epoch 273/500\n",
      "461/468 [============================>.] - ETA: 0s - loss: 1.3451 - acc: 0.4295\n",
      "Epoch 00273: val_loss did not improve from 1.43601\n",
      "468/468 [==============================] - 3s 6ms/step - loss: 1.3440 - acc: 0.4306 - val_loss: 1.5843 - val_acc: 0.3654\n",
      "Epoch 274/500\n",
      "462/468 [============================>.] - ETA: 0s - loss: 1.3500 - acc: 0.4448\n",
      "Epoch 00274: val_loss did not improve from 1.43601\n",
      "468/468 [==============================] - 3s 6ms/step - loss: 1.3433 - acc: 0.4487 - val_loss: 1.4879 - val_acc: 0.3942\n",
      "Epoch 275/500\n",
      "464/468 [============================>.] - ETA: 0s - loss: 1.3435 - acc: 0.4504\n",
      "Epoch 00275: val_loss did not improve from 1.43601\n",
      "468/468 [==============================] - 3s 6ms/step - loss: 1.3462 - acc: 0.4498 - val_loss: 1.5121 - val_acc: 0.3558\n",
      "Epoch 276/500\n",
      "462/468 [============================>.] - ETA: 0s - loss: 1.3575 - acc: 0.4470\n",
      "Epoch 00276: val_loss did not improve from 1.43601\n",
      "468/468 [==============================] - 3s 6ms/step - loss: 1.3493 - acc: 0.4519 - val_loss: 1.4916 - val_acc: 0.4231\n",
      "Epoch 277/500\n",
      "463/468 [============================>.] - ETA: 0s - loss: 1.3366 - acc: 0.4460\n",
      "Epoch 00277: val_loss did not improve from 1.43601\n",
      "468/468 [==============================] - 3s 6ms/step - loss: 1.3438 - acc: 0.4444 - val_loss: 1.4582 - val_acc: 0.4231\n",
      "Epoch 278/500\n",
      "464/468 [============================>.] - ETA: 0s - loss: 1.3418 - acc: 0.4386\n",
      "Epoch 00278: val_loss did not improve from 1.43601\n",
      "468/468 [==============================] - 3s 6ms/step - loss: 1.3455 - acc: 0.4380 - val_loss: 1.5087 - val_acc: 0.4135\n",
      "Epoch 279/500\n",
      "466/468 [============================>.] - ETA: 0s - loss: 1.3450 - acc: 0.4442\n",
      "Epoch 00279: val_loss did not improve from 1.43601\n",
      "468/468 [==============================] - 3s 7ms/step - loss: 1.3430 - acc: 0.4455 - val_loss: 1.5106 - val_acc: 0.4327\n",
      "Epoch 280/500\n",
      "468/468 [==============================] - ETA: 0s - loss: 1.3429 - acc: 0.4199\n",
      "Epoch 00280: val_loss did not improve from 1.43601\n",
      "468/468 [==============================] - 3s 6ms/step - loss: 1.3429 - acc: 0.4199 - val_loss: 1.5167 - val_acc: 0.3462\n",
      "Epoch 281/500\n",
      "467/468 [============================>.] - ETA: 0s - loss: 1.3423 - acc: 0.4325\n",
      "Epoch 00281: val_loss did not improve from 1.43601\n",
      "468/468 [==============================] - 3s 7ms/step - loss: 1.3429 - acc: 0.4316 - val_loss: 1.5371 - val_acc: 0.4038\n",
      "Epoch 282/500\n",
      "460/468 [============================>.] - ETA: 0s - loss: 1.3476 - acc: 0.4337\n",
      "Epoch 00282: val_loss did not improve from 1.43601\n",
      "468/468 [==============================] - 3s 6ms/step - loss: 1.3468 - acc: 0.4348 - val_loss: 1.5146 - val_acc: 0.4038\n",
      "Epoch 283/500\n",
      "459/468 [============================>.] - ETA: 0s - loss: 1.3392 - acc: 0.4499\n",
      "Epoch 00283: val_loss did not improve from 1.43601\n",
      "468/468 [==============================] - 3s 6ms/step - loss: 1.3478 - acc: 0.4487 - val_loss: 1.4887 - val_acc: 0.3942\n",
      "Epoch 284/500\n",
      "460/468 [============================>.] - ETA: 0s - loss: 1.3405 - acc: 0.4272\n",
      "Epoch 00284: val_loss did not improve from 1.43601\n",
      "468/468 [==============================] - 3s 6ms/step - loss: 1.3465 - acc: 0.4241 - val_loss: 1.5081 - val_acc: 0.3942\n",
      "Epoch 285/500\n",
      "460/468 [============================>.] - ETA: 0s - loss: 1.3403 - acc: 0.4467\n",
      "Epoch 00285: val_loss did not improve from 1.43601\n",
      "468/468 [==============================] - 3s 6ms/step - loss: 1.3430 - acc: 0.4444 - val_loss: 1.5249 - val_acc: 0.3846\n",
      "Epoch 286/500\n",
      "464/468 [============================>.] - ETA: 0s - loss: 1.3471 - acc: 0.4461\n",
      "Epoch 00286: val_loss did not improve from 1.43601\n",
      "468/468 [==============================] - 3s 6ms/step - loss: 1.3466 - acc: 0.4487 - val_loss: 1.5085 - val_acc: 0.4038\n",
      "Epoch 287/500\n",
      "461/468 [============================>.] - ETA: 0s - loss: 1.3394 - acc: 0.4490\n",
      "Epoch 00287: val_loss did not improve from 1.43601\n",
      "468/468 [==============================] - 3s 6ms/step - loss: 1.3398 - acc: 0.4476 - val_loss: 1.4874 - val_acc: 0.3846\n",
      "Epoch 288/500\n",
      "467/468 [============================>.] - ETA: 0s - loss: 1.3393 - acc: 0.4368\n",
      "Epoch 00288: val_loss did not improve from 1.43601\n",
      "468/468 [==============================] - 3s 6ms/step - loss: 1.3402 - acc: 0.4370 - val_loss: 1.4743 - val_acc: 0.3846\n",
      "Epoch 289/500\n",
      "466/468 [============================>.] - ETA: 0s - loss: 1.3505 - acc: 0.4281\n",
      "Epoch 00289: val_loss did not improve from 1.43601\n",
      "468/468 [==============================] - 3s 6ms/step - loss: 1.3491 - acc: 0.4295 - val_loss: 1.4966 - val_acc: 0.3750\n",
      "Epoch 290/500\n",
      "468/468 [==============================] - ETA: 0s - loss: 1.3397 - acc: 0.4402\n",
      "Epoch 00290: val_loss did not improve from 1.43601\n",
      "468/468 [==============================] - 3s 6ms/step - loss: 1.3397 - acc: 0.4402 - val_loss: 1.5352 - val_acc: 0.3846\n",
      "Epoch 291/500\n",
      "463/468 [============================>.] - ETA: 0s - loss: 1.3420 - acc: 0.4428\n",
      "Epoch 00291: val_loss did not improve from 1.43601\n",
      "468/468 [==============================] - 3s 6ms/step - loss: 1.3444 - acc: 0.4412 - val_loss: 1.5101 - val_acc: 0.3846\n",
      "Epoch 292/500\n",
      "468/468 [==============================] - ETA: 0s - loss: 1.3437 - acc: 0.4359\n",
      "Epoch 00292: val_loss did not improve from 1.43601\n",
      "468/468 [==============================] - 3s 6ms/step - loss: 1.3437 - acc: 0.4359 - val_loss: 1.5267 - val_acc: 0.3846\n",
      "Epoch 293/500\n",
      "467/468 [============================>.] - ETA: 0s - loss: 1.3384 - acc: 0.4390\n",
      "Epoch 00293: val_loss did not improve from 1.43601\n",
      "468/468 [==============================] - 3s 6ms/step - loss: 1.3394 - acc: 0.4391 - val_loss: 1.5480 - val_acc: 0.3462\n",
      "Epoch 294/500\n",
      "461/468 [============================>.] - ETA: 0s - loss: 1.3439 - acc: 0.4273\n",
      "Epoch 00294: val_loss did not improve from 1.43601\n",
      "468/468 [==============================] - 3s 6ms/step - loss: 1.3456 - acc: 0.4274 - val_loss: 1.5133 - val_acc: 0.3654\n",
      "Epoch 295/500\n",
      "468/468 [==============================] - ETA: 0s - loss: 1.3403 - acc: 0.4455\n",
      "Epoch 00295: val_loss did not improve from 1.43601\n",
      "468/468 [==============================] - 3s 6ms/step - loss: 1.3403 - acc: 0.4455 - val_loss: 1.5717 - val_acc: 0.3942\n",
      "Epoch 296/500\n",
      "461/468 [============================>.] - ETA: 0s - loss: 1.3442 - acc: 0.4479\n",
      "Epoch 00296: val_loss did not improve from 1.43601\n",
      "468/468 [==============================] - 3s 6ms/step - loss: 1.3369 - acc: 0.4530 - val_loss: 1.5164 - val_acc: 0.3942\n",
      "Epoch 297/500\n",
      "468/468 [==============================] - ETA: 0s - loss: 1.3424 - acc: 0.4370\n",
      "Epoch 00297: val_loss did not improve from 1.43601\n",
      "468/468 [==============================] - 3s 6ms/step - loss: 1.3424 - acc: 0.4370 - val_loss: 1.5129 - val_acc: 0.4135\n",
      "Epoch 298/500\n",
      "461/468 [============================>.] - ETA: 0s - loss: 1.3404 - acc: 0.4534\n",
      "Epoch 00298: val_loss did not improve from 1.43601\n",
      "468/468 [==============================] - 3s 6ms/step - loss: 1.3403 - acc: 0.4519 - val_loss: 1.4748 - val_acc: 0.4231\n",
      "Epoch 299/500\n",
      "464/468 [============================>.] - ETA: 0s - loss: 1.3365 - acc: 0.4569\n",
      "Epoch 00299: val_loss did not improve from 1.43601\n",
      "468/468 [==============================] - 3s 6ms/step - loss: 1.3374 - acc: 0.4583 - val_loss: 1.4879 - val_acc: 0.4135\n",
      "Epoch 300/500\n",
      "461/468 [============================>.] - ETA: 0s - loss: 1.3397 - acc: 0.4360\n",
      "Epoch 00300: val_loss did not improve from 1.43601\n",
      "468/468 [==============================] - 3s 6ms/step - loss: 1.3398 - acc: 0.4348 - val_loss: 1.5131 - val_acc: 0.3750\n",
      "Epoch 301/500\n",
      "468/468 [==============================] - ETA: 0s - loss: 1.3403 - acc: 0.4252\n",
      "Epoch 00301: val_loss did not improve from 1.43601\n",
      "468/468 [==============================] - 3s 6ms/step - loss: 1.3403 - acc: 0.4252 - val_loss: 1.5349 - val_acc: 0.3846\n",
      "Epoch 302/500\n",
      "461/468 [============================>.] - ETA: 0s - loss: 1.3294 - acc: 0.4414\n",
      "Epoch 00302: val_loss did not improve from 1.43601\n",
      "468/468 [==============================] - 3s 6ms/step - loss: 1.3367 - acc: 0.4412 - val_loss: 1.5252 - val_acc: 0.3942\n",
      "Epoch 303/500\n",
      "468/468 [==============================] - ETA: 0s - loss: 1.3363 - acc: 0.4498\n",
      "Epoch 00303: val_loss did not improve from 1.43601\n",
      "468/468 [==============================] - 3s 6ms/step - loss: 1.3363 - acc: 0.4498 - val_loss: 1.5356 - val_acc: 0.3942\n",
      "Epoch 304/500\n",
      "467/468 [============================>.] - ETA: 0s - loss: 1.3377 - acc: 0.4486\n",
      "Epoch 00304: val_loss did not improve from 1.43601\n",
      "468/468 [==============================] - 3s 6ms/step - loss: 1.3371 - acc: 0.4498 - val_loss: 1.4534 - val_acc: 0.3654\n",
      "Epoch 305/500\n",
      "461/468 [============================>.] - ETA: 0s - loss: 1.3392 - acc: 0.4555\n",
      "Epoch 00305: val_loss did not improve from 1.43601\n",
      "468/468 [==============================] - 3s 6ms/step - loss: 1.3405 - acc: 0.4519 - val_loss: 1.5096 - val_acc: 0.3558\n",
      "Epoch 306/500\n",
      "468/468 [==============================] - ETA: 0s - loss: 1.3408 - acc: 0.4241\n",
      "Epoch 00306: val_loss did not improve from 1.43601\n",
      "468/468 [==============================] - 3s 6ms/step - loss: 1.3408 - acc: 0.4241 - val_loss: 1.5159 - val_acc: 0.4327\n",
      "Epoch 307/500\n",
      "461/468 [============================>.] - ETA: 0s - loss: 1.3423 - acc: 0.4382\n",
      "Epoch 00307: val_loss did not improve from 1.43601\n",
      "468/468 [==============================] - 3s 6ms/step - loss: 1.3397 - acc: 0.4391 - val_loss: 1.5305 - val_acc: 0.4231\n",
      "Epoch 308/500\n",
      "463/468 [============================>.] - ETA: 0s - loss: 1.3395 - acc: 0.4503\n",
      "Epoch 00308: val_loss did not improve from 1.43601\n",
      "468/468 [==============================] - 3s 6ms/step - loss: 1.3396 - acc: 0.4476 - val_loss: 1.5311 - val_acc: 0.3846\n",
      "Epoch 309/500\n",
      "461/468 [============================>.] - ETA: 0s - loss: 1.3405 - acc: 0.4328\n",
      "Epoch 00309: val_loss did not improve from 1.43601\n",
      "468/468 [==============================] - 3s 6ms/step - loss: 1.3369 - acc: 0.4338 - val_loss: 1.5424 - val_acc: 0.3942\n",
      "Epoch 310/500\n",
      "463/468 [============================>.] - ETA: 0s - loss: 1.3453 - acc: 0.4320\n",
      "Epoch 00310: val_loss did not improve from 1.43601\n",
      "468/468 [==============================] - 3s 6ms/step - loss: 1.3417 - acc: 0.4338 - val_loss: 1.5318 - val_acc: 0.4038\n",
      "Epoch 311/500\n",
      "461/468 [============================>.] - ETA: 0s - loss: 1.3429 - acc: 0.4393\n",
      "Epoch 00311: val_loss did not improve from 1.43601\n",
      "468/468 [==============================] - 3s 6ms/step - loss: 1.3364 - acc: 0.4402 - val_loss: 1.5043 - val_acc: 0.3942\n",
      "Epoch 312/500\n",
      "463/468 [============================>.] - ETA: 0s - loss: 1.3412 - acc: 0.4384\n",
      "Epoch 00312: val_loss did not improve from 1.43601\n",
      "468/468 [==============================] - 3s 6ms/step - loss: 1.3394 - acc: 0.4402 - val_loss: 1.4611 - val_acc: 0.4038\n",
      "Epoch 313/500\n",
      "461/468 [============================>.] - ETA: 0s - loss: 1.3466 - acc: 0.4577\n",
      "Epoch 00313: val_loss did not improve from 1.43601\n",
      "468/468 [==============================] - 3s 6ms/step - loss: 1.3419 - acc: 0.4594 - val_loss: 1.5217 - val_acc: 0.4327\n",
      "Epoch 314/500\n",
      "463/468 [============================>.] - ETA: 0s - loss: 1.3377 - acc: 0.4503\n",
      "Epoch 00314: val_loss did not improve from 1.43601\n",
      "468/468 [==============================] - 3s 6ms/step - loss: 1.3362 - acc: 0.4509 - val_loss: 1.5084 - val_acc: 0.3942\n",
      "Epoch 315/500\n",
      "463/468 [============================>.] - ETA: 0s - loss: 1.3306 - acc: 0.4384\n",
      "Epoch 00315: val_loss did not improve from 1.43601\n",
      "468/468 [==============================] - 3s 6ms/step - loss: 1.3344 - acc: 0.4359 - val_loss: 1.5222 - val_acc: 0.3750\n",
      "Epoch 316/500\n",
      "461/468 [============================>.] - ETA: 0s - loss: 1.3291 - acc: 0.4599\n",
      "Epoch 00316: val_loss did not improve from 1.43601\n",
      "468/468 [==============================] - 3s 6ms/step - loss: 1.3340 - acc: 0.4583 - val_loss: 1.5371 - val_acc: 0.3750\n",
      "Epoch 317/500\n",
      "468/468 [==============================] - ETA: 0s - loss: 1.3310 - acc: 0.4476\n",
      "Epoch 00317: val_loss did not improve from 1.43601\n",
      "468/468 [==============================] - 3s 6ms/step - loss: 1.3310 - acc: 0.4476 - val_loss: 1.5706 - val_acc: 0.3558\n",
      "Epoch 318/500\n",
      "468/468 [==============================] - ETA: 0s - loss: 1.3434 - acc: 0.4295\n",
      "Epoch 00318: val_loss did not improve from 1.43601\n",
      "468/468 [==============================] - 3s 6ms/step - loss: 1.3434 - acc: 0.4295 - val_loss: 1.5268 - val_acc: 0.3750\n",
      "Epoch 319/500\n",
      "460/468 [============================>.] - ETA: 0s - loss: 1.3342 - acc: 0.4337\n",
      "Epoch 00319: val_loss did not improve from 1.43601\n",
      "468/468 [==============================] - 3s 6ms/step - loss: 1.3333 - acc: 0.4359 - val_loss: 1.5211 - val_acc: 0.4038\n",
      "Epoch 320/500\n",
      "468/468 [==============================] - ETA: 0s - loss: 1.3358 - acc: 0.4412\n",
      "Epoch 00320: val_loss did not improve from 1.43601\n",
      "468/468 [==============================] - 3s 6ms/step - loss: 1.3358 - acc: 0.4412 - val_loss: 1.5018 - val_acc: 0.4327\n",
      "Epoch 321/500\n",
      "468/468 [==============================] - ETA: 0s - loss: 1.3390 - acc: 0.4412\n",
      "Epoch 00321: val_loss did not improve from 1.43601\n",
      "468/468 [==============================] - 3s 6ms/step - loss: 1.3390 - acc: 0.4412 - val_loss: 1.5049 - val_acc: 0.3942\n",
      "Epoch 322/500\n",
      "460/468 [============================>.] - ETA: 0s - loss: 1.3388 - acc: 0.4413\n",
      "Epoch 00322: val_loss did not improve from 1.43601\n",
      "468/468 [==============================] - 3s 6ms/step - loss: 1.3375 - acc: 0.4391 - val_loss: 1.5065 - val_acc: 0.4423\n",
      "Epoch 323/500\n",
      "464/468 [============================>.] - ETA: 0s - loss: 1.3334 - acc: 0.4386\n",
      "Epoch 00323: val_loss did not improve from 1.43601\n",
      "468/468 [==============================] - 3s 6ms/step - loss: 1.3357 - acc: 0.4391 - val_loss: 1.4968 - val_acc: 0.4038\n",
      "Epoch 324/500\n",
      "468/468 [==============================] - ETA: 0s - loss: 1.3406 - acc: 0.4647\n",
      "Epoch 00324: val_loss did not improve from 1.43601\n",
      "468/468 [==============================] - 3s 6ms/step - loss: 1.3406 - acc: 0.4647 - val_loss: 1.5388 - val_acc: 0.3462\n",
      "Epoch 325/500\n",
      "465/468 [============================>.] - ETA: 0s - loss: 1.3381 - acc: 0.4462\n",
      "Epoch 00325: val_loss did not improve from 1.43601\n",
      "468/468 [==============================] - 3s 6ms/step - loss: 1.3381 - acc: 0.4476 - val_loss: 1.5184 - val_acc: 0.3846\n",
      "Epoch 326/500\n",
      "468/468 [==============================] - ETA: 0s - loss: 1.3319 - acc: 0.4327\n",
      "Epoch 00326: val_loss did not improve from 1.43601\n",
      "468/468 [==============================] - 3s 6ms/step - loss: 1.3319 - acc: 0.4327 - val_loss: 1.5203 - val_acc: 0.3558\n",
      "Epoch 327/500\n",
      "460/468 [============================>.] - ETA: 0s - loss: 1.3431 - acc: 0.4337\n",
      "Epoch 00327: val_loss did not improve from 1.43601\n",
      "468/468 [==============================] - 3s 6ms/step - loss: 1.3395 - acc: 0.4338 - val_loss: 1.5726 - val_acc: 0.3750\n",
      "Epoch 328/500\n",
      "466/468 [============================>.] - ETA: 0s - loss: 1.3370 - acc: 0.4399\n",
      "Epoch 00328: val_loss did not improve from 1.43601\n",
      "468/468 [==============================] - 3s 6ms/step - loss: 1.3386 - acc: 0.4402 - val_loss: 1.5102 - val_acc: 0.4519\n",
      "Epoch 329/500\n",
      "468/468 [==============================] - ETA: 0s - loss: 1.3374 - acc: 0.4444\n",
      "Epoch 00329: val_loss did not improve from 1.43601\n",
      "468/468 [==============================] - 3s 6ms/step - loss: 1.3374 - acc: 0.4444 - val_loss: 1.5146 - val_acc: 0.4327\n",
      "Epoch 330/500\n",
      "464/468 [============================>.] - ETA: 0s - loss: 1.3356 - acc: 0.4353\n",
      "Epoch 00330: val_loss did not improve from 1.43601\n",
      "468/468 [==============================] - 3s 6ms/step - loss: 1.3369 - acc: 0.4338 - val_loss: 1.5166 - val_acc: 0.3750\n",
      "Epoch 331/500\n",
      "465/468 [============================>.] - ETA: 0s - loss: 1.3345 - acc: 0.4301\n",
      "Epoch 00331: val_loss did not improve from 1.43601\n",
      "468/468 [==============================] - 3s 6ms/step - loss: 1.3356 - acc: 0.4306 - val_loss: 1.4885 - val_acc: 0.3750\n",
      "Epoch 332/500\n",
      "468/468 [==============================] - ETA: 0s - loss: 1.3307 - acc: 0.4476\n",
      "Epoch 00332: val_loss did not improve from 1.43601\n",
      "468/468 [==============================] - 3s 6ms/step - loss: 1.3307 - acc: 0.4476 - val_loss: 1.5128 - val_acc: 0.4135\n",
      "Epoch 333/500\n",
      "467/468 [============================>.] - ETA: 0s - loss: 1.3357 - acc: 0.4550\n",
      "Epoch 00333: val_loss did not improve from 1.43601\n",
      "468/468 [==============================] - 3s 6ms/step - loss: 1.3362 - acc: 0.4541 - val_loss: 1.5337 - val_acc: 0.4135\n",
      "Epoch 334/500\n",
      "461/468 [============================>.] - ETA: 0s - loss: 1.3343 - acc: 0.4262\n",
      "Epoch 00334: val_loss did not improve from 1.43601\n",
      "468/468 [==============================] - 3s 6ms/step - loss: 1.3342 - acc: 0.4252 - val_loss: 1.5213 - val_acc: 0.4038\n",
      "Epoch 335/500\n",
      "468/468 [==============================] - ETA: 0s - loss: 1.3316 - acc: 0.4284\n",
      "Epoch 00335: val_loss did not improve from 1.43601\n",
      "468/468 [==============================] - 3s 6ms/step - loss: 1.3316 - acc: 0.4284 - val_loss: 1.4933 - val_acc: 0.3846\n",
      "Epoch 336/500\n",
      "459/468 [============================>.] - ETA: 0s - loss: 1.3334 - acc: 0.4434\n",
      "Epoch 00336: val_loss did not improve from 1.43601\n",
      "468/468 [==============================] - 3s 7ms/step - loss: 1.3365 - acc: 0.4412 - val_loss: 1.5058 - val_acc: 0.4135\n",
      "Epoch 337/500\n",
      "463/468 [============================>.] - ETA: 0s - loss: 1.3408 - acc: 0.4438\n",
      "Epoch 00337: val_loss did not improve from 1.43601\n",
      "468/468 [==============================] - 3s 6ms/step - loss: 1.3371 - acc: 0.4434 - val_loss: 1.4684 - val_acc: 0.3942\n",
      "Epoch 338/500\n",
      "463/468 [============================>.] - ETA: 0s - loss: 1.3364 - acc: 0.4568\n",
      "Epoch 00338: val_loss did not improve from 1.43601\n",
      "468/468 [==============================] - 3s 6ms/step - loss: 1.3351 - acc: 0.4562 - val_loss: 1.5405 - val_acc: 0.4135\n",
      "Epoch 339/500\n",
      "467/468 [============================>.] - ETA: 0s - loss: 1.3346 - acc: 0.4433\n",
      "Epoch 00339: val_loss did not improve from 1.43601\n",
      "468/468 [==============================] - 3s 6ms/step - loss: 1.3362 - acc: 0.4423 - val_loss: 1.5159 - val_acc: 0.4038\n",
      "Epoch 340/500\n",
      "467/468 [============================>.] - ETA: 0s - loss: 1.3343 - acc: 0.4486\n",
      "Epoch 00340: val_loss did not improve from 1.43601\n",
      "468/468 [==============================] - 3s 7ms/step - loss: 1.3334 - acc: 0.4487 - val_loss: 1.5098 - val_acc: 0.4038\n",
      "Epoch 341/500\n",
      "466/468 [============================>.] - ETA: 0s - loss: 1.3299 - acc: 0.4399\n",
      "Epoch 00341: val_loss did not improve from 1.43601\n",
      "468/468 [==============================] - 3s 6ms/step - loss: 1.3294 - acc: 0.4391 - val_loss: 1.5363 - val_acc: 0.4135\n",
      "Epoch 342/500\n",
      "467/468 [============================>.] - ETA: 0s - loss: 1.3336 - acc: 0.4379\n",
      "Epoch 00342: val_loss did not improve from 1.43601\n",
      "468/468 [==============================] - 3s 6ms/step - loss: 1.3345 - acc: 0.4370 - val_loss: 1.4588 - val_acc: 0.4327\n",
      "Epoch 343/500\n",
      "463/468 [============================>.] - ETA: 0s - loss: 1.3327 - acc: 0.4579\n",
      "Epoch 00343: val_loss did not improve from 1.43601\n",
      "468/468 [==============================] - 3s 6ms/step - loss: 1.3304 - acc: 0.4583 - val_loss: 1.4853 - val_acc: 0.3942\n",
      "Epoch 344/500\n",
      "464/468 [============================>.] - ETA: 0s - loss: 1.3287 - acc: 0.4569\n",
      "Epoch 00344: val_loss did not improve from 1.43601\n",
      "468/468 [==============================] - 3s 6ms/step - loss: 1.3272 - acc: 0.4573 - val_loss: 1.5141 - val_acc: 0.3462\n",
      "Epoch 345/500\n",
      "467/468 [============================>.] - ETA: 0s - loss: 1.3358 - acc: 0.4411\n",
      "Epoch 00345: val_loss did not improve from 1.43601\n",
      "468/468 [==============================] - 3s 6ms/step - loss: 1.3353 - acc: 0.4412 - val_loss: 1.5231 - val_acc: 0.3942\n",
      "Epoch 346/500\n",
      "467/468 [============================>.] - ETA: 0s - loss: 1.3385 - acc: 0.4283\n",
      "Epoch 00346: val_loss did not improve from 1.43601\n",
      "468/468 [==============================] - 3s 6ms/step - loss: 1.3380 - acc: 0.4284 - val_loss: 1.5368 - val_acc: 0.3750\n",
      "Epoch 347/500\n",
      "466/468 [============================>.] - ETA: 0s - loss: 1.3296 - acc: 0.4464\n",
      "Epoch 00347: val_loss did not improve from 1.43601\n",
      "468/468 [==============================] - 3s 6ms/step - loss: 1.3315 - acc: 0.4455 - val_loss: 1.4707 - val_acc: 0.4327\n",
      "Epoch 348/500\n",
      "463/468 [============================>.] - ETA: 0s - loss: 1.3370 - acc: 0.4395\n",
      "Epoch 00348: val_loss did not improve from 1.43601\n",
      "468/468 [==============================] - 3s 6ms/step - loss: 1.3352 - acc: 0.4402 - val_loss: 1.4833 - val_acc: 0.4038\n",
      "Epoch 349/500\n",
      "461/468 [============================>.] - ETA: 0s - loss: 1.3409 - acc: 0.4469\n",
      "Epoch 00349: val_loss did not improve from 1.43601\n",
      "468/468 [==============================] - 3s 6ms/step - loss: 1.3352 - acc: 0.4498 - val_loss: 1.4840 - val_acc: 0.4423\n",
      "Epoch 350/500\n",
      "462/468 [============================>.] - ETA: 0s - loss: 1.3348 - acc: 0.4600\n",
      "Epoch 00350: val_loss did not improve from 1.43601\n",
      "468/468 [==============================] - 3s 6ms/step - loss: 1.3372 - acc: 0.4573 - val_loss: 1.5588 - val_acc: 0.3750\n",
      "Epoch 351/500\n",
      "464/468 [============================>.] - ETA: 0s - loss: 1.3346 - acc: 0.4418\n",
      "Epoch 00351: val_loss did not improve from 1.43601\n",
      "468/468 [==============================] - 3s 6ms/step - loss: 1.3358 - acc: 0.4402 - val_loss: 1.5073 - val_acc: 0.3942\n",
      "Epoch 352/500\n",
      "468/468 [==============================] - ETA: 0s - loss: 1.3334 - acc: 0.4476\n",
      "Epoch 00352: val_loss did not improve from 1.43601\n",
      "468/468 [==============================] - 3s 6ms/step - loss: 1.3334 - acc: 0.4476 - val_loss: 1.5101 - val_acc: 0.4038\n",
      "Epoch 353/500\n",
      "466/468 [============================>.] - ETA: 0s - loss: 1.3310 - acc: 0.4335\n",
      "Epoch 00353: val_loss did not improve from 1.43601\n",
      "468/468 [==============================] - 3s 7ms/step - loss: 1.3298 - acc: 0.4327 - val_loss: 1.5237 - val_acc: 0.3750\n",
      "Epoch 354/500\n",
      "464/468 [============================>.] - ETA: 0s - loss: 1.3321 - acc: 0.4364\n",
      "Epoch 00354: val_loss did not improve from 1.43601\n",
      "468/468 [==============================] - 3s 6ms/step - loss: 1.3317 - acc: 0.4370 - val_loss: 1.4769 - val_acc: 0.3942\n",
      "Epoch 355/500\n",
      "468/468 [==============================] - ETA: 0s - loss: 1.3333 - acc: 0.4583\n",
      "Epoch 00355: val_loss did not improve from 1.43601\n",
      "468/468 [==============================] - 3s 6ms/step - loss: 1.3333 - acc: 0.4583 - val_loss: 1.4964 - val_acc: 0.4135\n",
      "Epoch 356/500\n",
      "463/468 [============================>.] - ETA: 0s - loss: 1.3371 - acc: 0.4384\n",
      "Epoch 00356: val_loss did not improve from 1.43601\n",
      "468/468 [==============================] - 3s 6ms/step - loss: 1.3357 - acc: 0.4391 - val_loss: 1.4873 - val_acc: 0.4423\n",
      "Epoch 357/500\n",
      "463/468 [============================>.] - ETA: 0s - loss: 1.3367 - acc: 0.4460\n",
      "Epoch 00357: val_loss did not improve from 1.43601\n",
      "468/468 [==============================] - 3s 6ms/step - loss: 1.3376 - acc: 0.4455 - val_loss: 1.5044 - val_acc: 0.4423\n",
      "Epoch 358/500\n",
      "461/468 [============================>.] - ETA: 0s - loss: 1.3310 - acc: 0.4534\n",
      "Epoch 00358: val_loss did not improve from 1.43601\n",
      "468/468 [==============================] - 3s 6ms/step - loss: 1.3286 - acc: 0.4562 - val_loss: 1.4893 - val_acc: 0.4327\n",
      "Epoch 359/500\n",
      "463/468 [============================>.] - ETA: 0s - loss: 1.3297 - acc: 0.4438\n",
      "Epoch 00359: val_loss did not improve from 1.43601\n",
      "468/468 [==============================] - 3s 6ms/step - loss: 1.3329 - acc: 0.4423 - val_loss: 1.5169 - val_acc: 0.4135\n",
      "Epoch 360/500\n",
      "465/468 [============================>.] - ETA: 0s - loss: 1.3345 - acc: 0.4398\n",
      "Epoch 00360: val_loss did not improve from 1.43601\n",
      "468/468 [==============================] - 3s 6ms/step - loss: 1.3336 - acc: 0.4391 - val_loss: 1.5055 - val_acc: 0.4135\n",
      "Epoch 361/500\n",
      "468/468 [==============================] - ETA: 0s - loss: 1.3337 - acc: 0.4455\n",
      "Epoch 00361: val_loss did not improve from 1.43601\n",
      "468/468 [==============================] - 3s 6ms/step - loss: 1.3337 - acc: 0.4455 - val_loss: 1.4516 - val_acc: 0.4519\n",
      "Epoch 362/500\n",
      "468/468 [==============================] - ETA: 0s - loss: 1.3295 - acc: 0.4551\n",
      "Epoch 00362: val_loss did not improve from 1.43601\n",
      "468/468 [==============================] - 3s 6ms/step - loss: 1.3295 - acc: 0.4551 - val_loss: 1.5125 - val_acc: 0.4327\n",
      "Epoch 363/500\n",
      "464/468 [============================>.] - ETA: 0s - loss: 1.3203 - acc: 0.4558\n",
      "Epoch 00363: val_loss did not improve from 1.43601\n",
      "468/468 [==============================] - 3s 6ms/step - loss: 1.3244 - acc: 0.4530 - val_loss: 1.5475 - val_acc: 0.4038\n",
      "Epoch 364/500\n",
      "463/468 [============================>.] - ETA: 0s - loss: 1.3332 - acc: 0.4525\n",
      "Epoch 00364: val_loss did not improve from 1.43601\n",
      "468/468 [==============================] - 3s 6ms/step - loss: 1.3314 - acc: 0.4530 - val_loss: 1.4960 - val_acc: 0.4135\n",
      "Epoch 365/500\n",
      "467/468 [============================>.] - ETA: 0s - loss: 1.3267 - acc: 0.4518\n",
      "Epoch 00365: val_loss did not improve from 1.43601\n",
      "468/468 [==============================] - 3s 6ms/step - loss: 1.3264 - acc: 0.4509 - val_loss: 1.4765 - val_acc: 0.4135\n",
      "Epoch 366/500\n",
      "466/468 [============================>.] - ETA: 0s - loss: 1.3279 - acc: 0.4464\n",
      "Epoch 00366: val_loss did not improve from 1.43601\n",
      "468/468 [==============================] - 3s 6ms/step - loss: 1.3273 - acc: 0.4455 - val_loss: 1.5095 - val_acc: 0.4135\n",
      "Epoch 367/500\n",
      "468/468 [==============================] - ETA: 0s - loss: 1.3307 - acc: 0.4541\n",
      "Epoch 00367: val_loss did not improve from 1.43601\n",
      "468/468 [==============================] - 3s 6ms/step - loss: 1.3307 - acc: 0.4541 - val_loss: 1.5316 - val_acc: 0.3942\n",
      "Epoch 368/500\n",
      "464/468 [============================>.] - ETA: 0s - loss: 1.3326 - acc: 0.4310\n",
      "Epoch 00368: val_loss did not improve from 1.43601\n",
      "468/468 [==============================] - 3s 6ms/step - loss: 1.3340 - acc: 0.4306 - val_loss: 1.4776 - val_acc: 0.3942\n",
      "Epoch 369/500\n",
      "459/468 [============================>.] - ETA: 0s - loss: 1.3308 - acc: 0.4455\n",
      "Epoch 00369: val_loss did not improve from 1.43601\n",
      "468/468 [==============================] - 3s 6ms/step - loss: 1.3285 - acc: 0.4444 - val_loss: 1.5005 - val_acc: 0.3942\n",
      "Epoch 370/500\n",
      "461/468 [============================>.] - ETA: 0s - loss: 1.3296 - acc: 0.4523\n",
      "Epoch 00370: val_loss did not improve from 1.43601\n",
      "468/468 [==============================] - 3s 6ms/step - loss: 1.3282 - acc: 0.4519 - val_loss: 1.4972 - val_acc: 0.4038\n",
      "Epoch 371/500\n",
      "461/468 [============================>.] - ETA: 0s - loss: 1.3297 - acc: 0.4469\n",
      "Epoch 00371: val_loss did not improve from 1.43601\n",
      "468/468 [==============================] - 3s 6ms/step - loss: 1.3274 - acc: 0.4466 - val_loss: 1.5221 - val_acc: 0.3750\n",
      "Epoch 372/500\n",
      "467/468 [============================>.] - ETA: 0s - loss: 1.3318 - acc: 0.4486\n",
      "Epoch 00372: val_loss did not improve from 1.43601\n",
      "468/468 [==============================] - 3s 6ms/step - loss: 1.3317 - acc: 0.4487 - val_loss: 1.5097 - val_acc: 0.4135\n",
      "Epoch 373/500\n",
      "461/468 [============================>.] - ETA: 0s - loss: 1.3236 - acc: 0.4555\n",
      "Epoch 00373: val_loss did not improve from 1.43601\n",
      "468/468 [==============================] - 3s 6ms/step - loss: 1.3262 - acc: 0.4541 - val_loss: 1.5180 - val_acc: 0.3942\n",
      "Epoch 374/500\n",
      "467/468 [============================>.] - ETA: 0s - loss: 1.3307 - acc: 0.4475\n",
      "Epoch 00374: val_loss did not improve from 1.43601\n",
      "468/468 [==============================] - 3s 6ms/step - loss: 1.3311 - acc: 0.4466 - val_loss: 1.5495 - val_acc: 0.3462\n",
      "Epoch 375/500\n",
      "465/468 [============================>.] - ETA: 0s - loss: 1.3394 - acc: 0.4398\n",
      "Epoch 00375: val_loss did not improve from 1.43601\n",
      "468/468 [==============================] - 3s 6ms/step - loss: 1.3396 - acc: 0.4402 - val_loss: 1.5084 - val_acc: 0.4423\n",
      "Epoch 376/500\n",
      "465/468 [============================>.] - ETA: 0s - loss: 1.3367 - acc: 0.4441\n",
      "Epoch 00376: val_loss did not improve from 1.43601\n",
      "468/468 [==============================] - 3s 6ms/step - loss: 1.3370 - acc: 0.4444 - val_loss: 1.4961 - val_acc: 0.4038\n",
      "Epoch 377/500\n",
      "468/468 [==============================] - ETA: 0s - loss: 1.3235 - acc: 0.4583\n",
      "Epoch 00377: val_loss did not improve from 1.43601\n",
      "468/468 [==============================] - 3s 6ms/step - loss: 1.3235 - acc: 0.4583 - val_loss: 1.5491 - val_acc: 0.4038\n",
      "Epoch 378/500\n",
      "466/468 [============================>.] - ETA: 0s - loss: 1.3328 - acc: 0.4517\n",
      "Epoch 00378: val_loss did not improve from 1.43601\n",
      "468/468 [==============================] - 3s 6ms/step - loss: 1.3323 - acc: 0.4519 - val_loss: 1.4428 - val_acc: 0.4423\n",
      "Epoch 379/500\n",
      "465/468 [============================>.] - ETA: 0s - loss: 1.3274 - acc: 0.4441\n",
      "Epoch 00379: val_loss did not improve from 1.43601\n",
      "468/468 [==============================] - 3s 6ms/step - loss: 1.3289 - acc: 0.4444 - val_loss: 1.4938 - val_acc: 0.4327\n",
      "Epoch 380/500\n",
      "466/468 [============================>.] - ETA: 0s - loss: 1.3281 - acc: 0.4614\n",
      "Epoch 00380: val_loss did not improve from 1.43601\n",
      "468/468 [==============================] - 3s 6ms/step - loss: 1.3284 - acc: 0.4594 - val_loss: 1.5243 - val_acc: 0.3942\n",
      "Epoch 381/500\n",
      "459/468 [============================>.] - ETA: 0s - loss: 1.3247 - acc: 0.4401\n",
      "Epoch 00381: val_loss did not improve from 1.43601\n",
      "468/468 [==============================] - 3s 6ms/step - loss: 1.3229 - acc: 0.4412 - val_loss: 1.5492 - val_acc: 0.3942\n",
      "Epoch 382/500\n",
      "467/468 [============================>.] - ETA: 0s - loss: 1.3213 - acc: 0.4636\n",
      "Epoch 00382: val_loss did not improve from 1.43601\n",
      "468/468 [==============================] - 3s 6ms/step - loss: 1.3255 - acc: 0.4626 - val_loss: 1.5391 - val_acc: 0.3846\n",
      "Epoch 383/500\n",
      "462/468 [============================>.] - ETA: 0s - loss: 1.3281 - acc: 0.4513\n",
      "Epoch 00383: val_loss did not improve from 1.43601\n",
      "468/468 [==============================] - 3s 6ms/step - loss: 1.3261 - acc: 0.4541 - val_loss: 1.5495 - val_acc: 0.3942\n",
      "Epoch 384/500\n",
      "463/468 [============================>.] - ETA: 0s - loss: 1.3245 - acc: 0.4546\n",
      "Epoch 00384: val_loss did not improve from 1.43601\n",
      "468/468 [==============================] - 3s 6ms/step - loss: 1.3279 - acc: 0.4551 - val_loss: 1.5322 - val_acc: 0.3750\n",
      "Epoch 385/500\n",
      "466/468 [============================>.] - ETA: 0s - loss: 1.3320 - acc: 0.4345\n",
      "Epoch 00385: val_loss did not improve from 1.43601\n",
      "468/468 [==============================] - 3s 6ms/step - loss: 1.3332 - acc: 0.4338 - val_loss: 1.4808 - val_acc: 0.4519\n",
      "Epoch 386/500\n",
      "463/468 [============================>.] - ETA: 0s - loss: 1.3314 - acc: 0.4363\n",
      "Epoch 00386: val_loss did not improve from 1.43601\n",
      "468/468 [==============================] - 3s 6ms/step - loss: 1.3333 - acc: 0.4338 - val_loss: 1.4949 - val_acc: 0.3846\n",
      "Epoch 387/500\n",
      "461/468 [============================>.] - ETA: 0s - loss: 1.3263 - acc: 0.4490\n",
      "Epoch 00387: val_loss did not improve from 1.43601\n",
      "468/468 [==============================] - 3s 6ms/step - loss: 1.3242 - acc: 0.4509 - val_loss: 1.5503 - val_acc: 0.4423\n",
      "Epoch 388/500\n",
      "460/468 [============================>.] - ETA: 0s - loss: 1.3288 - acc: 0.4457\n",
      "Epoch 00388: val_loss did not improve from 1.43601\n",
      "468/468 [==============================] - 3s 6ms/step - loss: 1.3271 - acc: 0.4455 - val_loss: 1.5169 - val_acc: 0.4231\n",
      "Epoch 389/500\n",
      "460/468 [============================>.] - ETA: 0s - loss: 1.3215 - acc: 0.4598\n",
      "Epoch 00389: val_loss did not improve from 1.43601\n",
      "468/468 [==============================] - 3s 6ms/step - loss: 1.3201 - acc: 0.4605 - val_loss: 1.5162 - val_acc: 0.4038\n",
      "Epoch 390/500\n",
      "462/468 [============================>.] - ETA: 0s - loss: 1.3293 - acc: 0.4610\n",
      "Epoch 00390: val_loss did not improve from 1.43601\n",
      "468/468 [==============================] - 3s 6ms/step - loss: 1.3276 - acc: 0.4637 - val_loss: 1.4694 - val_acc: 0.3942\n",
      "Epoch 391/500\n",
      "465/468 [============================>.] - ETA: 0s - loss: 1.3223 - acc: 0.4581\n",
      "Epoch 00391: val_loss did not improve from 1.43601\n",
      "468/468 [==============================] - 3s 6ms/step - loss: 1.3243 - acc: 0.4573 - val_loss: 1.5487 - val_acc: 0.4231\n",
      "Epoch 392/500\n",
      "462/468 [============================>.] - ETA: 0s - loss: 1.3248 - acc: 0.4675\n",
      "Epoch 00392: val_loss did not improve from 1.43601\n",
      "468/468 [==============================] - 3s 6ms/step - loss: 1.3252 - acc: 0.4679 - val_loss: 1.5264 - val_acc: 0.3942\n",
      "Epoch 393/500\n",
      "467/468 [============================>.] - ETA: 0s - loss: 1.3294 - acc: 0.4400\n",
      "Epoch 00393: val_loss did not improve from 1.43601\n",
      "468/468 [==============================] - 3s 6ms/step - loss: 1.3291 - acc: 0.4412 - val_loss: 1.5396 - val_acc: 0.3750\n",
      "Epoch 394/500\n",
      "460/468 [============================>.] - ETA: 0s - loss: 1.3201 - acc: 0.4685\n",
      "Epoch 00394: val_loss did not improve from 1.43601\n",
      "468/468 [==============================] - 3s 6ms/step - loss: 1.3199 - acc: 0.4690 - val_loss: 1.5202 - val_acc: 0.3846\n",
      "Epoch 395/500\n",
      "464/468 [============================>.] - ETA: 0s - loss: 1.3300 - acc: 0.4461\n",
      "Epoch 00395: val_loss did not improve from 1.43601\n",
      "468/468 [==============================] - 3s 6ms/step - loss: 1.3341 - acc: 0.4455 - val_loss: 1.5134 - val_acc: 0.4231\n",
      "Epoch 396/500\n",
      "464/468 [============================>.] - ETA: 0s - loss: 1.3174 - acc: 0.4515\n",
      "Epoch 00396: val_loss did not improve from 1.43601\n",
      "468/468 [==============================] - 3s 6ms/step - loss: 1.3225 - acc: 0.4498 - val_loss: 1.4916 - val_acc: 0.4038\n",
      "Epoch 397/500\n",
      "463/468 [============================>.] - ETA: 0s - loss: 1.3212 - acc: 0.4492\n",
      "Epoch 00397: val_loss did not improve from 1.43601\n",
      "468/468 [==============================] - 3s 6ms/step - loss: 1.3236 - acc: 0.4476 - val_loss: 1.5761 - val_acc: 0.4038\n",
      "Epoch 398/500\n",
      "463/468 [============================>.] - ETA: 0s - loss: 1.3322 - acc: 0.4341\n",
      "Epoch 00398: val_loss did not improve from 1.43601\n",
      "468/468 [==============================] - 3s 6ms/step - loss: 1.3310 - acc: 0.4348 - val_loss: 1.4925 - val_acc: 0.4038\n",
      "Epoch 399/500\n",
      "461/468 [============================>.] - ETA: 0s - loss: 1.3289 - acc: 0.4328\n",
      "Epoch 00399: val_loss did not improve from 1.43601\n",
      "468/468 [==============================] - 3s 6ms/step - loss: 1.3299 - acc: 0.4316 - val_loss: 1.5123 - val_acc: 0.4135\n",
      "Epoch 400/500\n",
      "461/468 [============================>.] - ETA: 0s - loss: 1.3187 - acc: 0.4382\n",
      "Epoch 00400: val_loss did not improve from 1.43601\n",
      "468/468 [==============================] - 3s 6ms/step - loss: 1.3203 - acc: 0.4380 - val_loss: 1.5437 - val_acc: 0.3942\n",
      "Epoch 401/500\n",
      "462/468 [============================>.] - ETA: 0s - loss: 1.3271 - acc: 0.4643\n",
      "Epoch 00401: val_loss did not improve from 1.43601\n",
      "468/468 [==============================] - 3s 6ms/step - loss: 1.3228 - acc: 0.4658 - val_loss: 1.4828 - val_acc: 0.4135\n",
      "Epoch 402/500\n",
      "463/468 [============================>.] - ETA: 0s - loss: 1.3182 - acc: 0.4644\n",
      "Epoch 00402: val_loss did not improve from 1.43601\n",
      "468/468 [==============================] - 3s 6ms/step - loss: 1.3215 - acc: 0.4615 - val_loss: 1.5128 - val_acc: 0.3846\n",
      "Epoch 403/500\n",
      "462/468 [============================>.] - ETA: 0s - loss: 1.3254 - acc: 0.4481\n",
      "Epoch 00403: val_loss did not improve from 1.43601\n",
      "468/468 [==============================] - 3s 6ms/step - loss: 1.3264 - acc: 0.4476 - val_loss: 1.5278 - val_acc: 0.3558\n",
      "Epoch 404/500\n",
      "464/468 [============================>.] - ETA: 0s - loss: 1.3285 - acc: 0.4461\n",
      "Epoch 00404: val_loss did not improve from 1.43601\n",
      "468/468 [==============================] - 3s 6ms/step - loss: 1.3250 - acc: 0.4476 - val_loss: 1.5292 - val_acc: 0.3750\n",
      "Epoch 405/500\n",
      "467/468 [============================>.] - ETA: 0s - loss: 1.3288 - acc: 0.4465\n",
      "Epoch 00405: val_loss did not improve from 1.43601\n",
      "468/468 [==============================] - 3s 6ms/step - loss: 1.3286 - acc: 0.4466 - val_loss: 1.5722 - val_acc: 0.3462\n",
      "Epoch 406/500\n",
      "462/468 [============================>.] - ETA: 0s - loss: 1.3220 - acc: 0.4589\n",
      "Epoch 00406: val_loss did not improve from 1.43601\n",
      "468/468 [==============================] - 3s 6ms/step - loss: 1.3227 - acc: 0.4594 - val_loss: 1.4660 - val_acc: 0.4038\n",
      "Epoch 407/500\n",
      "460/468 [============================>.] - ETA: 0s - loss: 1.3237 - acc: 0.4576\n",
      "Epoch 00407: val_loss did not improve from 1.43601\n",
      "468/468 [==============================] - 3s 6ms/step - loss: 1.3280 - acc: 0.4551 - val_loss: 1.5008 - val_acc: 0.3750\n",
      "Epoch 408/500\n",
      "465/468 [============================>.] - ETA: 0s - loss: 1.3185 - acc: 0.4462\n",
      "Epoch 00408: val_loss did not improve from 1.43601\n",
      "468/468 [==============================] - 3s 6ms/step - loss: 1.3221 - acc: 0.4455 - val_loss: 1.5424 - val_acc: 0.4327\n",
      "Epoch 409/500\n",
      "468/468 [==============================] - ETA: 0s - loss: 1.3261 - acc: 0.4541\n",
      "Epoch 00409: val_loss did not improve from 1.43601\n",
      "468/468 [==============================] - 3s 6ms/step - loss: 1.3261 - acc: 0.4541 - val_loss: 1.4837 - val_acc: 0.4327\n",
      "Epoch 410/500\n",
      "461/468 [============================>.] - ETA: 0s - loss: 1.3315 - acc: 0.4501\n",
      "Epoch 00410: val_loss did not improve from 1.43601\n",
      "468/468 [==============================] - 3s 6ms/step - loss: 1.3279 - acc: 0.4519 - val_loss: 1.4975 - val_acc: 0.4231\n",
      "Epoch 411/500\n",
      "466/468 [============================>.] - ETA: 0s - loss: 1.3268 - acc: 0.4399\n",
      "Epoch 00411: val_loss did not improve from 1.43601\n",
      "468/468 [==============================] - 3s 6ms/step - loss: 1.3253 - acc: 0.4412 - val_loss: 1.4864 - val_acc: 0.3942\n",
      "Epoch 412/500\n",
      "465/468 [============================>.] - ETA: 0s - loss: 1.3289 - acc: 0.4441\n",
      "Epoch 00412: val_loss did not improve from 1.43601\n",
      "468/468 [==============================] - 3s 6ms/step - loss: 1.3268 - acc: 0.4455 - val_loss: 1.5034 - val_acc: 0.4231\n",
      "Epoch 413/500\n",
      "464/468 [============================>.] - ETA: 0s - loss: 1.3235 - acc: 0.4504\n",
      "Epoch 00413: val_loss did not improve from 1.43601\n",
      "468/468 [==============================] - 3s 6ms/step - loss: 1.3200 - acc: 0.4509 - val_loss: 1.5428 - val_acc: 0.4135\n",
      "Epoch 414/500\n",
      "466/468 [============================>.] - ETA: 0s - loss: 1.3259 - acc: 0.4421\n",
      "Epoch 00414: val_loss did not improve from 1.43601\n",
      "468/468 [==============================] - 3s 6ms/step - loss: 1.3286 - acc: 0.4412 - val_loss: 1.4632 - val_acc: 0.4712\n",
      "Epoch 415/500\n",
      "467/468 [============================>.] - ETA: 0s - loss: 1.3191 - acc: 0.4529\n",
      "Epoch 00415: val_loss did not improve from 1.43601\n",
      "468/468 [==============================] - 3s 6ms/step - loss: 1.3185 - acc: 0.4541 - val_loss: 1.5190 - val_acc: 0.3942\n",
      "Epoch 416/500\n",
      "466/468 [============================>.] - ETA: 0s - loss: 1.3326 - acc: 0.4560\n",
      "Epoch 00416: val_loss did not improve from 1.43601\n",
      "468/468 [==============================] - 3s 6ms/step - loss: 1.3319 - acc: 0.4551 - val_loss: 1.5003 - val_acc: 0.4327\n",
      "Epoch 417/500\n",
      "468/468 [==============================] - ETA: 0s - loss: 1.3233 - acc: 0.4444\n",
      "Epoch 00417: val_loss did not improve from 1.43601\n",
      "468/468 [==============================] - 3s 6ms/step - loss: 1.3233 - acc: 0.4444 - val_loss: 1.5238 - val_acc: 0.4327\n",
      "Epoch 418/500\n",
      "466/468 [============================>.] - ETA: 0s - loss: 1.3236 - acc: 0.4571\n",
      "Epoch 00418: val_loss did not improve from 1.43601\n",
      "468/468 [==============================] - 3s 6ms/step - loss: 1.3234 - acc: 0.4573 - val_loss: 1.4701 - val_acc: 0.3846\n",
      "Epoch 419/500\n",
      "466/468 [============================>.] - ETA: 0s - loss: 1.3296 - acc: 0.4496\n",
      "Epoch 00419: val_loss did not improve from 1.43601\n",
      "468/468 [==============================] - 3s 6ms/step - loss: 1.3294 - acc: 0.4487 - val_loss: 1.4973 - val_acc: 0.4038\n",
      "Epoch 420/500\n",
      "468/468 [==============================] - ETA: 0s - loss: 1.3153 - acc: 0.4626\n",
      "Epoch 00420: val_loss did not improve from 1.43601\n",
      "468/468 [==============================] - 3s 6ms/step - loss: 1.3153 - acc: 0.4626 - val_loss: 1.5417 - val_acc: 0.3942\n",
      "Epoch 421/500\n",
      "467/468 [============================>.] - ETA: 0s - loss: 1.3259 - acc: 0.4507\n",
      "Epoch 00421: val_loss did not improve from 1.43601\n",
      "468/468 [==============================] - 3s 6ms/step - loss: 1.3250 - acc: 0.4519 - val_loss: 1.4833 - val_acc: 0.4135\n",
      "Epoch 422/500\n",
      "461/468 [============================>.] - ETA: 0s - loss: 1.3223 - acc: 0.4458\n",
      "Epoch 00422: val_loss did not improve from 1.43601\n",
      "468/468 [==============================] - 3s 6ms/step - loss: 1.3203 - acc: 0.4466 - val_loss: 1.5159 - val_acc: 0.3846\n",
      "Epoch 423/500\n",
      "468/468 [==============================] - ETA: 0s - loss: 1.3268 - acc: 0.4359\n",
      "Epoch 00423: val_loss did not improve from 1.43601\n",
      "468/468 [==============================] - 3s 6ms/step - loss: 1.3268 - acc: 0.4359 - val_loss: 1.5178 - val_acc: 0.3654\n",
      "Epoch 424/500\n",
      "464/468 [============================>.] - ETA: 0s - loss: 1.3289 - acc: 0.4321\n",
      "Epoch 00424: val_loss did not improve from 1.43601\n",
      "468/468 [==============================] - 3s 6ms/step - loss: 1.3264 - acc: 0.4327 - val_loss: 1.4891 - val_acc: 0.4038\n",
      "Epoch 425/500\n",
      "466/468 [============================>.] - ETA: 0s - loss: 1.3210 - acc: 0.4582\n",
      "Epoch 00425: val_loss did not improve from 1.43601\n",
      "468/468 [==============================] - 3s 6ms/step - loss: 1.3212 - acc: 0.4573 - val_loss: 1.5134 - val_acc: 0.3750\n",
      "Epoch 426/500\n",
      "460/468 [============================>.] - ETA: 0s - loss: 1.3097 - acc: 0.4543\n",
      "Epoch 00426: val_loss did not improve from 1.43601\n",
      "468/468 [==============================] - 3s 6ms/step - loss: 1.3121 - acc: 0.4519 - val_loss: 1.5527 - val_acc: 0.3846\n",
      "Epoch 427/500\n",
      "464/468 [============================>.] - ETA: 0s - loss: 1.3239 - acc: 0.4289\n",
      "Epoch 00427: val_loss did not improve from 1.43601\n",
      "468/468 [==============================] - 3s 6ms/step - loss: 1.3244 - acc: 0.4284 - val_loss: 1.5120 - val_acc: 0.3942\n",
      "Epoch 428/500\n",
      "459/468 [============================>.] - ETA: 0s - loss: 1.3254 - acc: 0.4532\n",
      "Epoch 00428: val_loss did not improve from 1.43601\n",
      "468/468 [==============================] - 3s 6ms/step - loss: 1.3236 - acc: 0.4541 - val_loss: 1.5216 - val_acc: 0.3942\n",
      "Epoch 429/500\n",
      "464/468 [============================>.] - ETA: 0s - loss: 1.3287 - acc: 0.4526\n",
      "Epoch 00429: val_loss did not improve from 1.43601\n",
      "468/468 [==============================] - 3s 6ms/step - loss: 1.3241 - acc: 0.4541 - val_loss: 1.4988 - val_acc: 0.4423\n",
      "Epoch 430/500\n",
      "460/468 [============================>.] - ETA: 0s - loss: 1.3278 - acc: 0.4293\n",
      "Epoch 00430: val_loss did not improve from 1.43601\n",
      "468/468 [==============================] - 3s 6ms/step - loss: 1.3271 - acc: 0.4316 - val_loss: 1.5056 - val_acc: 0.4231\n",
      "Epoch 431/500\n",
      "461/468 [============================>.] - ETA: 0s - loss: 1.3229 - acc: 0.4414\n",
      "Epoch 00431: val_loss did not improve from 1.43601\n",
      "468/468 [==============================] - 3s 6ms/step - loss: 1.3240 - acc: 0.4423 - val_loss: 1.4969 - val_acc: 0.4231\n",
      "Epoch 432/500\n",
      "467/468 [============================>.] - ETA: 0s - loss: 1.3235 - acc: 0.4304\n",
      "Epoch 00432: val_loss did not improve from 1.43601\n",
      "468/468 [==============================] - 3s 6ms/step - loss: 1.3235 - acc: 0.4306 - val_loss: 1.5206 - val_acc: 0.3846\n",
      "Epoch 433/500\n",
      "464/468 [============================>.] - ETA: 0s - loss: 1.3262 - acc: 0.4526\n",
      "Epoch 00433: val_loss did not improve from 1.43601\n",
      "468/468 [==============================] - 3s 6ms/step - loss: 1.3248 - acc: 0.4541 - val_loss: 1.5376 - val_acc: 0.3750\n",
      "Epoch 434/500\n",
      "468/468 [==============================] - ETA: 0s - loss: 1.3173 - acc: 0.4498\n",
      "Epoch 00434: val_loss did not improve from 1.43601\n",
      "468/468 [==============================] - 3s 6ms/step - loss: 1.3173 - acc: 0.4498 - val_loss: 1.5243 - val_acc: 0.4231\n",
      "Epoch 435/500\n",
      "468/468 [==============================] - ETA: 0s - loss: 1.3237 - acc: 0.4423\n",
      "Epoch 00435: val_loss did not improve from 1.43601\n",
      "468/468 [==============================] - 3s 6ms/step - loss: 1.3237 - acc: 0.4423 - val_loss: 1.5374 - val_acc: 0.3846\n",
      "Epoch 436/500\n",
      "468/468 [==============================] - ETA: 0s - loss: 1.3180 - acc: 0.4434\n",
      "Epoch 00436: val_loss did not improve from 1.43601\n",
      "468/468 [==============================] - 3s 7ms/step - loss: 1.3180 - acc: 0.4434 - val_loss: 1.4967 - val_acc: 0.3942\n",
      "Epoch 437/500\n",
      "465/468 [============================>.] - ETA: 0s - loss: 1.3242 - acc: 0.4570\n",
      "Epoch 00437: val_loss did not improve from 1.43601\n",
      "468/468 [==============================] - 3s 6ms/step - loss: 1.3251 - acc: 0.4562 - val_loss: 1.5177 - val_acc: 0.4135\n",
      "Epoch 438/500\n",
      "464/468 [============================>.] - ETA: 0s - loss: 1.3268 - acc: 0.4429\n",
      "Epoch 00438: val_loss did not improve from 1.43601\n",
      "468/468 [==============================] - 3s 6ms/step - loss: 1.3225 - acc: 0.4455 - val_loss: 1.5150 - val_acc: 0.4038\n",
      "Epoch 439/500\n",
      "464/468 [============================>.] - ETA: 0s - loss: 1.3225 - acc: 0.4688\n",
      "Epoch 00439: val_loss did not improve from 1.43601\n",
      "468/468 [==============================] - 3s 6ms/step - loss: 1.3240 - acc: 0.4690 - val_loss: 1.5271 - val_acc: 0.4135\n",
      "Epoch 440/500\n",
      "461/468 [============================>.] - ETA: 0s - loss: 1.3226 - acc: 0.4490\n",
      "Epoch 00440: val_loss did not improve from 1.43601\n",
      "468/468 [==============================] - 3s 6ms/step - loss: 1.3221 - acc: 0.4476 - val_loss: 1.5451 - val_acc: 0.4038\n",
      "Epoch 441/500\n",
      "461/468 [============================>.] - ETA: 0s - loss: 1.3151 - acc: 0.4360\n",
      "Epoch 00441: val_loss did not improve from 1.43601\n",
      "468/468 [==============================] - 3s 6ms/step - loss: 1.3218 - acc: 0.4359 - val_loss: 1.5693 - val_acc: 0.3942\n",
      "Epoch 442/500\n",
      "466/468 [============================>.] - ETA: 0s - loss: 1.3213 - acc: 0.4410\n",
      "Epoch 00442: val_loss did not improve from 1.43601\n",
      "468/468 [==============================] - 3s 6ms/step - loss: 1.3242 - acc: 0.4412 - val_loss: 1.5294 - val_acc: 0.4135\n",
      "Epoch 443/500\n",
      "463/468 [============================>.] - ETA: 0s - loss: 1.3202 - acc: 0.4708\n",
      "Epoch 00443: val_loss did not improve from 1.43601\n",
      "468/468 [==============================] - 3s 6ms/step - loss: 1.3223 - acc: 0.4701 - val_loss: 1.5086 - val_acc: 0.4038\n",
      "Epoch 444/500\n",
      "464/468 [============================>.] - ETA: 0s - loss: 1.3214 - acc: 0.4537\n",
      "Epoch 00444: val_loss did not improve from 1.43601\n",
      "468/468 [==============================] - 3s 6ms/step - loss: 1.3201 - acc: 0.4530 - val_loss: 1.5686 - val_acc: 0.3365\n",
      "Epoch 445/500\n",
      "468/468 [==============================] - ETA: 0s - loss: 1.3183 - acc: 0.4551\n",
      "Epoch 00445: val_loss did not improve from 1.43601\n",
      "468/468 [==============================] - 3s 6ms/step - loss: 1.3183 - acc: 0.4551 - val_loss: 1.5072 - val_acc: 0.4423\n",
      "Epoch 446/500\n",
      "467/468 [============================>.] - ETA: 0s - loss: 1.3240 - acc: 0.4507\n",
      "Epoch 00446: val_loss did not improve from 1.43601\n",
      "468/468 [==============================] - 3s 6ms/step - loss: 1.3232 - acc: 0.4519 - val_loss: 1.5235 - val_acc: 0.4327\n",
      "Epoch 447/500\n",
      "459/468 [============================>.] - ETA: 0s - loss: 1.3295 - acc: 0.4532\n",
      "Epoch 00447: val_loss did not improve from 1.43601\n",
      "468/468 [==============================] - 3s 6ms/step - loss: 1.3248 - acc: 0.4562 - val_loss: 1.5225 - val_acc: 0.3750\n",
      "Epoch 448/500\n",
      "465/468 [============================>.] - ETA: 0s - loss: 1.3221 - acc: 0.4581\n",
      "Epoch 00448: val_loss did not improve from 1.43601\n",
      "468/468 [==============================] - 3s 6ms/step - loss: 1.3197 - acc: 0.4594 - val_loss: 1.5580 - val_acc: 0.4231\n",
      "Epoch 449/500\n",
      "464/468 [============================>.] - ETA: 0s - loss: 1.3185 - acc: 0.4440\n",
      "Epoch 00449: val_loss did not improve from 1.43601\n",
      "468/468 [==============================] - 3s 6ms/step - loss: 1.3192 - acc: 0.4444 - val_loss: 1.5562 - val_acc: 0.4231\n",
      "Epoch 450/500\n",
      "464/468 [============================>.] - ETA: 0s - loss: 1.3200 - acc: 0.4591\n",
      "Epoch 00450: val_loss did not improve from 1.43601\n",
      "468/468 [==============================] - 3s 6ms/step - loss: 1.3201 - acc: 0.4594 - val_loss: 1.5979 - val_acc: 0.3462\n",
      "Epoch 451/500\n",
      "462/468 [============================>.] - ETA: 0s - loss: 1.3159 - acc: 0.4394\n",
      "Epoch 00451: val_loss did not improve from 1.43601\n",
      "468/468 [==============================] - 3s 6ms/step - loss: 1.3183 - acc: 0.4391 - val_loss: 1.5846 - val_acc: 0.3269\n",
      "Epoch 452/500\n",
      "463/468 [============================>.] - ETA: 0s - loss: 1.3229 - acc: 0.4428\n",
      "Epoch 00452: val_loss did not improve from 1.43601\n",
      "468/468 [==============================] - 3s 7ms/step - loss: 1.3227 - acc: 0.4423 - val_loss: 1.5263 - val_acc: 0.3654\n",
      "Epoch 453/500\n",
      "466/468 [============================>.] - ETA: 0s - loss: 1.3191 - acc: 0.4614\n",
      "Epoch 00453: val_loss did not improve from 1.43601\n",
      "468/468 [==============================] - 3s 7ms/step - loss: 1.3176 - acc: 0.4626 - val_loss: 1.4922 - val_acc: 0.4327\n",
      "Epoch 454/500\n",
      "462/468 [============================>.] - ETA: 0s - loss: 1.3153 - acc: 0.4556\n",
      "Epoch 00454: val_loss did not improve from 1.43601\n",
      "468/468 [==============================] - 3s 7ms/step - loss: 1.3232 - acc: 0.4541 - val_loss: 1.5189 - val_acc: 0.3846\n",
      "Epoch 455/500\n",
      "459/468 [============================>.] - ETA: 0s - loss: 1.3217 - acc: 0.4412\n",
      "Epoch 00455: val_loss did not improve from 1.43601\n",
      "468/468 [==============================] - 3s 6ms/step - loss: 1.3204 - acc: 0.4423 - val_loss: 1.5900 - val_acc: 0.3942\n",
      "Epoch 456/500\n",
      "463/468 [============================>.] - ETA: 0s - loss: 1.3167 - acc: 0.4557\n",
      "Epoch 00456: val_loss did not improve from 1.43601\n",
      "468/468 [==============================] - 3s 6ms/step - loss: 1.3162 - acc: 0.4562 - val_loss: 1.5088 - val_acc: 0.3750\n",
      "Epoch 457/500\n",
      "462/468 [============================>.] - ETA: 0s - loss: 1.3218 - acc: 0.4600\n",
      "Epoch 00457: val_loss did not improve from 1.43601\n",
      "468/468 [==============================] - 3s 6ms/step - loss: 1.3188 - acc: 0.4615 - val_loss: 1.5412 - val_acc: 0.3750\n",
      "Epoch 458/500\n",
      "460/468 [============================>.] - ETA: 0s - loss: 1.3219 - acc: 0.4728\n",
      "Epoch 00458: val_loss did not improve from 1.43601\n",
      "468/468 [==============================] - 3s 6ms/step - loss: 1.3209 - acc: 0.4733 - val_loss: 1.6045 - val_acc: 0.3558\n",
      "Epoch 459/500\n",
      "466/468 [============================>.] - ETA: 0s - loss: 1.3245 - acc: 0.4421\n",
      "Epoch 00459: val_loss did not improve from 1.43601\n",
      "468/468 [==============================] - 3s 6ms/step - loss: 1.3225 - acc: 0.4434 - val_loss: 1.4717 - val_acc: 0.3558\n",
      "Epoch 460/500\n",
      "464/468 [============================>.] - ETA: 0s - loss: 1.3123 - acc: 0.4558\n",
      "Epoch 00460: val_loss did not improve from 1.43601\n",
      "468/468 [==============================] - 3s 6ms/step - loss: 1.3128 - acc: 0.4562 - val_loss: 1.4937 - val_acc: 0.4231\n",
      "Epoch 461/500\n",
      "465/468 [============================>.] - ETA: 0s - loss: 1.3275 - acc: 0.4645\n",
      "Epoch 00461: val_loss did not improve from 1.43601\n",
      "468/468 [==============================] - 3s 6ms/step - loss: 1.3262 - acc: 0.4647 - val_loss: 1.5270 - val_acc: 0.4519\n",
      "Epoch 462/500\n",
      "462/468 [============================>.] - ETA: 0s - loss: 1.3084 - acc: 0.4556\n",
      "Epoch 00462: val_loss did not improve from 1.43601\n",
      "468/468 [==============================] - 3s 6ms/step - loss: 1.3183 - acc: 0.4551 - val_loss: 1.4667 - val_acc: 0.4423\n",
      "Epoch 463/500\n",
      "468/468 [==============================] - ETA: 0s - loss: 1.3182 - acc: 0.4605\n",
      "Epoch 00463: val_loss did not improve from 1.43601\n",
      "468/468 [==============================] - 3s 6ms/step - loss: 1.3182 - acc: 0.4605 - val_loss: 1.5239 - val_acc: 0.4135\n",
      "Epoch 464/500\n",
      "465/468 [============================>.] - ETA: 0s - loss: 1.3245 - acc: 0.4613\n",
      "Epoch 00464: val_loss did not improve from 1.43601\n",
      "468/468 [==============================] - 3s 6ms/step - loss: 1.3235 - acc: 0.4605 - val_loss: 1.5016 - val_acc: 0.4327\n",
      "Epoch 465/500\n",
      "460/468 [============================>.] - ETA: 0s - loss: 1.3243 - acc: 0.4435\n",
      "Epoch 00465: val_loss did not improve from 1.43601\n",
      "468/468 [==============================] - 3s 6ms/step - loss: 1.3182 - acc: 0.4487 - val_loss: 1.5303 - val_acc: 0.4038\n",
      "Epoch 466/500\n",
      "462/468 [============================>.] - ETA: 0s - loss: 1.3207 - acc: 0.4535\n",
      "Epoch 00466: val_loss did not improve from 1.43601\n",
      "468/468 [==============================] - 3s 6ms/step - loss: 1.3217 - acc: 0.4509 - val_loss: 1.5191 - val_acc: 0.3942\n",
      "Epoch 467/500\n",
      "466/468 [============================>.] - ETA: 0s - loss: 1.3148 - acc: 0.4292\n",
      "Epoch 00467: val_loss did not improve from 1.43601\n",
      "468/468 [==============================] - 3s 6ms/step - loss: 1.3186 - acc: 0.4284 - val_loss: 1.4877 - val_acc: 0.4423\n",
      "Epoch 468/500\n",
      "467/468 [============================>.] - ETA: 0s - loss: 1.3198 - acc: 0.4518\n",
      "Epoch 00468: val_loss did not improve from 1.43601\n",
      "468/468 [==============================] - 3s 6ms/step - loss: 1.3197 - acc: 0.4519 - val_loss: 1.5587 - val_acc: 0.4135\n",
      "Epoch 469/500\n",
      "459/468 [============================>.] - ETA: 0s - loss: 1.3139 - acc: 0.4597\n",
      "Epoch 00469: val_loss did not improve from 1.43601\n",
      "468/468 [==============================] - 3s 6ms/step - loss: 1.3137 - acc: 0.4605 - val_loss: 1.5568 - val_acc: 0.3558\n",
      "Epoch 470/500\n",
      "464/468 [============================>.] - ETA: 0s - loss: 1.3181 - acc: 0.4537\n",
      "Epoch 00470: val_loss did not improve from 1.43601\n",
      "468/468 [==============================] - 3s 6ms/step - loss: 1.3199 - acc: 0.4509 - val_loss: 1.4998 - val_acc: 0.4135\n",
      "Epoch 471/500\n",
      "464/468 [============================>.] - ETA: 0s - loss: 1.3184 - acc: 0.4644\n",
      "Epoch 00471: val_loss did not improve from 1.43601\n",
      "468/468 [==============================] - 3s 6ms/step - loss: 1.3188 - acc: 0.4658 - val_loss: 1.5175 - val_acc: 0.4327\n",
      "Epoch 472/500\n",
      "466/468 [============================>.] - ETA: 0s - loss: 1.3180 - acc: 0.4517\n",
      "Epoch 00472: val_loss did not improve from 1.43601\n",
      "468/468 [==============================] - 3s 6ms/step - loss: 1.3161 - acc: 0.4530 - val_loss: 1.4666 - val_acc: 0.4038\n",
      "Epoch 473/500\n",
      "464/468 [============================>.] - ETA: 0s - loss: 1.3195 - acc: 0.4677\n",
      "Epoch 00473: val_loss did not improve from 1.43601\n",
      "468/468 [==============================] - 3s 6ms/step - loss: 1.3148 - acc: 0.4690 - val_loss: 1.5394 - val_acc: 0.4231\n",
      "Epoch 474/500\n",
      "461/468 [============================>.] - ETA: 0s - loss: 1.3280 - acc: 0.4501\n",
      "Epoch 00474: val_loss did not improve from 1.43601\n",
      "468/468 [==============================] - 3s 6ms/step - loss: 1.3267 - acc: 0.4519 - val_loss: 1.4997 - val_acc: 0.4038\n",
      "Epoch 475/500\n",
      "468/468 [==============================] - ETA: 0s - loss: 1.3118 - acc: 0.4541\n",
      "Epoch 00475: val_loss did not improve from 1.43601\n",
      "468/468 [==============================] - 3s 6ms/step - loss: 1.3118 - acc: 0.4541 - val_loss: 1.5365 - val_acc: 0.3942\n",
      "Epoch 476/500\n",
      "466/468 [============================>.] - ETA: 0s - loss: 1.3150 - acc: 0.4506\n",
      "Epoch 00476: val_loss did not improve from 1.43601\n",
      "468/468 [==============================] - 3s 6ms/step - loss: 1.3169 - acc: 0.4498 - val_loss: 1.5670 - val_acc: 0.3942\n",
      "Epoch 477/500\n",
      "466/468 [============================>.] - ETA: 0s - loss: 1.3185 - acc: 0.4388\n",
      "Epoch 00477: val_loss did not improve from 1.43601\n",
      "468/468 [==============================] - 3s 6ms/step - loss: 1.3200 - acc: 0.4391 - val_loss: 1.4598 - val_acc: 0.4038\n",
      "Epoch 478/500\n",
      "466/468 [============================>.] - ETA: 0s - loss: 1.3070 - acc: 0.4506\n",
      "Epoch 00478: val_loss did not improve from 1.43601\n",
      "468/468 [==============================] - 3s 6ms/step - loss: 1.3100 - acc: 0.4498 - val_loss: 1.4949 - val_acc: 0.3942\n",
      "Epoch 479/500\n",
      "463/468 [============================>.] - ETA: 0s - loss: 1.3271 - acc: 0.4654\n",
      "Epoch 00479: val_loss did not improve from 1.43601\n",
      "468/468 [==============================] - 3s 6ms/step - loss: 1.3282 - acc: 0.4658 - val_loss: 1.5198 - val_acc: 0.4038\n",
      "Epoch 480/500\n",
      "468/468 [==============================] - ETA: 0s - loss: 1.3199 - acc: 0.4423\n",
      "Epoch 00480: val_loss did not improve from 1.43601\n",
      "468/468 [==============================] - 3s 6ms/step - loss: 1.3199 - acc: 0.4423 - val_loss: 1.5462 - val_acc: 0.3846\n",
      "Epoch 481/500\n",
      "464/468 [============================>.] - ETA: 0s - loss: 1.3201 - acc: 0.4623\n",
      "Epoch 00481: val_loss did not improve from 1.43601\n",
      "468/468 [==============================] - 3s 6ms/step - loss: 1.3189 - acc: 0.4615 - val_loss: 1.5243 - val_acc: 0.3942\n",
      "Epoch 482/500\n",
      "466/468 [============================>.] - ETA: 0s - loss: 1.3210 - acc: 0.4442\n",
      "Epoch 00482: val_loss did not improve from 1.43601\n",
      "468/468 [==============================] - 3s 6ms/step - loss: 1.3207 - acc: 0.4434 - val_loss: 1.5245 - val_acc: 0.4327\n",
      "Epoch 483/500\n",
      "460/468 [============================>.] - ETA: 0s - loss: 1.3183 - acc: 0.4543\n",
      "Epoch 00483: val_loss did not improve from 1.43601\n",
      "468/468 [==============================] - 3s 6ms/step - loss: 1.3183 - acc: 0.4541 - val_loss: 1.4869 - val_acc: 0.4231\n",
      "Epoch 484/500\n",
      "462/468 [============================>.] - ETA: 0s - loss: 1.3244 - acc: 0.4470\n",
      "Epoch 00484: val_loss did not improve from 1.43601\n",
      "468/468 [==============================] - 3s 6ms/step - loss: 1.3202 - acc: 0.4466 - val_loss: 1.4964 - val_acc: 0.3942\n",
      "Epoch 485/500\n",
      "466/468 [============================>.] - ETA: 0s - loss: 1.3095 - acc: 0.4635\n",
      "Epoch 00485: val_loss did not improve from 1.43601\n",
      "468/468 [==============================] - 3s 6ms/step - loss: 1.3121 - acc: 0.4626 - val_loss: 1.5232 - val_acc: 0.4135\n",
      "Epoch 486/500\n",
      "462/468 [============================>.] - ETA: 0s - loss: 1.3166 - acc: 0.4578\n",
      "Epoch 00486: val_loss did not improve from 1.43601\n",
      "468/468 [==============================] - 3s 6ms/step - loss: 1.3194 - acc: 0.4562 - val_loss: 1.4998 - val_acc: 0.4038\n",
      "Epoch 487/500\n",
      "465/468 [============================>.] - ETA: 0s - loss: 1.3123 - acc: 0.4495\n",
      "Epoch 00487: val_loss did not improve from 1.43601\n",
      "468/468 [==============================] - 3s 6ms/step - loss: 1.3123 - acc: 0.4476 - val_loss: 1.5506 - val_acc: 0.3173\n",
      "Epoch 488/500\n",
      "461/468 [============================>.] - ETA: 0s - loss: 1.3081 - acc: 0.4642\n",
      "Epoch 00488: val_loss did not improve from 1.43601\n",
      "468/468 [==============================] - 3s 6ms/step - loss: 1.3141 - acc: 0.4605 - val_loss: 1.4888 - val_acc: 0.3750\n",
      "Epoch 489/500\n",
      "465/468 [============================>.] - ETA: 0s - loss: 1.3221 - acc: 0.4409\n",
      "Epoch 00489: val_loss did not improve from 1.43601\n",
      "468/468 [==============================] - 3s 6ms/step - loss: 1.3189 - acc: 0.4423 - val_loss: 1.4744 - val_acc: 0.4327\n",
      "Epoch 490/500\n",
      "464/468 [============================>.] - ETA: 0s - loss: 1.3115 - acc: 0.4623\n",
      "Epoch 00490: val_loss did not improve from 1.43601\n",
      "468/468 [==============================] - 3s 6ms/step - loss: 1.3111 - acc: 0.4615 - val_loss: 1.5622 - val_acc: 0.4135\n",
      "Epoch 491/500\n",
      "467/468 [============================>.] - ETA: 0s - loss: 1.3183 - acc: 0.4368\n",
      "Epoch 00491: val_loss did not improve from 1.43601\n",
      "468/468 [==============================] - 3s 6ms/step - loss: 1.3178 - acc: 0.4370 - val_loss: 1.4996 - val_acc: 0.4327\n",
      "Epoch 492/500\n",
      "467/468 [============================>.] - ETA: 0s - loss: 1.3170 - acc: 0.4732\n",
      "Epoch 00492: val_loss did not improve from 1.43601\n",
      "468/468 [==============================] - 3s 6ms/step - loss: 1.3157 - acc: 0.4733 - val_loss: 1.5698 - val_acc: 0.3654\n",
      "Epoch 493/500\n",
      "460/468 [============================>.] - ETA: 0s - loss: 1.3218 - acc: 0.4511\n",
      "Epoch 00493: val_loss did not improve from 1.43601\n",
      "468/468 [==============================] - 3s 6ms/step - loss: 1.3188 - acc: 0.4530 - val_loss: 1.5437 - val_acc: 0.4135\n",
      "Epoch 494/500\n",
      "460/468 [============================>.] - ETA: 0s - loss: 1.3099 - acc: 0.4500\n",
      "Epoch 00494: val_loss did not improve from 1.43601\n",
      "468/468 [==============================] - 3s 6ms/step - loss: 1.3126 - acc: 0.4487 - val_loss: 1.5675 - val_acc: 0.3654\n",
      "Epoch 495/500\n",
      "462/468 [============================>.] - ETA: 0s - loss: 1.3133 - acc: 0.4578\n",
      "Epoch 00495: val_loss did not improve from 1.43601\n",
      "468/468 [==============================] - 3s 6ms/step - loss: 1.3126 - acc: 0.4583 - val_loss: 1.4925 - val_acc: 0.3846\n",
      "Epoch 496/500\n",
      "467/468 [============================>.] - ETA: 0s - loss: 1.3227 - acc: 0.4497\n",
      "Epoch 00496: val_loss did not improve from 1.43601\n",
      "468/468 [==============================] - 3s 6ms/step - loss: 1.3221 - acc: 0.4498 - val_loss: 1.5066 - val_acc: 0.4519\n",
      "Epoch 497/500\n",
      "466/468 [============================>.] - ETA: 0s - loss: 1.3156 - acc: 0.4667\n",
      "Epoch 00497: val_loss did not improve from 1.43601\n",
      "468/468 [==============================] - 3s 6ms/step - loss: 1.3154 - acc: 0.4669 - val_loss: 1.4853 - val_acc: 0.3846\n",
      "Epoch 498/500\n",
      "461/468 [============================>.] - ETA: 0s - loss: 1.3211 - acc: 0.4620\n",
      "Epoch 00498: val_loss did not improve from 1.43601\n",
      "468/468 [==============================] - 3s 6ms/step - loss: 1.3155 - acc: 0.4647 - val_loss: 1.4911 - val_acc: 0.4519\n",
      "Epoch 499/500\n",
      "463/468 [============================>.] - ETA: 0s - loss: 1.3136 - acc: 0.4525\n",
      "Epoch 00499: val_loss did not improve from 1.43601\n",
      "468/468 [==============================] - 3s 6ms/step - loss: 1.3129 - acc: 0.4519 - val_loss: 1.5337 - val_acc: 0.3846\n",
      "Epoch 500/500\n",
      "463/468 [============================>.] - ETA: 0s - loss: 1.3111 - acc: 0.4579\n",
      "Epoch 00500: val_loss did not improve from 1.43601\n",
      "468/468 [==============================] - 3s 6ms/step - loss: 1.3157 - acc: 0.4551 - val_loss: 1.5168 - val_acc: 0.4038\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(X_train, Y_train, \n",
    "                    validation_data=(X_valid, Y_valid), \n",
    "                    epochs=500, \n",
    "                    batch_size=2,\n",
    "                    callbacks=[checkpoint])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAEGCAYAAABo25JHAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/YYfK9AAAACXBIWXMAAAsTAAALEwEAmpwYAABAw0lEQVR4nO2dd5xU1dnHv8+U7Y2FhYVdunQQFLBGwPKKvSdi7DH6GlM0xagxUdNeEzUmJjGW2CMajD3WWEFigQWpUgWBpe0usI3tM+f948zduTM7W4AdlmWe7+ezn7lz59y559y9c37nKedcMcagKIqiJC6erq6AoiiK0rWoECiKoiQ4KgSKoigJjgqBoihKgqNCoCiKkuD4uroCe0qvXr3MoEGDuroaiqIo3YoFCxaUGWPyYn3W7YRg0KBBFBUVdXU1FEVRuhUisqG1z9Q1pCiKkuCoECiKoiQ4KgSKoigJTreLESiKkpg0NjZSXFxMXV1dV1flgCYlJYXCwkL8fn+Hj1EhUBSlW1BcXExmZiaDBg1CRLq6Ogckxhh27NhBcXExgwcP7vBx6hpSFKVbUFdXR8+ePVUE2kBE6Nmz5x5bTSoEiqJ0G1QE2mdvrlHCCMGqbVX84T+rKKuu7+qqKIqiHFAkjBCsLanmL++vZUd1Q1dXRVGUbkpGRkZXVyEuJIwQeEMtDeqDeBRFUSJIGCFw/GaBoAqBoij7hjGGG2+8kbFjxzJu3DhmzZoFwNatW5kyZQoTJkxg7NixfPTRRwQCAa644ormsn/84x+7uPYtSZj0UW9ICNQiUJTuzy//vZwvtlR26neO7pfF7WeO6VDZF198kUWLFrF48WLKysqYPHkyU6ZM4ZlnnmH69OnceuutBAIBampqWLRoEZs3b2bZsmUAlJeXd2q9O4OEsQi8HkcIurgiiqJ0e+bOnctFF12E1+ulT58+TJ06lfnz5zN58mQef/xx7rjjDpYuXUpmZiZDhgxh3bp1fP/73+ett94iKyurq6vfgoSxCJyMKnUNKUr3p6Mj93hhWvEsTJkyhTlz5vD6669z6aWXcuONN3LZZZexePFi3n77be6//36ee+45Hnvssf1c47ZJQItAhUBRlH1jypQpzJo1i0AgQGlpKXPmzOGII45gw4YN9O7dm6uvvpqrrrqKhQsXUlZWRjAY5Pzzz+fXv/41Cxcu7OrqtyBhLAKvBosVRekkzj33XD755BPGjx+PiHDXXXeRn5/Pk08+yd13343f7ycjI4OnnnqKzZs3c+WVVxIMBgG48847u7j2LYmbEIhIf+ApIB8IAg8bY+6LUW4a8CfAD5QZY6bGoz4etQgURdlHqqurAZuFePfdd3P33XdHfH755Zdz+eWXtzjuQLQC3MTTImgCfmyMWSgimcACEXnHGPOFU0BEcoC/AacYYzaKSO94VcbjZA0F43UGRVGU7kncYgTGmK3GmIWh7SpgBVAQVeybwIvGmI2hciXxqo8zoSygFoGiKEoE+yVYLCKDgMOAz6I+Gg70EJEPRWSBiFwWrzp4dB6BoihKTOIeLBaRDOAF4AZjTPQMEB8wETgRSAU+EZFPjTGro77jGuAagAEDBuxVPcKuIRUCRVEUN3G1CETEjxWBmcaYF2MUKQbeMsbsNsaUAXOA8dGFjDEPG2MmGWMm5eXl7VVdnPRRzRpSFEWJJG5CIHZxn0eBFcaYe1sp9gpwnIj4RCQNOBIbS+h01DWkKIoSm3i6ho4FLgWWisii0L6fAQMAjDEPGmNWiMhbwBJsiukjxphl8aiMp3n10Xh8u6IoSvclbkJgjJkLtPuoHGPM3cDd7ZXbV3RCmaIo+5OMjIzmeQfRfPXVV5xxxhnNC9F1NQmzxIROKFMURYlNwi0xoUKgKAcBb94M25Z27nfmj4NTf9fqxzfddBMDBw7kuuuuA+COO+5ARJgzZw67du2isbGR3/zmN5x99tl7dNq6ujq+853vUFRUhM/n49577+X4449n+fLlXHnllTQ0NBAMBnnhhRfo168f3/jGNyguLiYQCPCLX/yCCy+8cJ+aDQkkBJ5m11AXV0RRlG7JjBkzuOGGG5qF4LnnnuOtt97ihz/8IVlZWZSVlXHUUUdx1lln7dED5O+//34Ali5dysqVKzn55JNZvXo1Dz74INdffz0XX3wxDQ0NBAIB3njjDfr168frr78OQEVFRae0LXGEwAkWa4xAUbo/bYzc48Vhhx1GSUkJW7ZsobS0lB49etC3b19++MMfMmfOHDweD5s3b2b79u3k5+d3+Hvnzp3L97//fQBGjhzJwIEDWb16NUcffTS//e1vKS4u5rzzzmPYsGGMGzeOn/zkJ9x0002cccYZHHfccZ3StoSJEegy1Iqi7CsXXHABzz//PLNmzWLGjBnMnDmT0tJSFixYwKJFi+jTpw91dXV79J2tPdvgm9/8Jq+++iqpqalMnz6d999/n+HDh7NgwQLGjRvHLbfcwq9+9avOaFYCWQSOa0iFQFGUvWTGjBlcffXVlJWVMXv2bJ577jl69+6N3+/ngw8+YMOGDXv8nVOmTGHmzJmccMIJrF69mo0bNzJixAjWrVvHkCFD+MEPfsC6detYsmQJI0eOJDc3l0suuYSMjAyeeOKJTmlXwgmBuoYURdlbxowZQ1VVFQUFBfTt25eLL76YM888k0mTJjFhwgRGjhy5x9953XXXce211zJu3Dh8Ph9PPPEEycnJzJo1i6effhq/309+fj633XYb8+fP58Ybb8Tj8eD3+3nggQc6pV3SmllyoDJp0iRTVFS0x8ft3N3A4b9+hzvOHM0Vxw6OQ80URYknK1asYNSoUV1djW5BrGslIguMMZNilU+YGIFv93ameT7H07i7q6uiKIpyQJEwriHf5k95Iulunq89Ehjb1dVRFCUBWLp0KZdeemnEvuTkZD77LHpF/q4lYYTA47VNNcGmLq6Joih7izFmj3L0u5px48axaNGi/XrOvXH3J4xryOMJaV4w0LUVURRlr0hJSWHHjh171dElCsYYduzYQUpKyh4dlzAWgXi9dkOFQFG6JYWFhRQXF1NaWtrVVTmgSUlJobCwcI+OSRgh8HisEBgVAkXplvj9fgYP1oy/eJA4rqHmGIEKgaIoipuEEQLxOK4hDRYriqK4SRghQEKuIaMWgaIoipvEEQKP4xrSdagVRVHcJJAQWIvAo64hRVGUCBJHCESzhhRFUWIRNyEQkf4i8oGIrBCR5SJyfRtlJ4tIQEQuiFd9nCfTaIxAURQlknjOI2gCfmyMWSgimcACEXnHGPOFu5CIeIHfA2/HsS7NFoEYjREoiqK4iZtFYIzZaoxZGNquAlYABTGKfh94ASiJV12A5hiBzixWFEWJZL/ECERkEHAY8FnU/gLgXODB+FdCYwSKoiixiLsQiEgGdsR/gzGmMurjPwE3mXYc9yJyjYgUiUjRXq8z4lgEGiNQFEWJIK5rDYmIHysCM40xL8YoMgn4Z2hZ2V7AaSLSZIx52V3IGPMw8DDYJ5TtVWVCQiBqESiKokQQNyEQ27s/Cqwwxtwbq4wxZrCr/BPAa9Ei0HkVUotAURQlFvG0CI4FLgWWisii0L6fAQMAjDHxjwu40WCxoihKTOImBMaYuUCHHyVkjLkiXnUBXOmjKgSKoihuEmdmsQaLFUVRYpI4QqAWgaIoSkwSRwhCS0ygq48qiqJEkDhCoFlDiqIoMUkcIQg9j0DXGlIURYkkgYTAiRHo8wgURVHcJI4Q6OqjiqIoMUkcIdAlJhRFUWKSOEIgQhABtQgURVEiSBwhAIJ4dB6BoihKFIknBKgQKIqiuEksIRAPohPKFEVRIkgsIcCrFoGiKEoUiSUE4tUYgaIoShQJJQQGj84jUBRFiSKhhCAomjWkKIoSTUIJgUFdQ4qiKNEklBBYi0BdQ4qiKG4SSgiMCoGiKEoLEkwI1DWkKIoSTdyEQET6i8gHIrJCRJaLyPUxylwsIktCfx+LyPh41Qds+qhH5xEoiqJE4IvjdzcBPzbGLBSRTGCBiLxjjPnCVWY9MNUYs0tETgUeBo6MV4UMHjzqGlIURYkgbkJgjNkKbA1tV4nICqAA+MJV5mPXIZ8ChfGqD6hrSFEUJRb7JUYgIoOAw4DP2ih2FfBmK8dfIyJFIlJUWlq61/Uw4kFQi0BRFMVN3IVARDKAF4AbjDGVrZQ5HisEN8X63BjzsDFmkjFmUl5e3l7XxYhXXUOKoihRxDNGgIj4sSIw0xjzYitlDgUeAU41xuyIZ30QjREoiqJEE8+sIQEeBVYYY+5tpcwA4EXgUmPM6njVxcFo1pCiKEoL4mkRHAtcCiwVkUWhfT8DBgAYYx4EbgN6An+zukGTMWZSvCpkYwSN8fp6RVGUbkk8s4bmAtJOmW8D345XHVqcz+PDSz3BoMHjabNqiqIoCUNCzSxGvHgJEjCmq2uiKIpywJBQQmDEi48AgaAKgaIoikNiCYHHi4egCoGiKIqLxBIC8eEnoK4hRVEUF4klBB4fXgIEAioEiqIoDgklBHh8NkagFoGiKEozCSUExhECjREoiqI0k1BCgMeHT1QIFEVR3CSeEGjWkKIoSgQJJwRedQ0piqJEkFBCYDw2fbRJhUBRFKWZhBIC8frxEiCoWUOKoijNJJQQOOmjTTqPQFEUpZkEEwI/PrUIFEVRIkgsIfB68YqhKaAPp1EURXFIKCEQjx+AYJM+nEZRFMUhoYQAj30OTzDQ1MUVURRFOXBIKCEQb0gImhq6uCaKoigHDh0SAhG5XkSyxPKoiCwUkZPjXbnORrwh15BaBIqiKM101CL4ljGmEjgZyAOuBH4Xt1rFC8ciCGiMQFEUxaGjQuA86f004HFjzGLaeTC9iPQXkQ9EZIWILBeR62OUERH5s4isFZElInL4nlV/z3AsAqNCoCiK0kxHhWCBiPwHKwRvi0gmEGznmCbgx8aYUcBRwHdFZHRUmVOBYaG/a4AHOlzzvcDj1awhRVGUaHwdLHcVMAFYZ4ypEZFcrHuoVYwxW4Gtoe0qEVkBFABfuIqdDTxljDHApyKSIyJ9Q8d2Ph61CBRFUaLpqEVwNLDKGFMuIpcAPwcqOnoSERkEHAZ8FvVRAbDJ9b44tC/6+GtEpEhEikpLSzt62hZ4fFb3jAaLFUVRmumoEDwA1IjIeOCnwAbgqY4cKCIZwAvADaGAc8THMQ5psf6DMeZhY8wkY8ykvLy8DlY5Rl08GixWFEWJpqNC0BRy35wN3GeMuQ/IbO8gEfFjRWCmMebFGEWKgf6u94XAlg7WaY/x+EKuoaBaBIqiKA4dFYIqEbkFuBR4XUS8gL+tA0REgEeBFcaYe1sp9ipwWSh76CigIm7xAcLBYqPBYkVRlGY6Giy+EPgmdj7BNhEZANzdzjHHYoVjqYgsCu37GTAAwBjzIPAGNhNpLVBDOwHofcWjE8oURVFa0CEhCHX+M4HJInIGMM8Y02aMwBgzl3bmGoTcTd/taGX3FZ/fyRrSJSYURVEcOrrExDeAecDXgW8An4nIBfGsWDzw+pMACDSpRaAoiuLQUdfQrcBkY0wJgIjkAe8Cz8erYvHA59MJZYqiKNF0NFjscUQgxI49OPaAwR+yCDR9VFEUJUxHLYK3RORt4NnQ+wuxgd5uhdexCFQIFEVRmulosPhGETkfmwkkwMPGmJfiWrN44EwoU9eQoihKMx21CDDGvICdHNZ9cR5VqRPKFEVRmmlTCESkihhLPmCtAmOMyYpLreKFx2tf1TWkKIrSTJtCYIxpdxmJboXOLFYURWlBt8v82Se8yfZVJ5QpiqI0k1hC4LNC4AnUd3FFFEVRDhwSTAhSABAVAkVRlGYSSwi8foIInqAKgaIoikNiCYEIjfjxaoxAURSlmcQSAqBJkvAEVQgURVEcEk4IGlUIFEVRIkg4IWjy+PFpjEBRFKWZxBMCScIb1AlliqIoDgknBAFPEj6jFoGiKIpDQgqBX2MEiqIozSSgECTjQ11DiqIoDnETAhF5TERKRGRZK59ni8i/RWSxiCwXkSvjVRc3AU8SfqMWgaIoikM8LYIngFPa+Py7wBfGmPHANOAPIpIUx/oAYLxJ+I1aBIqiKA5xEwJjzBxgZ1tFgEwRESAjVDbuT4wJepPxq2tIURSlma6MEfwVGAVsAZYC1xtjgrEKisg1IlIkIkWlpaX7dFLjTSbZNNAYiHkqRVGUhKMrhWA6sAjoB0wA/ioiMZ94Zox52BgzyRgzKS8vb9/O6ksmWRqpaQjs2/coiqIcJHSlEFwJvGgsa4H1wMh4n1T8KSTRRK0KgaIoCtC1QrAROBFARPoAI4B18T6px59CMo3UNqoQKIqiQDvPLN4XRORZbDZQLxEpBm4H/ADGmAeBXwNPiMhSQICbjDFl8aqPgxWCBmrqNWCsKIoCcRQCY8xF7Xy+BTg5XudvDUnJxiuG+ppKIGd/n15RFOWAI+FmFnvScwForNrRxTVRFEU5MEhAIegFQKA67l4oRVGUbkHCCYEvwwqBqVEhUBRFgQQUgqQsKwTUtDXpWVEUJXFIQCGwE9I8tbu6uCaKoigHBgknBGmZuQSM4K1Ti0BRFAUSUAh8Ph8VZOCrV4tAURQFElAIAMoli6SG8q6uhqIoygFBQgpBpWSR0qgWgaIoCiSoENR4s0hpLO/qaiiKohwQJKQQ1PpzSGuq7OpqKIqiHBAkpBA0JOWQGawAY7q6KoqiKF1OQgpBY3IP/DRBQ3VXV0VRFKXLSUghCKTYhed0drGiKEqCCgFpPQEI6sJziqIoiSkEkm6Xmair2NbFNVEURel6ElIIvFl9AKgvVyFQFEVJSCFIys4HoLFiaxfXRFEUpetJSCHIysyk0qTRVLm9q6uiKIrS5SSkEPTOSqbUZBOsUiFQFEWJmxCIyGMiUiIiy9ooM01EFonIchGZHa+6RNMnK4UysvHsLtlfp1QURTlgiadF8ARwSmsfikgO8DfgLGPMGODrcaxLBBnJPnZILqm1GixWFGUf2b0DSlZ0dS32ibgJgTFmDtDWjK1vAi8aYzaGyu/X4XlJ8kB6NGyF56+CYHB/nlrpCsrWgroClX2ldhes/yhy39+Osn/dmK6MEQwHeojIhyKyQEQua62giFwjIkUiUlRaWtopJ9+VNthuLHseKjZ2ynfGjfuPhKLHuroW3Zu/ToQ/jLDbS5+HNe90bX2U7skbP4Unz4Cd6yHQBDvXwUHgYu5KIfABE4HTgenAL0RkeKyCxpiHjTGTjDGT8vLyOuXkDTmHhN98+T5s+LhTvrfTCTRB6UrYNL+ra3IQEFpk8IWrYOYFXVsVh7pKWPVWV9dC6Si7QwPRVW/Cf34Ofz6sa+vTSXSlEBQDbxljdhtjyoA5wPj9dfKk/JGsCA6wb177ITx+avxPWroaXroWAo3tly3fBHPuDi+MV7EpvnUD6zpR98n+5fHT4NkLdd2rjrK7zLpnugp/qn0tngdr3438LNC0/+vTSXSlELwCHCciPhFJA44E9lvEpaBnFqc1/B9Bb0p4Z8Pu8LYxsPAfdsTm0FQPc+6Bpoa9O+nL34HFz8KWz9sv+/Yt8P5vrLUCUN4B91X5pr2zHBY9A0+eCX8Ybv/2Nzu+hOrOcfm1y871+3ZseScKcjAA25fa7fo9fD7Gpw9C2Zo9O6Z8I3z135b7i4vgzZu7x7Lsdw+Fu4ZG7vv4r/Gx6HdtsH1C7a7wtakKJZjU7AQTFVsM1HfeucvWhs+1H4hn+uizwCfACBEpFpGrRORaEbkWwBizAngLWALMAx4xxrSaatrZ9M9Nw+ChNqN/eKe7s/1qLrz6PXj1+1YAAD79G7z/a5j/9707qddvXwMdEJKkDPu68VP7WrnZdhxt8aex8OhJe16vl78D6+fs+XEAd2TD27fu3bEOfzkc/jRu744NBm0d5v6pjTKu6/bnCa2XCzRZi61kZezP/zzBXuPOorY8vN1Q0/HjGmrgrZusNbEn/HUyPBHjmMemw2cPRA6EDmSM6/9pDPzn1s636IMBuO9QePh4+P0gmPew3e90zrW7WgpBUweEYP4j8MK32y5Tvikc07r/yD2u+t4Qz6yhi4wxfY0xfmNMoTHmUWPMg8aYB11l7jbGjDbGjDXG/CledYlF/1xr4u1MLgzv3LUhvO38w794GZ46B7YthXfvsPsa9+BH68YRgsba9sv60+zr+tD0imATVHfQbROdBVVf1XrnBpBV2Ppnbhp2R1pDdRX29ZO/dux4h/Uf2aCbmybXNamrsFZCNMFAS7dAXbl9ff83rZ+vqS72/ugR8Pal1mJz/1CDgfYFeP0cWPZiy/01O6F0VexjGuvg3dtc7/fgnqoNuZF276EV5VyH6PsvGHJpONeyo2xdDG/c2LVZd8492Nk4lmNZ6P+34t+w5l2o2mLfxxKC6Ou6eQG8+L+R9/Lq/4St/FjU7IwcbJS28bvtRBJyZjFAv+xUMpJ9bDD54Z0vXmMzSl77EdS7brCNH8MTp4ffyx5etnWzrZB4k+1790iwNZwf5Y614X1tuSXc31lXDp89DA9Ps+9fvAb+dqTtfGLRa1jk+9bK/V+/yOvgFs43fgq7vgq/L11tR+pbFrX8nifPgHkPxfapBhrhnhHWSojuqD+8047OdruWD3e2vUn2tXYXfHJ/uPMOBuHD38VuT9FjkT9Sp0MTV5nHT4UHvxb7+C9esaPyJ8+E569s+fnfT4D7j4h97LyH4fOnw+/3ZDTe3DnspSunupUsl47cl26eudC2o7WsmWCg9UHPtmW2s92yyHbmu8tg42ew4jX7f1/1lh28NNa1LqbQ8cGRm4Ya+9vYNK/1MiXLW+778E77mlUY6S5yiB5wrHwdlvzTxvocyje2fk2Wv9RlSSsJKwQejzC2IIulNT3CO+srbEZJ0aMtffLuWMGeCsFTZ9nOxOmsNn5iBactnJFv0NVZfv6P1uMTbn9xzQ5480YbizAGiueHj3du3v/8HB45CbYvjxQbgN/2sZ30B3eGr4MTzCx2/XjKXUIw7yF44NiwJbXqDfv6xk/CHcznM2HBk+FjGqpszKX5fY3txB3rYOe68GfbloZ/UAuecLU1JAS+0LV95Xvw9s/sNQb7+vGficnrP7KC4whSs/i7lGDTZ1DyhR3dRfPcZbDB5XOP7hh2hUaVjsDUV4UFKtoC6KgQfPkBPH1+++XqKqx708Etuq11nrGCsMFA+0HQ1oK3r/4Afpsf+7MHj7WutoenwrMX2d/dYyfDrIvtNX32Qjt4efsWK6bVJZGWx+JZkX50b5KNfzgutjXvRAbgq7aH41DbltrfxptRVqmb7V+03FezA0adBROvsEkc0R1/tGvI6TMcy82YkBDU2Lr8cVzkffWvK2z7o+mIy2kfSVghABhfmMMnFaGnlR11XeSHa9+LKu3+kUvkRzO/Dv+M8Q+Mxuuzr0WP2hs/mjn32FF0U0Ps0dnn/4D/3hf7u2tdN717xNxYC1n97PYbP7EdG8DHf7EC8cAxsTOSih6D2b+D2XfZ907H6sZtAYD9cdw3ASo2w4LH7b7i+XbkaAy8ch38+wfh8uvn2JiLw8vXwru3h9+7fyT/ODe87c7WiLYInP+bM2p2d4atsW0J/GUSPP8t+17EBt1XvBYu47bGWsv6qquwfzvXR9a9vtJekzsLwy4sjy/yWLcwbFkE82LEoYqLYMWrLfcHA/aYNe+GO8IXr7HWmzNAqNwcLu90nju+tIOB5vqXh+pSFx5w/OlQuH+yHaE7/+/Fs+z5nGvuvt/cLApZPNEiF+1qK54fOZDZ4LrXti6xr/cMgy0Lw/tfusbGNpy2BBps/OONn1gBmHmB7Vgd/jAc7gmljNfFEPx5f7dWrXPdNxdF1tEYew1zh0BaaPBYE9XuaGFwzlOzE5a/DL/MgcbQtVj9lp2/5FirbWUS1lWGrKtWLPVOwNd+kYOX8f1zeKhpFOunP8TgwcNsMNhhextx62DUP23Nf1ov6x5NRY/mS1ZC75Hh93P/aF8/+UvkTe/GbYYXPQ59xkD/IyKFw32D1ldCZl/rz4XWR54jz4CjvwePnxJZF4/Xvm4LZbc4sQuI7apqqoVnZ0SKxKZPYdkLLctGZ0+tfjvy/fo5NkZSvimcttfvcNi80F5LX1K4rd5kO2J0foxL/mU7iY5MHPv78VE7pGXQ3e2Pr9xsLapoqrfDi1eHr7VDXTl89pDddoQp2npw/18enmpfJ3wTktLtds1OeOTE2PUvesx2gAAn/RJ6DLQdDcDCp+DkX4etQqeeEB4MODgj+/vGQ05/+Pa7UFls9z17IWT3h2s/sp1wSjak9w7VrZ0n/VVshnd+AdNugX4TIkUJ7D3lvh6r3wxv5w4Od8rRkyprdoRjaA7blobFIXqg4uD4+SUkBIHG8PUDmPStlm6j3SVWbLILITU0eIyOEdTusgOvhhr4+L7wda7dGTngAdgREj7nfq3Z0bKew062fUt9Jbx3h3Ul3l4erncnktAWwaGF2YAw13cUZOzBRDV3hod71BgLdwDOfYODNX23uQTHGSm996vWv89xFa34N7x2Q/gGdp/HPULbvjzSvdSaEPhTIW9E+L3zY6kMPbPB8aU31sCsS0KfRf2gHdyxAwfHApp8dXjfxs8iy7hHVCPPsBbQ50+HfuwCI06HY6+3aXpfvm87/t2hH5DXZ11NjuW26VPrStq6KHYd28IEWu6rcj27YsGT9vrHKhMtAmBH8o47KyPUeUYHZmMFi7d8buMPxQtav9bGWNdVeIe1OsRrO+stn1vxWfYCpPawlojjzgo9srWZ2nI76qzeZgXC7Q6FkLCGrDFfCvhCMa/Fs2xMqjVWvGqF6e2f2ZH/4llRbQja++2En1vXizuu5O5sty2Jbjwsmhm5y5cSvneTs2Ddhy2t6MrQ51s+t9vRA5KNn9jOd8x54X1lq+1rzgBIy43dzqfPs66wew6x53REqmZXWNAdnHOunwMr34A3b2r5fdmhJI76qnA8qa14yT6Q0EJQkJNKz/QkFm2qgLRe4Q/yD237QOdH21Qf26fnpr2JQju/tPMV/n5iZOZMNOc/al+dm9gZWTpppq1ZBE+fZ38MOaHJcw3Vsd1O/jR7g9/oCp72GRt2G7n99Sv+bTN/VrYigu5AezT9j4Bv/MNuu0ej0YyISnMs3wBJadA/lE737IV2JO/8oJoawqZ4cnbr39sRHOvHjTunu7V2V20Hf3rL/S9cFb5nti2xKcnRMQdHoHe7RobzHrYdxSvfDf/fo6nYFBnwLl1tYz6n3GnFdOsS6yJa9QYMm26FdPlLtlxt1L357h22s3Zwxz8AsvqGR+W5Q8PurdVv2piUw/u/jVyPx/k/Z/aFh6bAB1EZXs4cip7DIL1XpBA7lmT/I2P/XwAGHRfe9iWHr9X2pfDU2fCOKzvrH+fCZ6HERRO08ZZot6eTEDHlRlqQfyhkFcSuh0O0pVBf0XKAsGVRONb47u02OzGaZiGohAz7VEW++qhluU4goYVARDhsQA8WbtxlOxmH6JFSNM6PtqI4cv/yl20Ay8GY9mdBPneZna8Q7ZMEmP5/4e0Rp8HwU8IjQ+fczvfXVVhRSO3RciJKsMn+CAHqq+2knGicEUtaTysKg46DgcfY0ebCp6xgpbg62CfPCG3soZmanGnNfWjpYnPj/Aii65jVN/x+6yJY9brdrq8MC9ygVrJ8YnF5jJF9LNwWQdnqcAaYm+pt4E9pud8hPc8GCxc+1bLzef/XNr7xnGvJrZWhgHt9VesWwXu/sm7Mwy6xdVr8jPXdjzgNeh4SKcrDT4ZDZ9jtdR+G/e+n32tfTcDGrxyiO6+meptB59Qp2iceaLJWxJy7XPcH4XRJf0rbabK9hkUOyBwKJ8Npd7fcD5A3CvJc7lWPL2zFxuLL9yNTTku+sO3MKoBLXw7vH3sB9B4Veezlr9n7L9a9uafUV1rruHBy2NqIJjs0x6lkZdj6ihaZTiKhhQBg0qAerC/bTVn1HkTmP/+HDcZFz1L91+XhVMONn8JdQ6y/vyNkD4h8P/3/rJns4E+1Qd+yNXYClzMqdXzXdeWQkmNv0liuGWdE8eaNka4iB2eOgwic8zc49fdwSMhP/p+fW8HJG9XyuJ4xRKUtkjPDVkw033gqvJ0ZI9vEOe7QC+31muzK96+vCoti4cSO16egg2Wjc78D9XBHBdy8CS55wVoCFcXWAjz2hpbHn3YPDGhnhcqnz4MNc62fv++EsFBWFoddiOO/CTeug+uXwCH/Y1OTa3ZY6y05dH0mfNP6+HtFzRIfeqIVB7D3weYiKJgEk6+CaT+DXi7XoC81nC5ZMMm+Oj7v3KGhwHiU66h0BfyuPy1wJlCuc01aPO2eqEJivzfdJQQjQ2KSlAF9W1l9Jn9cOH4EtoOtasV6cjNkGvQebbeXvWA7fSepAuD8R+xvYaRL0Aon21f3+bIHwJDoGFMHyRsRec5oHMvjzRvtAOKYH8CR/7t352qHhBeCIwZbf99/14bcKWm9YOx5bRwRYsmslj5/CLt3Zv/emt6xfMmxGBeVEphdaH/MDiIw8Fg7CnNP4KrZYX3lteWQmmNznNfGCJC2Z+W4MznGnGuD0MOn206grsIeP2Nmy068I8sS3OqyUJIz7V80186F0WeH3khYuJz3EA5Un/cwXL8Y+oUW/MrIB4xN0wUrYOc8CD9aAVNvbnkud8aOO/jtFpbW+Pb7dsR9QihYnJJlz5fZJ9RZm7Abzk2v4ZAU1e5Bx8EtxS3LHnWdvf4QFt9Vb9qO4dwHIL2nDQjnDAgnD/QZY/3jEO5A3PfP5a/Z+8PjgSP+N3w/OMdPuwlOusNue/yRbspJV7r+N0DhJDvwqK+yqZSO27K1+RYOziq/F85sea1zBliLwX2fOiN9J2EhFrmDI/+HmxdEphe3xtATYKorfdSfGh58eHzhgOyMmfCLHXDTV7Gtves+hrPvb7m/d+j/F8tybC4zqu3JnNGWR6zBUSeR8EIwoTCH/KwUHvvvV9T++Cu4YQkcdqn9x7fH/Edi7zcmvDRER/jRivBIzcG5Cdw3yuiz7ciox2DXuYLWOlj1uu3MWxthRAerHBeBQ2vLXqRk2dceg+1o7cKnIz8/oZ3lJfpEjdhaEwKnfjcsg5+sjnRDOdvuNng8MP4iOPdh24lF1DkHJlxkr8Xxt0T+GDP7wjWz7Y/30BmRGRjTfmZH25e+3LrLKCkdfl7S0n+ckR92m0SIWIh+E8Ijdud/3Xd8y2uRkm2zoZzRqjMCrNpiUxfdZLt81b3HhFMQnWBmhqvjSHXNlzntLvjpOjjxdnv9HBzrzp8Ko84M70/OCtczNdfeC/WVNjif0ad9n7mb5Cw45ER73b/umlPitM1tETj7nLTJ4TGec5XRJ/L+iqa10XpaL2txz3jGWnAn3GbrNvVmuDrK+vP6Iq+fczzY6+KLEoihJ8ApIbduwUSYelO4rpe74ks9h4Vd0sNPiXQFf39h2J3rEEchSOj0UbATy66eMoRfv/YFv3qnmDvPC615E/2PBzsadlYDbYsHjrG+0JwBLSemHTrDzjZ0OO0e22FFd+COf/C6T8KpZV6/7cQaa6wvOW+ktQ6cLCOvr/XFy6LrHd2pxHIXgasDyAm9uq7L94rCqXTRXPaK7TB6DIz6vqywv9ONY2nkxHAtJGXYEWi0mHm8MP7ClguppUQFi5MzoKbeBqlHnWk7ofyx1q/uJjXHfufQ48P+czfeJOgxKHb6ntOBZfe3MYrzH7XlkzPsDz4lO9xhjT3fdj5OHv6I08IT8JxA9/BTrGU35hybgbJrfcskBncHnN4zLObO/8TdqTr/PzfH/Sjyfe5Q6z469gcw4GibAWOCdjCQHBoQ9BoW+V3JWeFMKLBxkAFHh+c79BgUmcZ54dPh6zDmHFgV+j043+HU3ZsU3udYJxc8ZoPQWX3txMMls+z52loCZMBRsO6D8PuCibaDHX2W/V+PPN3+ORx/S+vf5ea6T8KusVj3s+PKOvq6sFCk5ER27um9wu7CqT+1mV4Ojihf95nNLgT7v4kTCS8EAFd9bTDry6r557xNXDdtKP1zQyrtTYocKd+8CX4V6gjPvt8GzxY+1TJF0UnnKzzCCoHHD7eVWUtBBM57yOYbe5PCZq+7Q/Umh0ccKVnhUTnY45PSrV+6ZIWd+7Bjjb2Jvv6kdR011lr3yf/1s99bu7NlnnK0b7+1CS2OEDidqzt1LndI666hIdPa/j6HpEw7smxtVOfxh0dN0ULgEO0LT86KfH/5v22n4YhANN9+33YWbhdEtJgAXPdp68FgJ5j8P7+yHeW4GM87cDosf1pkcsJFz9pZ169cF17BstchVkyd9u1abztVN44bpU9o8BJtEbjbE2tgE43XB5e61kzKKrTuHLdF0HNY5P9h7HnhzzL7wY9X2FiNIwQDjo4Ugj5Ri/Y5/3fnfu8xyF6fcx4IC5mzJENSOowIWQUn/dJ2rMNPCc//GXWmve9GnmknlG1bErl8ypQb4cjvWNHcVzJ6h4Uq2iJA7PW+IxSUdjLbAg2R97mIdSveus3uj7VmU++R1jrw+mOLeSehQhDie8cP47miYu57bw2lVfWcP7GQs66ZbVPonFx9j8dOulo3OzyazB1sp8jHWtis/5H2CWjOjeLuhKI7PrdrKLvAnqs9eo+CU++y9et/RLhznxHKrb6jwvpM/35CePKPg9u9BDD6nNjncEaoKTn21S1YTkeTkm3b+JPV1k/eVqaUM3q68i1rBZmgnTQTy1100wZ7zZ4M+f5bEwL3yBdaXrs+Y2wH3RqFE1sGmJ2OMzkrbGW1ZZoPnmLTJAdPab2M01E7loCb5vTeGFk1J91hhcA9cgUbvBz4NTg9FHiNtgjcuP3oHSUlCyqw/2fn/9PrkPDIdfqdYUv2rL+Gxd+dvnvK72znfvjlNrspuhN2fhPO/pQsuDUkqk7KcouOFmsVnBaa9e6kS/c7PBx7cCxcx/JNzQ3HdTobb1Q3Gj3YcO6laCFwcPZ5PHDM91vO9dnThIy9QIUgRH52CqeMyef5BTZ4V13fxPSrjyS5z2i7LIOTITT9t5EHDj0Bfr7dZoy8dG1knm9hKNvCF+OHH01yhu247xm+Z+lph19mUwGP/m7szwsmwoxnYcjUcGrg5Kuhp8s15IxcWqsXhK2SpHT7wz7y2nAZd8eTHzXiczj2hsgg3sCjw9utZUI4IyDHbdRaZyZiTe6UHPhWjAD+3pCSZa9b/yPh7tC1ak2IwMYXjrimpSi5cTpqJ0PLjeNCi+V67DMavhdjzkVqDlz5evh9MMoicLM3s1EveNxOyus9Ojyq7TnMJhEkpcEoVwD58EvD224hTs2BaaGAfVaUzxvCwhcrbbTHYBvHGNvO2kqDpthZ0kNPCO/LzLeWeUqOdSk5mU/x4ntFdl7AXw6394EbJ2Z07A3tC/LJv2n78zihQuBi2og8Xl28Ba9HWLBhF1c9UcTT3z4Szn+E8poGvtywk4kDW/GJZxdan6cjBFNvtiPR1B521N5RCiZB33YmtLnxJYctgNYYGZqc1Xe8zZk+/Z7w0hexUh3dOJNenEwbEZux4yYtt/2Fsf7nl/Zvb3DcKG09x+GGZbZubWWY7CnOdbvsVTtDuy28vvaDeY5IxBKLzNDI2p2ds7e4hfnM+9pegrwt8obD+aG1d3IGWjdd30OtkI05t+1jO4qz9k4si1CkZRwjFsNPhlu3R7rtzvu7XV4kd3B43ko8cVxQsQZV/tTwfseVesIv4l+nPUCFwMVJo/tw2rh8vjGpP1c8Pp+5a8sYdPPr9MpIbp5n8O6PpnJI71by4J1Uv/P+Dod+w253JPvIzUXP7F3lO8K33w9PSPH64Lad7a+k6vi12yqXf6hNJYwXvUdb91FbI/Jo87wzGTLV/u0rU260HWosN5zXBz/8ov0037Y444/w0R8j4w8Tr9j773MzZJp1/bW2vEI04mm5sF4sHIugtbklHSU6dpPey2aOHWiItG2BdxFiusPj6VxMmjTJFBXFmIXbySzaVM4597d8rF9BTirnH17Aks0VZKb4uXbqEMb0y6ahKYjfK0hTfduzS7sb7/0aPrrHuj6i0zT3F4FGu1bNyDPisuCWEgecgUGskb6bl6+z6wVd/QEUHB7/eiUwIrLAGBPTR6YWQSuML8zmkcsmccwhPZn56Ua2VdbRv0cqd/z7C/78fngZiX8vDs9inNA/h5w0P6eP68vXJ/VnaXEFs1eXUNAjlXMmFCDdsRNz3B1xzGFuF68/Mq9dOfBpTwAcnBnsKgJdiloEe4gxhj+9u4b73guvn37k4Fwykn2s2FrJlgqbPXTcsF58tCa8+Ftmso/7Lz6c1CQvCzbsYnd9Ey99vpm/XHQYo/pmsXRzBYcP6MGumgbSk3ykJnWir3tfCAbsImVjzutYJpOiKAckbVkEKgT7QEllHb2zwm6ghqYgH39Zxm2vLGfjTuv7nD6mD28v79jj9IbkpbOu1AbPLjpiAEuKyzl+RG++e/whFG3YSV5mMmu2VzMgN42GQJDJg3JpCgTxebWDVhSlbbpECETkMeAMoMQY00pOIYjIZOBT4EJjTDvPbzywhKA1yqrrWbq5gv490jikdwaPfLSOJcUVZKX6ePrTjST7PNxx1hjys1O48nGbFtg/N5W+2anMW9/OstUuZkzuz0ufb+YHJw6jR1oSFbWNjC3Ior4xSJLPQ01DEyPzsxjUKxxknbO6lDH9suiRloTH0w1dVYqi7BVdJQRTgGrgqdaEQES8wDtAHfDYwSIEbVHXaLNwUvzW9bNqWxVD89KbR/XGGJZuruCTL3dw8ph8Fm8q55YXl5Li97CrppFeGUkce0gvXlnUgRUWQ/TPTWVcQTZ5Gck8+YldmfSoIbmce5iNW2wtr2PK8F4cWpiDR4iIZQSCBq8KhqJ0e7rMNSQig4DX2hCCG4BGYHKo3EEvBHvD7vom0pK8FO+qpbBHKiLCAx9+ySMfreOdH01lS7mdgj+wZxpP/PcrHpz9JRMG5JDi82KAitpG1myvorLOzh0YmZ/Jym2x0z2PHJzL1BF55KQm8c4X21i6uYLzJxbSFDDccupIdUMpSjflgBQCESkAngFOAB6lDSEQkWuAawAGDBgwccOGGOvtJxjGGAJBE7NjNsbEzFAqrarH5xF6pCextqSaT9ftoKquidPG5XP+A5+Qkezlqx2tPzikV0YS5x1eyNFDelJaVc//jO5Dj/TwrOl1pdVkJPsi4iaKohwYHKhC8C/gD8aYT0XkCdQi6FLqmwIk+7x8/GUZzxcVs6umgTvOGsPm8lrSknw8Nnc9ry6OdEcleT0M7Z3BUUNyOaR3Bre+ZB+ekpeZTFaKj79dPJHsVD8lVXW8umgLVxw7iMIebU+xbwwE8Xmke6baKsoBzIEqBOsJP+ewF1ADXGOMebmt71Qh6Bqq65uY/9VOxhVks6S4nFS/j7eXb+O5ok3UNISXAe6bncLWihgL8IUYV5DN8SPyGNgznfRkL/O/2sXAnmkcNaQnK7ZWcv8Ha0n2ebnplJE8/t/13HfRYaT5vRGB7bLqenpltPHAD0VRWnBACkFUuSdQi6BbYozhzjdX8v7KEr4+sZBrpgxh4cZd9MpI5pVFW0jxe6hpCPDPeZvYVtm6QPi9QmMg9r1YkJPKk986ghVbK7nvvTWsLanmqCG5HDEol1PG9qUgJ5V1ZdUcNqADSy0rSoLSVVlDzwLTsKP97cDtgB/AGPNgVNknUCE4qDHG0BAI4vd4aAgE+ccnG3h2/kaG5mXwzhfbGZmfyf9OHcKw3pnMW7+Tv7y/hl01sZ+RkJuexM7dLRegG9Mvi4KcVL7asZu6xiBXfW0wY/plMXt1KYuLK7hu2lBy05N4fclWJvTPoaSqjrPGFzB3bRkfrirheyccws7dDby5dBs/Pnm4uqeUgwqdUKYc0NQ2BEj2eVrMa/iytJrd9U2k+L28tngL9YEglxw5kKxUP+fc/1/OPawAY2y56PjF3pCe5GV3yM111wWHMvOzjVTXNXLt1KGcOb4f1fVNfOfpBawpqeb0cX254aTh5GUms2lnDVc/VURGso+TRvfh6uOGtJlyu6O6ntz0JBUaZb+iQqAc9Nz60lJqGgLMmNyfmoYAQ/My+GhtKT3Tk8hOTeLut1eycGM5p47Np6quiaF56by5bBuHFmZz3LA8Jg7swe2vLmfBhvBDdVL9XmpD8z5iua4yk31cd/whPDNvA5t2hh/2npeZzClj8vnPF9vIz0rhf0b3obYxQG1DkL7ZKfz2jRUA3HbGaKaOyGNoXgYLN+5ie0Udp46za/bvqK5nTUk14wtzAFhftpvy2gaOGdrG8w4UpQ1UCJSEJ9bEOCdTymFtSTUn3TubwwbkcPq4vpx/eCFbKmqZv34nm3bVUrRhF9sqapk2vDdjCrJ4YeFmFm8qp3dmMscM7ckri7dw6th83li6rfk7+2Qls72yHo9AsJWf2uEDcli4sRyAzBQfJ4/O5+3l26iut/M+3II0pl8WWyvqGNIrnW99bTB+r4eJA3tQWdvIoF7pbNpZw6adNQzOS2dLeR090vx8sbWSo4b0pFdGMn9+bw1ThucxoX8OwaBh1fYqRvTJpLYxQHry/l2Dsq4x0DyxUok/KgSK0kGq6hrJSPZ12G2zfEsFfbNT6ZHmpyEQJNnnJRg0LCoup192Kj3S/Xy2bidDe2eQ4vPwxtKtpCX5GJGfye/eXInBUNsQYFjvTCpqG9mws4ay6noG9UzjwskD+N2bK5ufhdEeRw3JZeGGchoCLZ99OzI/kzPH9+Put1cBNs6Sk+ZvXtsK4JDeGUwelEuvjCR6Zyazrmw3m3bWUJCTytmHFbC7vonCHmn84uVl9AyV+WpHDeceVsDxI3rzrwWbeGvZNn588gg8AoN7pZOTZueZ7K5vYnN5LcP72FVJF2zYyYUPfcpFRwzgl2eNiXALbq2oxRjolxP5WMdV26oYkJt24CzI2M1QIVCUbsyyzRXUNARI8nkY1DONoq920RgI8trSrYzpl8Wu3Q1U1TVRtGEXY/plcdb4fizbXMkbS7fi80pzQN6xKsDOIC+pqmfzrloaAkFG9MmkX04KH6wqjTh3dqqfitpw0D4tyRuRLpyZ4qMqNGM9GkdsRvfNYtGmcop31fKDE4fx8uebmxdldLh26lC+dewg/vtlGT+ctZgkr4cHLjmczBQ/PTOSmPnpRh7773rG9MvC6xGOGdqLm08dyaadNXzy5Q5Sk7wU9EiloqYRj0fISfXz5rJtvLiwmJtOGcmakmp+fPJwdoaulfNwqe2Vdeyub2JI3j4+GAeoaWgKXaPYllVpVT3Jfg9ZKX4+37gLn8fDuMLsmGXjgQqBoiQ4u+ub+GBVCVV1TYwryGZMvyxEhGDQ8NrSrUwdlkd2mp+lxRW8tnQLkwfmsruhibMnFLCjup6fPr+E9Tt2s75sNz85eQTJPg8XTCwkM8XPg7O/ZPmWCq44ZjCLN5Xz7yVb+Naxg7lh1qLm87sD8Q7XTh3Kg7O/bFHXZJ+H+qaWVk00mck+qupji1B7nDSqD2tKqtheWUdjwJCXkUx1fRMCBIyhsEcqw3pnsmlXDdPH5LNyWxXz1u/g5lNHkuzz8tGaMrZV1HLauL4cWpjDc0WbeHTuerJT/ZwyJp+AMUwe1IOjhvRkV00jh/TOYOztbzOsdwYPXDKRk+6dDcCDlxxORrKfwXnppCd5yU710xgwJPk81DUGqKxt5LUlW7ns6IH7vLyLCoGiKJ1CeU1Ds7unPZZvqWDRpnLGF+YwoGcamck+ymsa8XqFjTtqGNMvizvfXMnJo/uwubyWN5du45zD+jFtRG9eXbSFn76wBICrvjaYR+euB+DQwmy+PrGQP767hp27GxiSl870Mfl8tKaULeV1XHHMIHY3NPHQ7HXN9ThicG7Eqr79slOanxty9oR+BIKGt5Zto8kVxBnVN4utFbWk+r1tTpDsbPKzUthWWYfXIwRc9RlfmE1Gio9vHTuYE0f12avvViFQFKXb0RgIUt8UJCPZR01DE7vrA+Rl2hnllXWNpPq9+FsZJe/c3UDxrhrSk30MzcvgzjdWgMD3jj+EFVur+MZDn/Dtrw3m52eMBuzM+ac++YrMUApw3+xwfGJpcQWby2s5ekhP5n+1k8q6Ruoag6Qne/liayXGQH1jgCc/2cB3pg1lWO8MGgNBGgOG37+1kt6ZyfTLSY14UBXA7WeOpkdaEh+uKuHldlYTTkvy0tAU5GenjeJbXxu8V9dThUBRFMXFvPU7ObQwu9OyloJBw4adNQx2PfsDbGZUktfOkQkGDe+tLGFIXjrPzd/EjdNHNLt7lhSXk+TzkJ7kY0lxBUcNySXF72Xhxl1MGphLwBgqahspiAqg7wkqBIqiKAlOW0Kgi8sriqIkOCoEiqIoCY4KgaIoSoKjQqAoipLgqBAoiqIkOCoEiqIoCY4KgaIoSoKjQqAoipLgdLsJZSJSCmzYy8N7AWXtljq40DYnBtrmxGBf2jzQGJMX64NuJwT7gogUtTaz7mBF25wYaJsTg3i1WV1DiqIoCY4KgaIoSoKTaELwcFdXoAvQNicG2ubEIC5tTqgYgaIoitKSRLMIFEVRlChUCBRFURKchBECETlFRFaJyFoRubmr69NZiMhjIlIiIstc+3JF5B0RWRN67eH67JbQNVglItO7ptb7hoj0F5EPRGSFiCwXketD+w/adotIiojME5HFoTb/MrT/oG0zgIh4ReRzEXkt9P6gbi+AiHwlIktFZJGIFIX2xbfdxpiD/g/wAl8CQ4AkYDEwuqvr1UltmwIcDixz7bsLuDm0fTPw+9D26FDbk4HBoWvi7eo27EWb+wKHh7YzgdWhth207QYEyAht+4HPgKMO5jaH2vEj4BngtdD7g7q9obZ8BfSK2hfXdieKRXAEsNYYs84Y0wD8Ezi7i+vUKRhj5gA7o3afDTwZ2n4SOMe1/5/GmHpjzHpgLfbadCuMMVuNMQtD21XACqCAg7jdxlIdeusP/RkO4jaLSCFwOvCIa/dB2952iGu7E0UICoBNrvfFoX0HK32MMVvBdppA79D+g+46iMgg4DDsCPmgbnfITbIIKAHeMcYc7G3+E/BTIOjadzC318EA/xGRBSJyTWhfXNvt24fKdickxr5EzJs9qK6DiGQALwA3GGMqRWI1zxaNsa/btdsYEwAmiEgO8JKIjG2jeLdus4icAZQYYxaIyLSOHBJjX7dpbxTHGmO2iEhv4B0RWdlG2U5pd6JYBMVAf9f7QmBLF9Vlf7BdRPoChF5LQvsPmusgIn6sCMw0xrwY2n3QtxvAGFMOfAicwsHb5mOBs0TkK6wr9wQReZqDt73NGGO2hF5LgJewrp64tjtRhGA+MExEBotIEjADeLWL6xRPXgUuD21fDrzi2j9DRJJFZDAwDJjXBfXbJ8QO/R8FVhhj7nV9dNC2W0TyQpYAIpIKnASs5CBtszHmFmNMoTFmEPb3+r4x5hIO0vY6iEi6iGQ628DJwDLi3e6ujpDvx0j8adjski+BW7u6Pp3YrmeBrUAjdnRwFdATeA9YE3rNdZW/NXQNVgGndnX997LNX8Oav0uARaG/0w7mdgOHAp+H2rwMuC20/6Bts6sd0whnDR3U7cVmNi4O/S13+qp4t1uXmFAURUlwEsU1pCiKorSCCoGiKEqCo0KgKIqS4KgQKIqiJDgqBIqiKAmOCoGi7EdEZJqzkqaiHCioECiKoiQ4KgSKEgMRuSS0/v8iEXkotOBbtYj8QUQWish7IpIXKjtBRD4VkSUi8pKzVryIHCIi74aeIbBQRIaGvj5DRJ4XkZUiMlPaWCRJUfYHKgSKEoWIjAIuxC7+NQEIABcD6cBCY8zhwGzg9tAhTwE3GWMOBZa69s8E7jfGjAeOwc4AB7ta6g3YteSHYNfVUZQuI1FWH1WUPeFEYCIwPzRYT8Uu8hUEZoXKPA28KCLZQI4xZnZo/5PAv0LrxRQYY14CMMbUAYS+b54xpjj0fhEwCJgb91YpSiuoEChKSwR40hhzS8ROkV9ElWtrfZa23D31ru0A+jtUuhh1DSlKS94DLgitB+88L3Yg9vdyQajMN4G5xpgKYJeIHBfafykw2xhTCRSLyDmh70gWkbT92QhF6Sg6ElGUKIwxX4jIz7FPifJgV3b9LrAbGCMiC4AKbBwB7LLAD4Y6+nXAlaH9lwIPicivQt/x9f3YDEXpMLr6qKJ0EBGpNsZkdHU9FKWzUdeQoihKgqMWgaIoSoKjFoGiKEqCo0KgKIqS4KgQKIqiJDgqBIqiKAmOCoGiKEqC8//40Mjs/Tk6awAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['loss', 'val_loss'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYgAAAEGCAYAAAB/+QKOAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/YYfK9AAAACXBIWXMAAAsTAAALEwEAmpwYAABm9UlEQVR4nO2dd5hU1dnAf2fKzu6yBViqIEVABaWI2Lsk1sRKrNGYmBiTWFKMJUVNTPKlJyYmlqiJvUQl9t67FAEBAamy1AUWdtk65Xx/nHvunHvnzuzsssPCcn7Ps8/O3HruvXPf97zlvEdIKbFYLBaLxU+oqxtgsVgslh0TqyAsFovFEohVEBaLxWIJxCoIi8VisQRiFYTFYrFYAol0dQM6kz59+shhw4Z1dTMsFotlp2HGjBkbpJR9g9Z1KwUxbNgwpk+f3tXNsFgslp0GIcSKbOusi8lisVgsgVgFYbFYLJZArIKwWCwWSyBWQVgsFoslEKsgLBaLxRKIVRAWi8ViCcQqCIvFYrEEYhWExWKxFIBUSvLotJW0JlKddsz19c28MHdNpx2vLayCsFgsuxZr5kBrY8FP89jMaq5+fA53v7us04558X+mc+n9M6lvjnfaMXNhFYTFYtmpqW1ozX/jps1w+xEw9dsdOldja4LmeJKVmxr50aOzaUkks25bXduk9mlJ5Dzm5sZWUqn8Jm6rrm102pH9vJ2JVRAWi2WnZOWmRr5173T2u+ll3lpUk99OccdyWPlhh8455voXOfL3r3PDU/N4fGY173y2Ieu2updfXhzNuk1dc5wJv3yZP7y0MK/zR8NKZG91lE6hZwS1CsJiseyUnHX7+7w8fx0A05dvcpe/u3gDNfUtwTtJJx4gOi761te3EA4JAOLJ7PGF+mYlxIujwedKJFM88MHnADw1azUADS0JXv10XdZjugqiWW03/LrnWLW5qf0XkSdWQVgslgyufmw2L8xd29XNyMqmhlbWbGl2v2sPjZSS8+/8kOP/+lbwjm6PW2zT+YsiSnS25AhAawti+opazrr9fZp8bqH/vLec372wQLXGac41j8/h4nums3xDQ+AxY5G0BfGKo0hue2NJxy+kDayCsFgsGfzv49W8uWh9VzcDgGnLN3GrTwge9JtXPN9TjuBvdXr0m7LFJVJOPMCRyCs3NfKLp+eRyGEJBFEUDlYQdc1xfjr1E+qb464F8eSs1Xy0bBNzqjd7tl29Oa3gtIJYsLbePY7adxVTP64GlMWh9Vp9c5yymCrGvXj91na1vT10q3LfFotl24knU7QmU2xu3D6ZMm3xldveB+BbRwwnEg6xdksz8aTX964tCLOXvq6umf4Vxd6DuQpCCfhrHp/De0s28qVxA9l/aO+c7TD9/VpBaCWgefDDz3ngw8/pUxZjw1avm+vXz33KhYcMY8r+g1m5qZFn5qx21wm8LqtTbnmXW8+fyJUPzwKgR1GE7zwwk6RzofXNCeqa1Lk3NmRxp3UC1oKwWLohUkre+WyDK1CC+PWz8znjn++yeH29x4+tM2S2NOWnIJrjSV5fuD5rJs6MFbVuUDVf1tU18+maOr5290fuMu1S+vjz2ozttfBuinsVxOn/fJe/vfpZesOkc01Olz3hKJqGlrazgprjaWshElb7b2mK88HSjez98+cZdu2zbtD6802NLKnxuonmVG/hqv/OJpWSnHPHB6w34iTagogbFslL89OxiEvum+F5lovXb2WaE3fZsLUdWVztxCoIi6Ub8uK8tXz1rg954MOsc8Hwr7eXMfPzzXzhz29x2G9fc5frXnguCyKVkqx1BPYLc9fy9X9P46f/mwtAayLFZQ/OZP7qOra2JDjz1vf47gMzWd2OYOqfX1rEhXd/xJtGdtLnm1QG0qbGTIGoXUxm+ueGrS18/Plm/vzyImNDfU2C5z9Zw0eOkF1f38LnGxv59n3Ts44xWLoh7crR96iuKc5V/53tKo93FisF8XYO5bylKZ4RWNYRkVbDMkrlyFC6/a2lLHXiFLWNre12keWLVRAWSzdkxgrVy/a7QPKhsVXt47cgpJRuT/0vryzi4P97lXV1ze52OtV0TvVmnpmzhp/97xMaHMvhrUU1HPrb13hy1iqenbOGZ+fkHg28tq45IxPpO/fPIG64vg4Y1stdpwW06WJasj7dg9+slUpStaeuJcl3Hpjprl9X18wdby/hxXnreHxGNXXNcfdapZS0JJKc/Ld33O1rHPfRf95b7o53MNHuJe2KMlkfkGG1tSWJlJJEKi3o8xwagZRw+UMfd+qIbU1BFYQQ4gQhxEIhxGIhxLU5tjtACJEUQkwxli0XQnwihJglhLDziFosefLivLV8/PlmACpKsufgZ6PRtSC8PfV/vrGE4dc9R0siyYvzVIZTTX2L6z7Swm1dnRKAPUuLMlxLH3++me89OJPvPTiTR6et5JRb3mHyn94AVGbOsGuf5b4PVgQGmeuaEyyp2UptQyulRWF+c/pYd51WRM2Gi2nRunr384Rfvsydby91LYiNDV7lt3ZLM4N7lQLwi2fmM+7Gl/jbq4u5/4MVDL/uOT7f6B15nY9bJxwSlBWrMG+/8hhfGN0fUOUy/GzY2sL1T87zuJienr06Y7tsvDhvrZtZ1ZkUTEEIIcLAP4ATgTHAuUKIMVm2+x3wYsBhjpFSTpBSTipUOy2WNrnzC/Dh7Z16yNqGVpbUtJ19sqRmKyf/7e2saY9Bx/32fTOY7lgQehTvfe8v5+L/TMu5rw6QagXR0Jok8fHDcMsBICV/eFEN5mpoSbrZonVNcVcJ6P2WOa6YXqVFbPVZMHs0zeHD2Hcpp5GrH5/DnOotLKlp4IqHPuZHj84C4I63lgQqiF9H7qLyxe/z0fJN9CotoqQo7K5raPW2AWCRL7vnV89+Su1WdR+lL83VTJnV17Z0w1aemKkyiPwpv1nHWRj0Ko3S2JpgoljEG9HL+dkXBgHwyLSVgdvf98EKNwurLUKkeLPo+yyNnc91kQc4edxuee3XXgppQRwILJZSLpVStgIPA6cGbHc58DiwY+TUWSx+qqfB81d36iEvuPtDJv/pzZy+40Qyxbfvm8G81XW8tiD9emxtSZBIpqg33CCapT5F0tCa5N73l/PzJ+fx6oL1nu0jIa+Q1K4P7WICCD/1HdiwCOJpN0pTPOn6x5dsaHB77/XNCZIp6bahNZly12kOXXkH/cVmfrZfE6WGgH9q9mpenKeCsqXRSIaC2LN/GedHXmXgsseZU72F4miI0qJ0EuaL89YxffkmT5B60dp6/Fx+v1KSfgXx6Zq6jJIYDS0JxuxWAcDTc7y9+Q1bWzh8ZB9e+sGR/N8ZYwmiRyxCczzFFZGplDaupv/mWQA8Y7jXjhjVh7evPoa9+pcDZGRnZaOcRoaG1hMSkm9HnuXXp++b137tpZAKYhBgqspqZ5mLEGIQcDpwW8D+EnhJCDFDCHFJtpMIIS4RQkwXQkyvqclzuL3F0ok8+OHnGS6Itpi7qg6A2dVbsm7zxMxVbo676efe94YXOev29xl740tc4aRBapb5FERjS4L73k8Hqt/6bAOvO4rCHwTVQWfTjy8jyu2Saq5zlzW1pi2In/9vLv+dXu2uq2uKs9IJJj89ezXvLdnoOcfmFiVyhlWG2d1x6fiprm30CHpQwtZkzZZmj4IBmHLb+65yi0VCGccAiBAck1m1uckNgmu2tiTQ+nvRukxrrzgaZs/+5Zx74JDAY+ogdS1lAJTEN3vW33zOBO6+6AB2713Kn88eH3iMfKnIUc5jWyikgggaquhXj38FrpFSBuWYHSalnIhyUX1PCHFk0EmklHdIKSdJKSf17dt3mxpssbSXptYkP5n6CWfdrnL1WxLJnKUSNDqt8cNlG7NuU725CSFg9MAK5q7ewmsL1rk+9plOjOHp2as9fvdlG7yCrKE1QXVtk2stfO3uj/j6f6bR0JrMCILqwGqDoSASjoKorU3XHGqOJ0kYO5uC+NUF690AOcAtry/2nGNji2pHnxJB7x5FGdccDQv3/KaF41cGja1Jd1SxyTIntfSksQMz1gFEUcdOOaLv2L378Z+vHwDAB0s3ebZtbE16rCk/Jb42+dFpv7Wy3DngBk/cpE9ZzC2dMcA/XsPgokOHeb4fOKw39198QM5zdxaFVBDVwO7G98GAP+oyCXhYCLEcmAL8UwhxGoCUcrXzfz0wFeWysljaxYwVtUy59b2cVTe3BZ1yubZO9b7/9NIiLr5nOh8t25SxbSolueCuDznx5rfdHrj2ZS9cW895//rAHUELqjdeFouwV/8yPlq2iW/8ZzqzVm7OOO76urQ/3J9Rs2Kj6o3v6bgwNEGpnDrjqckQiolwCQAvTE+nijbFk9QGpJoCKs8/h5ekRSpLoE9xim8dOdyz7qcnjeYf5010v+87qNL9XBL1WhAXHToMITL7oPr6b/zyPrz2o6O4/YL9OXREFT0cYR5xFIRu4r67VTBmoHIj6Weo2dqSyDk+oiRLjaU+ZUrxaSXajKMI69cyYfee7nam0gtSlgC/O3MsN3x5DF/Zf7C7LCklYwf0yNquzqSQCmIaMEoIMVwIUQScAzxlbiClHC6lHCalHAY8BnxXSvk/IUQPIUQ5gBCiB3AcMLeAbbUUgq018MJP3NTCruC6J+YwfUWtx/XSrpzxNqplmqWmb3xqnjuIa11dZqbKxoZW3v5sA5+uUe6a74SfoqJ2PgAX3zON95ZsZNm8j+DNPwCq3EJFcZSR/crcY6zZkplSubau2Q0w+1NTdemGvQZ4FYQehetdFmf95nrGzv0tVSjXVyMxAJ6fkVYQ81fXZaTPlhenBfjk0AxOD73tfh8hVnFr9C+cEPqIFqlcIRXhOMfu3Z+fnTza3e5bR+7B6IEVfCX8BkeGZvP33V7i6J4qOGwK0z/0f4UbT9nHc/4nvnsoACudctjFRSH26FvG8fsM4MFvHcysG44D0hZEOBTiu0eP4JKjRlBVFnOL72kuDL9Inw3T3XpHACEBy/7vJDdbqEdEwgvXQd0a7v3Gge49GNLbcctJyX5DelKCo8A3f05ZLMLxoWmcEnrPLZUBBCo7gLMPGIIQAnN1MiWN8RyFpWClNqSUCSHEZajspDBwt5RynhDiUmd9UNxB0x+Y6ty0CPCglPKFQrXVUiCe+xHMfxKGHwF7ndglTdC9uJDzhq3a3MRhv32NP0wZx1cm7Z5rV0Uqt+VhDib7z3vL3c/NAf5vsySCIMU10YdJLX2UrS2nuz3ffZ+fAolGOPz71DcnKC+OMLJfWrivMiyEPfr2YGlNA49OX8lZt7/PLeftlyG4dbB3VP8yz/K73lma0b5Hp6/kw+fu4faih7ghupwr4pdT0xymDyooqrntzSUUR0NceMgw7nhLHWdwr1JX8d1V9CcApjYfAcDRoVmcGJ7GiKItTGtR91y0qG11bEEL6EE9S/hD9A51otnwbxFmOPcpBedUxP7Klrsh9UcIKaUhBPQtU4ps5aYmwiGRMf5Au3J0DKK4KMLVJ+ztru9XHvNkMv0yeg8Aw5ofZN9BFfQpi/GDL+yJEIKqHkWs2dLM8NbPYNY/Ye0nHHnRMxy7dz+enLWaQ0ZUsUffMr568FD2HlCOePpBmAM01dIjFub2or8AsCr2s4xnAHDVcXtSFotwyoR0yPaHX9yLtxZtYG1ds6Mgtk+nq6C1mKSUzwHP+ZYFKgYp5UXG56XAtkVtLF2PLmsgCzPKMx+0H1gPItLpok/MXOUqCCkl7y3ZyKEjqjJ7ctL07zcwvI/XtM/maglK09xQn14WQ92bEClP2mMo4QipZJy6pjgVJV4LwhyBO7JvGUtrGnhshgoSX/bgx57zVfUoYqPTDn9A+FEjsKxZsLaeYSF1v4pIEBJQ0xJhdBjKRVpBrNnSzL6DKvjJSaOpbWjlvzOqGdSz2FUQfkpw2hBr4H0nSE2zV0HoeELI15MXMsmD3zqIg4ZXwZvGipY6KOnFyz84koqSqNt7X1vXTFksEtgjf/7KIxi0dC28DAN6eu9H/4pij4IwGdK7lH+ev7/7XVtpA/v1hflArUoC0Blbp04Y5HXpSadjEG/0BNt7ZIlhXHbsqIxlAyqLufNrk/jS399RnZ42Oi6dhR1JbSkcBZ7MJB+0BaEDqZGAOv6PTl/J+Xd+6Ek/dDFexGP++AZSShpaEq4CyKYggkbLmhaEdjvEibizhIGRxZFspa45QUVxlKFVaWFmxhh261lCSTR7oLSX4dfOKFoH3HTqPvQrj3mW6XONG1RBZUmURtR+pgUBUNVD7Xf8PgOA4LpNV5+wl7pWoa61pGUDR+2h/P1oC8IRksU5ruPQEX0yXEBawYzqX07/imJKiyKuG6osFtzvHT2wggrnloR980HkChL7f8Z6rMXeAxzFXa9Cq784dV/++JXxGfEeN0U43uQJrJtpugAvfP8I7r4o+5Av1woKie1mQVgFYenWaAtCp27qnqWpID5do/z0a4N6kL4X8Y1FNZz0t7c55o9vsGBtHbUNwb7gFQFpr+bgKt2rThChJZEiGvYKwJtfmse6umYqSiJEwyGeufxwwKsgyosj7N67JOM8x43pzyVH7sH4wT3d7YKE5l4DKpjsjO7VimJolRJuu1XGqCyJ0owTMxDe2Ecfx6UzeXQ/LjtmJNedNBo/vUqVNB5Q4kjYZAvDok6GkyPgtZAMykjKSUumtVLlBIf97jQPbjVX7+Ldeqr76B8bAmSMBj9wuKr6Oqgy5jnmoJ4lTDGCyS56Frt4o8ey8Y983ntABcfu3T9r00f1K+PbR+3BLeftl7bOC4xVEDsqa+bA2k86vv/yd2FTHpOl162Bxa/C+gWwakbHz5eTjk3OcstrnzHs2mdzViTNRjIlmfpxNdFEPceHprkWhHY1yUQrfPIYfPo0slkFZKWRhT3142qGXfssmxu8SuNfby1lxcZGtjTF+cWTnzC4+mnCpK2MGK18KfQ+r3y6jqeMUgkfLdvE9OW1nBJ6j++G/8fEkKow2iwjPPjh5xlZLA+9r0YT6/z2nuFWTgx96Am2l8UiDKhUgu2KyaOYNFTVJho7qJKfnDSa3j3UvpUlUUqKwhwSmscg0mOFRkfXctP+Dbxx1dEMrSqllGYOib+nVkrJ+voW98n5LQidqSOE4KqRq5jYUymQg0PzPc8AYECp4WLc6Mzr4MYg2rYgSASMWm7OVBDD+yjFkNGDXzMb3r0ZGjelBevaT+CDWyGunu+xe/dTp0pJBF6XqH/+57svOoB3rz2WEAGu01QS5jwKRk0l14KoXQ5LXk8vTyYyt9Vs/hyWGj61RCuhuY9x3bDFDC2Nd48YhGUbuF0F+Lgx+0CqnPznpPz2v3My1K1Kf+/o+QKoa45TgfLNdiQp7++vqRz6htZEuwcCvTB3LT94ZDZ/iv6LM4ve4fUNRwID3ODxVxofhscfAeC0HodzD9/11Nf5/QsqIrpmcwM9jeN+5gxcO/fAIdRPf4Qzi/7O0vBZ/CN5GgA/i9zPBZFXaAj1548vlvLlcQNZsLaes25/n2JaWFB8CwDVsg8ACZRg7FEUoTia7hVGRQJkupZS31e/z61Fz3Bsyx9ZKlVZhbLiCGWOgB3cs4S9B5YzfUUtpY61oHvFUkJJNMxDRb8G4KnTP6U4EqL8TuUCGnbjFiqKo/wmeifDV2kFkeKMiYPoOVdAAkrxKkptQSAl3Hc6VO4O/I6Hi37lbnOQ09Me0ycK+me12Rm016KsNp08kNOCaNgAlYO8ywIsiMNGVPHWohrXknB56eew7E0Ix7zZPy9cC332hJGTOWREFbFIiKP27Msb873xGf9o8LKYY5E1BMQBPvoXvHCNUgr7f00tixvK9b7T0p/f/Su8dpOam2LsFDz861hoqIEbNqso/Ls3w+vOvR39ZTjsB5nnLgDWgtjVMZVDJ7PUqTW0OI+aQ0Fov7P/Bc0H7SmocHq+iQ1L+dNLC3l3iRrw1SuVHqBW1qxSKd9bsoEbnpxLMiXdgGV9Y1owRki4bqJDR1QRciyOo3umUyGHCvV5ythe7pwAJ96sUj5LSfeEe6LuSavTRyuKhCgvjro2TJGTbVPhBF+jm5TCEkh6liqlURaLeBRncUQpC+0+0wqioTXhGdR1yvjdOM6JHWgqSqIME+YAP8lNp+7LkaOUkK8o8qayukJYC78tmfWFRvUvZ/lvT6ZXNAERn4/f6VX3dVxbp04Y5N89TTI/C+Jrhw7jh1/ckwsOHhp8nC0rM1OunaSAcEiw8Fcn8o3DhxP1jLaWnLCv916lVwVYtg3rvf/BU6bEg7ammjLnt6ChxruuyRhXU7dm509ztVh0KYdwqH39kGfmrGbP/uWERccVhC56tk4qt8vbM+dwb3KgGxRNyHSbmpLq89xVdcxdVcdZB6TTX+sMBVFMK1uJUBQJsdeAch5z7KIRxWkFWBxSvcreFWrdbGNgW4mhIMqEOm5cpn3w5bEItEoQ6Xx97XoSrcq1lCBMn7IYmxvjhEOCHx63J83xJCeNG8iKN9Q22koa5CiIxpZkzmA2KKUxcGUM9KXIlPKXO0H640dXcdQpk9nnBlVTs1+5I/ANQb3fkJ7BFdXiTVDWP2096GWo4Pns64+joiSHKApyMQVYEMXRMFdMzswAct0xW1ZCkc+WTXqTDPqUxTwKYvZ1R1BeXhHcLrMARCqp0m518NtUHtkUhL6GcIB1HC1VynfLSijtDVEj1lRUaoPUlm1gB8gegnQzMjJQ2uCyBz/muL+8RdgJ3OrcfiklbyxcnzH5u593F29we/paQQwQqiemyzjEDQXRnPK+Bmbd/y2NaeHUv0QpnV6lUXbvVerGHioSaWtk7EAlgHqXOwrCmIe4WGRmPJkWRJnRQ9dCylUQcSX8w6SocpaFhKBfeTF/PWc/ymIRzp40hD5lRZy+n+qNawuiNekLggf8Po7Zux/9yyKZ2ziCKCwTnhTN/Z14hyvkQlHu/UaWYgfxJij39cINt0tlaTTrQDEgSwyiHa5QHXfYUp0Z3PV9H9mvjAe/sV+6baGmjNRbFzPV1J2pTisIMwaRpU6XvoZQgILo0SfdZvAqiGgPqyAs24D549n8ecczHurWwMqPgoNoAPXroCXYffTC3DVuCmgi2/4BmNVG0xaEehHfW7KRi/49jXP/9YHRxtXQmn4B19c3c/6dH/KrZz8FoNXJwukvvKUvtramz1NBA1XR4PLN4Y3pWkIHDVYv6e7FrZSs/sDNRAo3pgO/JWF1rX3LixGkmPvByxwoPiVCgnu+mllxMyF0+zbTO5p+blpBVJUWqeQBR5iESDGwUvXe/YPBhlSVMv1nX2RoVQ9o2ECvkFIq3z5qD68ADnJpgFdxaAGnhaDjmhnRtweH7FGVdllpCyJSrCygIOKNUFzpdTNl61UHkc2CqF2h2tVSD1tzFINOGQpiwyLvumSm0h7TzxDGNQtVRYAAl5bHgljxLtSvxU3IkL4gdTQgCqeVa+3yzHe0R990m0FZFJrtaEFYF1N3xHyh/joWDvgmnPyn9h/nPyfDpiXwtWfUaGg/f9oTeu8BV3zsppPq3tal98/kLqdjlG8JY4AWY8IUbXloRaPLWHjqEf15NAw5FL7xPKDKRZiEnUyTAXiFYsroG+0dWsnL4auZGL/ZXTb3F8dz3i9v57S5P3GXDXWSY34Qvw3+8ybX73YIbEj37gH3Re8Zg0NC83mw6DcAXBv/Jn2LTwcgLsNEhWN99CiFFrhlzblsqkyXjyhygtT9mhbDg5M913P9l/dht54lHONk3gTyhxGIWAXLf+vEBkzhX7dKuS00UqpAqKdmpteC0EL2lR8e5TVAtJCLxLILrXiT6gHHKlyff7sURFAMYuMSuHkcHPQdWPAsbPk8e4KFjjtsXQcLnvGtCxjHYgrrZ38EGz9TLrKrfMrFVAL3nwF9R8M+pzvrZPp/vAkqB8MW35weWum89XsVYzDf0ZjzY6tzMuHCRuA9WrLdytdYC6I74u9xLX61Y8fZ5E1JDN5GlVqY+KuX3YqmflrzVBAzVtTy7uJ01VCtIC5/6GNeW7COWSvTAqA5nky/hJ+/5y43S1/EIiG+e9QwAAb4LIiE76ffO+ktFV8WizBUemtLjqlS/6OOxdA/EjCJjyMkQzLB6SPS5/jZwTFi0gl8k+6hpowXv/eWea6C1RZEz5BXkEZI0btHEVefsHfbrjvzuZnukLhvvIf+vZgCz3UxaQtC3VchhNflYioIfy9YHyPepHrAxYYvP5vbJYggC6J2ufq/+BWlHHKRK6AbZF2bSkPfk60BFXr9o5k3fpbpYpIpQCoLyo/5fBb55ktLOG3Q98l8NlFrQVi2BX+PK7SNjzmPYf2bG+PuLGZ+4r65cpfUbKW8OJIOdALzVm/hzFvf82xnCsAbn5rvGcS0cG09l9/3AW8FtEPTIxahrEi9sP2Ft22n7rc7+IaZ9CyNevaP4L3uQwcVMXpgD4akilTaZpDg0gInmeAr+1bCStgiS6lsXuP2mutlKb2F45rz+Z/1FWoFEQt5791Ro3rTJkFuP1Og+H8fyRaIFnufs+tiSnj/+2k2LQi/gkiBCCshpy0Itz1xda+CArR+Ei2ZcZOgGIS2hPzkcrEGWhDOsuLK4PXu+XzvhQinH6D//hUFDN7LFUfRz0grCPMacllrnYxVEDs62X70ufALrm1VEIHTdQTTHE+6I0+F46ZIJNX+//t4FRu2tvCrZz8lEhIs/o0aq5FMSU9gWGMqiNZEioZkit0qi1m9pZkHP/yczXVbwJc9aZa+6BELu0KvTDRTTiP1lFIeizCgZ6ZPeNb1x/HWohp37MH3jhwKRrgj3FrP81eeAP8KOwoi3ROPRUIqKKyFZCoOLY4A6LeP8iVrBYHhT87ybNxMGp/Qvfq4gCwdP2bqcnOd6rmbzzDhtyCcexZUM0t6LYgM3EycWKbbI5VQmT1BFgSo5XkpiObMtgXFHOJNyj/vJ6cFkcPFFKsI7gRo/EpLhDItCH2soOvMJeT1ebUrzryGVCpz347IiTywCmJHJ5XI7yUy6WwF0YYFYQaWr/rv7IyaRnFnLobvPzIr3URjdLSeqMZP0jcpTUNLgoP3qGL1lmYemb6S/qRf7lRKEgoJT02gkmjYIxh3ExtYKIcQi4ZVby+AI/dMTzo1so+3TpEr8PXLadznub84XnUeb3bWJeNKOEeKqRy0pxpB6/QG+/bpC5uWAxAWqcARuVccPoAjeo2BlM/vnU/P0RyTULdKCWZzv0SrV+BrheFP2zTPp3v8CAgbvydtQYQjmYI4lQBZlGlBREog0QSNG1XgOmL414MSGpKtmdcdFJdoqQtWEMmE6sG3BlhWuVxMRWXBwWm3rX4LwlAQ/hhOe/ErCFP5phKZx00lvc+lk7AxiEJTuwJurIRFL3Vsf1PYr5ymjtVWCY4MF1OWHPh/HZu5LOgFDepZGts1x9VnQYpbFh3DZeGpnk0TiewviZSSfzklo58tuo6nin5qHDf9Am5pipNISfbo24NTQ++wvPg8Lp2QFiy/f3GhJ3NqH7Gcl7acorKwHF6MXcvVkYfVqN1s98TE3yN2Rv+6y42eePTWg4nc+2XDgkgogRWrUAHK+jWusOnfNx1cDiN9A7MU+3x0LV9deHmwIGiLLasyP3tSMlu8QeJkgAWhf0P6/K0N8IeR8Pvh8N+L1O/wxsr02IZkPFPYJuPO71cqRaAD4zqF828T4HdDVTYcwEPnwR8DLKREc36Ctnp6ul2v/Tq9PBX3urc8bWyFfxwM//mSdxmoQLH5LuljP3CW+v/KDd5jecZB+LLASvNwDXra5Zx34XNw+1Fe5fvR7TD1297tC+Rysgqi0Kyarv7PfrBj+5sKYt4T6r9Zo6WtfSC7BRFUeynIHA8SSsZ29S3qsy5hfUXkCc+mz8yu9gwYM3nrsw3c+Y6qGbVPaAXjQun6UUHjHfbo04OLIypj6euj0gL6tjeXcOn9M1nnzK52VGiWWrH8bc/+3408RXE0lN9YEf+90L1J/fKa93nDQljxjhGDcCyIWDmU9AZkOtBpuFoiJDNiHS7L306/+Hs5pVPycfeZo251r9ljQfgUhFZ0ZudAX5t+9o0boXmzUnrzjA6AToBINAdYEMm0Uo1VwGHfh+N/A4d8L71NvDGtZBY+C40byCARYEEE8cl/05/f+n36czLudW+Z8YBkK9R86v2duC6mskx3HMBnTkC5ZoF3ueni8Y0jYfhRcPz/tX0NGvO3tWZW26nqVkHspGzroDWzB6MDVuagmSC2xcWUjGdORxkklAzf7WfOhO56pHDC8VyO6Kv8/CEkf37Z5ypxyDVKuime5PJjR3Ljl8e4y/pVFDO4wrmegJdi3uo6hlaVInP8tGORcH4vlP+l1P527bMPEh6mS6bF8f9r4aTz5A0BFfYrCP/vRbdhz+O9x8+F6RbxC3q9zMwicrOYktn3yzLexRXoidbgGIS+Z8UV0Hu4Ug6998je3iCSLXlaTplzXLjtMC2IgcZUM7lcTDrVNF9EKN1OV0E4xw9FYNzZ+R/L/w4nW1UcpzLLJFdWQeyimEJI9/qiAX5Wzz4dVxB/f+VTxt74EvNXp1/aeDyOlJJ/v7ssrTyMF+v8Oz8EzBLWzvy/TpA5RMpTx0eTTEk2Zok/AKSkKp9QagzAqiyJ0luHBow2nDhamfCfraunT1mMI4xYgp9YNJTfC5XNgnCfSYDyz7AgKtLCaes69eyMmJKQSbesRsYxw7G0wNGDzPIZdGimT/pdRRBgQWRRIuZ+iTbGLQRaEIl0po4poP0dnJYc2Ty6Le1VEOZ8D34LwpPRlSOLqaidCgJhKAhfFlM4mrtjl5E44Hsv6teojLdsrlGrIHZREsYP2FUQ2Sc3AfKPQQTwn7dVGer/zkgHOt9auI53Fm/gF0/P56zbP6CmvoV3F67O2FdPDBPXCsIp7xDOoiDqmuJuSYwjRvUJbE9JNOyZy6CyJJoWREbQ8fovqjr8iZSkV2mUQ0caCiLsDTYfNqJP8Avl7737X9IWn4spCH8Mwm9BREs8ClvIpDsNZgaRYkPAOPGWfFxMzXVpK8Uv6HX7PTEI7TJrzlyWr+BJtmb2xrUVBV4Bra+ltE+6vdkQYUdB5NGOrWvTn830YX8MIteYEDBcTO1UENLILtLPSZ8rFMksWGhi3gMpM39jm5apIHS2zt7OqCCEECcIIRYKIRYLIa7Nsd0BQoikEGJKe/fdIWmqhXtPg82ZFS5dZt4H/z4ZHrs4o3fkmfA+yILQzH4EnnHK/n76NDzx7cx9QPVepIRHvgr3nupVOj60u+Pf7y53l8lUkgGz/s4jRb/kzxu/y8/+7//48SPp+MVd0T+wv1hIsWNBVIl6Lg4/R7GeRhLJGav/zGNFNzI5NIObo7dwWugd3v77N3n2kzX0KYtx38UHBbanoiTqqQFUURxNv8Davw30+/xZbon+HYCepUV45qDwFWi76vi9gl+oB76iRuU+8BUV3DXvdygCi15Q83TkSn3Ux130gvJRxyrTwmnjZ0pIGBZE5eb5PB+7LvhYMpk+nhYuD54FU7+jBNv9U2Dd/Mz9WurTgeAg62DJa/DoBenvbsaM6XZqztwvG+W7BQeSn78Wpt+tPpsCWh+7v+M6bKmD564OPnakuO0gtQgQY/oe/+97SnBnsyDM2ODU78CnzxgupoCxC7lIJdMdhKTRUQDVSctVtDLZAh8/oOTGQ+dkJobULnMsiCwK4q7j2tfWPCmYghBChIF/ACcCY4BzhRBjsmz3O+DF9u67wzL/SVj6Orzx2+zbPHWZCmrOfSw9nB6V1XPQb4yRz8kAC0L7eqdekn4BH/kqzHlYffYrABFWP9hPn4alb3hLEfvQJSBMV0dpBEbNu5mDQgsYHfqcI0Jz1HwFDpPDH/O3olvYrTS9z8+j97tTQIZEigM2TGVSaBF3Ff2JU8Pv8deif3JK85MsqWnImPbSZN9BFe6cBwA9e0TT98RQEOEXruFL4fcBSc+SqFdo5JuDvvhlePg8+OwlePtPXoG575nqf/VHuRWEZslr6v+Ec73Caa8TMwbH9RJZ/PvJ1rTAiRj3aPaDavT44pfhuR9n7tdSBz2cTKmgFNYlr3nHSiRa1O/DY2XEM/cLYtzZsNcJal//fVn0vHoXwHsPhh4Oh14BZ96lnlNzncrM8XPCb1UKbLI1eztkMm2RjD4lvVwL0ln3q//ZLAiT2Q/CI+fnjkHsNjF4X/CmnyZ9iiKbYN/9YBh6mPr85HeV3Fj0QuZ2TZvV7zjbccxKuZ1IIS2IA4HFUsqlUspW4GHg1IDtLgcex1soON99d0z0y7l1Xe6RmBpDQTT4M3c8FoQSWNf8t42Z3/wWhNkTBUcgBPfItLsjbOTmh4S3N1MiWjNSM5MyxGBfh0vPoRsOmnnLoYxGd04AzT1GVdBR/coDLAitIDJdE1GSai5mM6MkqIfZpkkuvRbE+HPU/9YGAmMPQcQqYNjhyorQHPOT/PPVk0bgN+JTonUB82drmuugtCp9DGh7UJa/9IW/B+xi3NdwDM64A3oOUd+dkuQceEnmOUwBHY7AcTdBWT8lhGsDZj780l/g4O8YFoTxXlQYc0fEm1T7j/wxnH2fcQ5fpyCbBRGEvvag0c/nPEDWGRJTiXQ7/fc9qGIrwKgvwhE/yt0eAKRSDtmSXnK5r7aBQiqIQYDpY6l2lrkIIQYBpwO3tXdf4xiXCCGmCyGm19TUBG2y/dE9mq3r8uttGgObaht8CiUoBpHjB97UkuBfb/jS78yeKKg2ZQk6aheTmVnjH8cQo9Wd0MY9hQgzuo/vJXCuPZeCGCg2MbTKG3Q/as++nDFxEGfsN4hwSNDDN7m7KzQNCyLd/oSKU5gvcdCguLaKnSXjXqGpq3G2lXFjooWUKZxKemUXFkFoZe8XALpOVhAtW1SZiHDMcBXluF5/TALU70XKzP3KAgoE6hhPq/M8goKx2cYhxCqD3WT6mYWLMtNcywemP7fUo8ZZ+BSo/x6b1VTbVBDagghoc2lVZulyjUwaloNWEEYMApyUZ4NwNHtmkp9wNHhMErSduNJBCqkggtSsX/39FbhGygz7MZ991UIp75BSTpJSTurbN3vmynZF/zi2rs9TQaSzL8xSEUCgBRE0sEqzalM9NbW+rJBknOfmpM/xwaJV3PLinMD9owEKIulTECVkWhDD+1Zy/F6+F8pxY4gcPe5BYgPD+2SWvfjzWRP489kTAFxXlRus9ruYjEJoUZL0Ki3yvkhBvt+2hET9Gq/Q1EIvV+FCP7qjYPZmhWjfyHitpMxqnmAI1YB7q8trRIrTHYycFkRzcHXVVDLTHdMj4B3TwllbEBGfgoj2yG41FVeocQh+dGJFUAzCvH86USHsVxC+3ra5T94KIsCCiMTUwMcgZCq9r2uBaReTcz1l/X3tjGZOp5qNUA4F4Z8IqZMoZKmNasBUjYMBf+rLJOBhp1Z9H+AkIUQiz313XFwFsS57xktJ7/SAJo+C8I9GbYHl76g8dOcl9g+sen3heo5xPm9tasrs3cdbuP6J2ZzkdEKfeeFZelMPAXLq1PC7xJJxlsp0L8lvQZTQkqmkZJLKd30DgZzrurzoqaxemd3ExkAFgZTw3t8hXETvwZO4+PDhfGWS82Lql65FlbKg5xB3dHmEpJqSc4uhaP0WRM3CdLwmG1uqvcIwXKSETkcsCD/tsSDe+Yv677cgFj0fvP2K99WAtliF8t9v/lyVrC7umf0cidYsCiKeqSCCRgRrBfGaM2eyP8vOX4PJJJtloZ9ZpAjm/8/new/oP/rvT7LFm1Vl7t9WXOVt554HuZhAKYjqacHr9Lu/8FlVRcF1MTnnL+vnVYjhaP7CPaiciWYntCCmAaOEEMOFEEXAOcBT5gZSyuFSymFSymHAY8B3pZT/y2ffHRr3hymzWxA9Df3XmJ6RbHNjK6Y0XbamRs3L8NDZ6sVHCUE97zDA1/+d/rE2NDYTE94fUXNzsyeV8lfRf/PD6GOBzbo08gz/i11PxHALLVrrtUhKRKsRzHbYuDiz7LLzsvSXG8lGb+rYvXfAj3vdXHj55/DCNYg7J/PzL41h7wE+YdK0Rb0YZWllNnFwGaMHVviEg09BzLw3a3tcGjZ4XUyhsBJCHbEgAMafC1/8pbM8j36ZHkymBY7fhZKNaXeq/0MOUe1d+Kxa9vYfM7c9zhHo8Ya0gqgcko7Z+APX4B0bMOUu9V/33vVIcb+wyqYEAEYeq7Kg/Ohn1n+s+j/X93s96hrvd13PSc+pEG/yCtNwFEZ/WY3kPvkv0Hfv7G1qrVcpuH5XmVYYe53sjYOYmBb/oxd6x0GAir302TO9jVYc+1+knrl57479eToVGHJbEG0Nnu0gBVMQUsoEcBkqO+lT4FEp5TwhxKVCiEs7sm+h2trpmIHpoDx0UC/hyC9C39Gs2FDHD5xCdrUNrYQMBXH/G0bdJee4UZI0Zpl2s6GpmSL8VkgrEZE9DhBEn9K0UE20enuXlZE4+w/qHJP2tHF9GdE3oKeWz4QyTZu84wyAO786Xk3TmcxhQTiKlqpRKiYQRCrujf+Ewkrgt8uCMBTE6bfBYVdmLg/i9Nvh8B/4jmVYHaf83Win73cVb1JCddQXcp/na0/DoZcr4d5cl1aGp/0zXRJCZ+WYvW/tdvniL5XABW+xPcjszeeyII78MfzoU9h3ine5VlJf+kvmPkKoYP+X/5Z5zgO+qYK+8UZfJyEKZ9+vRnKP+gJ878P0uqASGOc+lKmUderwuK+o5xmE2SGMN2ZaEAPHw2WG9aGf65dvhis+hmuMbKS9vwRXzvJuu51dTAUdByGlfE5KuaeUcoSU8tfOstuklBl3V0p5kZTysVz77jR4FITTo3B+OM3xpKp+qqu0hiIsXF3L1I+Vv762Me4J6paLzIlVIiSyzsu8tbHJrYmUbk/cnT85X1664hD3cxleYT28MsSPJg9r1/GyMbJ3FiGWT/ZX61bvSGXIDBJCpgWx1UlmaGsiGbM3KLQF0Y65kDvqYgpHM4W7qeR6DU9/9gfqdeVUyJ3Zov3oxRXKKjJH6WsLJxlX7hjzOFoQ+YWvib83m8uC0PgzzfQzCxoUqrcNGnynz59KeDsZuay28v6ZyyoGZcY1dHYiZH+G5m8mVpFOhsg2WNV/HFPZVg7yPvdcWUw7oYtp18V8eXQQLdHCurpm9v75C9z/4efKtxsKQzjiKoR4MkXN1hZP+edyMnvSEZGksTXdcxTG9n98fm6mBZFqzV4QLghzBC/QA2/arIg3ITo6z7WfrPMM+MYGZHPVFVcGpzCax/VbEHrEbVsTyZjxo1BYvbwdtSBM2hrZHopmKhezF29mEvnbo6f3hMyePaTvhXaRxCq8FkS0JC20gjKotILwBI23wYLQ+O9JlnLsHkzFY55TC0vTHZhLKZuCX5+7fECABWHEpLKNRzB/p7HyttNccymuWLn3PDktCKsgdjxSSVXnH5RmX/KaM0ze6L3qCeITLVTXKmH/+IxqJZxCEZqSwhXemxpambe6jgkincJYTqYFcUxoFqElr7jfzXhBRCQzYhAiFW+fgkglVQzAoYfwKal4U9vVJfMlyFKonq4G9ZmYA7tM/BbEpmWwfoHPgvD9zHV56VxWSirhLcOgSyW0Z0BSW66kXPv59zWvwRQaddWw4j2YcY/6/cUb0wIyyIIoKlOZNFr4ZVgQJWnl9NlL6WWaaIAF4Vdm7YlBaLJZELkwp/GM+CwI8CrPXJljfvdMxSCnQ+BTEGWmgsjSvoSvU+F3Mflpy5o0zxOKWgtip+L138B9p6nMkQ9vg/tOV/XbPQpis/qfbEFPkBZPplzf7idrGlzhvXpzE5+uruOR2E3u7kEupnGhZQx74Wvud9PiiJIgZgSkZ8uRtDQ3e5RIm6TiamS2Q5nPglD+3TxcQPkQdJw7J6dHwGr0wDC/JVFc4RUUD50N/zwoMwaxzxnp7w012c8N6WCkOcGMCPvcGDmEsCabUGqrwm9bLqZwFHqPSH//94nw9BVw/5mqQ6IFZJCCqhrhHQ2cYUGUpoXWc1ep/+Y1jvyC+j/imPQy/3nak8Wk8SsI83onfNW7TldF9RQANIS8a0EY7sAgQbzXyd7tNdr95lcc5m8oqwVhdKaa69pWEEG/kbL+0H9f9dlfESCbBbHP6cHLtxGrILaFFc4cyqk4fO7MTdnqC47pgGiihSZnAhylIJLIUISkDBNxMoKenLWa1qT3BxBkQfgxYxYRksSIMz81lD2a72dx8b5ESWSNQXwvdH2bx+8hfAoi5RtE5m7YD4YdoT7n6hmZ69ySDm0ITe0u8FsSfgvCf1xQvbApd8NZTvaSO41mIvi8WuEEZTEB7PdVqBqpPufKHslmQbSVZhkuyu1iCkXhsulwfa23dLZMQeOmTOVlulBO+ZszGtgh0ILwCTNTQex+AFy/SY0QN9vr2d4fg6ikTfw9cvP7qbekB8dd/ApMvCDddo05liDIggjinAfUPfQrMK0gzFIbl8+EPY5Kf8+m/BMtMGh/FWBuMRVEnjEIgB8ugG8781MIkVYSoQiefPH++6r2X78pXQ6+k7EKYluod3q1kRKn1j8qTzzIgki0uIHl1kSKeKKVVXVx4oRdAf+f95aze0+vWVtuuHfiMvhHZrqPIiQpIk4LUQ4b1Y+xQ/oQJZHVxVTT2rYp3yMgDhI44brZk89VCdPsmflHnGZDv+z+mv/FFcHn8lsQQmT2CLNZEMUBAs2cLaznUCOIGmDa6+BmVgXRhjUX6GLyWRChkPrTcxvo7eMNmTEIT1qlr2S0a0EYCiIj6OyzCPzCLsPF1BkWhPFdiPQzNpWX+dzN0dVBMYigwXFCqHvo//0EDYTzW4pZLYhmdf+Ke3otiGwKJSgGoZ+t/1x+CyJS7Gybf7Xm9mIVRA4+39iYc74CN+87lUgHPv0xCG1BJFvdOkvxpGTz1kbe/KyWJCHPGIUj9vCmXZoZRHqeBT+mi2n/wWUMKg/RSoSK4ijRWDExkV1BtCbbnuh8j4qg0boBCsJ80XJVwjS3y6dWEKRfdr+CiJUHC+KgLCa/MM+WxRSkIEQ4HbSOVaRdIEEWhFZE2YRCW8qwLReTKZx0W01LQgtoLdT6GNN4+gVbTFsQjenguL/dbdX5yXAxdWIWk3tM59mZ9858nmabgyyIXBlr/gFxQQoil8I2SbSoe6wts22NQUD62fvHQRRo7IOJVRA5OPIPr3Pob53As7YEHKYtWJZ2QSRboXa5+pxKMHflhvSGWpAmmqmubSRMkmhiKxFSxAmTIOyJDwyqTP8QU1JQYcQg4lkUhOli+uUXBjCyd5RIUQlDq0opKlIvdz+xOXDfVLbCYwaxZIA7ySgw6FJckXbZ5BIKHgsiW1E4H02b1V+GgqgIflk9FoTzM8/3hcpmQehxEcXGOQMVhCNwtsmC8LuYfBaEi/P8zJHSWnBqS8ajIHyvfHGFirU0bkzv5xdmbd23DEXo+01tawwCjLhQg7FNlt+udnEZNc5yJlX4jxNUGymXy88k0ayeVcy5r1pJtScG4Uc/+3DEZ0HkOXhyG7AKog1aEilVQuB3Q9NKYuMSDnh4grtN/RvGgJ1UnE+rM0cOtzY38vsXFvKH6O28kbiAMEmShEkS9sQHdqswXozSXvQMtW1BeIrhPXQ2rJnN+OH9+cEX9yQWUwrilqK/Z+zX2Gc8R+4VkAOe0XjjpdQvv39kKziWgaMgspUp8K/Tgrwtv/zrzjNYN8/r4y6uCB7V2uqLH0CWTI8cMQiTUCSd9mlaEH5/O6Stp2wvf6+hwcs1QS4mU4iZvU7tYtJzK0BaoJf2UqPMPfn7PkGlUzdn3mMEt/2VUHuq/0EjnnV7TfRx+u3j7JeluJ2JDsq67fT91oceqv4HDWz0/9b0+T+6I72sIkvbgzCtMR3IbqsYoCbRou6fznh6+efO9tksiDxG1YdMC8L4vRaogqvn1AU/Q3fgQ6dWvRYQm5Z6Vqc2pcsVL1qzmWQ80y0VSTQiSHFG+B0AYsRJECJB2DPl5KCK9A8vFCsjKtM94USW0lkXHeLr8SRbiUSLiYZDFBdn6f0NO4LSbz7DVSekBctH/b4C48/L3NYcDzDyC9lHH0dKDAuik2MQmvXzoaRn+nu0B/TdE0671bud6QLL5Q7SHPfrdBmJrC6mAAsiKP9cl9rOZkHscTRc8kZa+PgJcjH512v2vwi+9TrsdVJ6mVaER/wIvv6c97r9AmnC+enP+pn5hd+ex8P5j8NFz2Rpj9HWrz2jFMKl76pr/NbrKmjbFhMvVNtr/BbE0dfCJW/CwHHe5VfOhit9hSf9z/n438CQg9tuA8CFT0GfkenvU+6C77yfeUzzPl4+E05zxv4mmtU68776tzfJx4JwK9taF9OOiS5jrB+Ozxwuat3sfn764889k+loQkJ6BpzFRIIkYRKEPBaEWeLCPyeuzDJ46DtHBvRInR5PcUmW/Oj++yhBZ1zLgYcc3faQ/XBUlQgJXFeE2yPPOwaRw8UU1EvbvNLbRv2S7LafdztTQeS0IBx69E1nwgRaEKF0im3MuG/RgPule+W5/Mu77ZfdagpyMZl45roQMGhi5khiUIq8akRuBVFUmk5d1a4g/7lLeqnyFFUjCMTcvr9jNQzYVwXJB+WYYMd/TeYzDMpq2m1C5n69hkGPKu8y/3MeNCm/NgAMPsB3rBKvdRbUvqoR3iKGoYjaT99XvSyIfGIQrgXhdzFZC2LHwhFidb7kl0jLZjfDqHpjPUUk2EpacNRJ9YL6R0XHHRdThBRn7DeIH3xhT4b3NkxZn5Dt3zNY6Iqg3rejICJFWX5E2mT2Bz/bGtwlZfaeS9gwgXO6mIIsiBwxCKMYH4km7/ldv7nvRWuvBREKp1++bNaPVhB5WxBtvPzZrjnIxdQWQSOJg74HdTJ0vEj/9wuzIIVpYra1PaXMcxE0yVO++J9zvhM0Qf5+/VwxIn3/zDjcNrmYjCwm0yVqLYiuI5UK8E2nkmxpjHPneys9i6MiSS1KqEREkihJ6km/lKulKvJVIRo8+yVlmLgMExZJhlb14MovjEKYQsMvZLMOvAoIeuoAZcYPXnjXmz9sEc7vBc/WE/dYEHm6mHR2SS4Xk79WjlkjR78k/h5nS4AFERQv0AiRbkO263OzmCpzKx0tUDusIKL5CQ6ToJHEQd+DAvracijOoiDaykLyKIgOjh73sy2pm7ncQZ11Xv8xg7LMivNQEPkoL08Wk41B7BDoAWtmCimv/5rWWw7inSW1GdvXSiXMIySJkqDOoyBUj3KYWOvZZ6/dejJyQE+iJAnrJ2EKDb+bRk/t6CeHBZFRcEy/wK4FYeZbh/J7wXNaEM79ymVBmG1a+wk8dQX8dd/M7XQwt8wX5PT0lrMEVk3cgUah7C+VCKcDmdmUm861j5UbFkSZd525f1vlSIIm3wH1DNrbgzavy3+NpsILElR+CyLXdJ1BmNu3Z66LXORTiykbGb37fNrUdjaf95iR7N/1b0PfT+Eb19Detul9QxFvQkY+wf9tpJATBu3UtCSUoKvA6PV/8l/6EjxD2maUoAiTIkqChlSRq361ghgi1nv2Gd6vkuYNmwmTTHcMTGFvxiBiFWqS998PJ4OgnqhWAH5hUdpbDfALVBABLiYRzvSV57Ig9IXkHGHs+9nNvCd4uwufgs/fT9e70nh6y1lcTCZmrzBaogKJZ90H/cbAv45Vy0UIpvwbFr/sLWNxyt/T00R+7Wmo/kid37QgzrxLzb+QaFaTEen0yrZKlp/8JzXyfPiRqvaVLm8SLlKuq9PvgKkB8zsHYT43f4+/rRpCWgG41lg7LQhPhlUn9Tk7c/BXPlbx5TNgfcDMdtnIpSD0s9D3NVdac15prtrFFIELpsKyN1UV3/2/nn97O4hVEFlodRREUC2koDmWa6US5lESFIkEqmi3IIR0XUzn7R0GYyrhaFGMplCECClcj5Y5oMe0ICZeGDybFwQHO7Vi8Ff0LB/oKAhnfYaLKaDMQtzrGstuQRguppzmb569tcpBMHYKrJqR2SZ/W4JetIrBqpid2RuNlqoifGNO8TUppATphPNg9az08r57w+4Hqs89d09P9KSFYrhItVFTNUIVzoO2p5stroT9nZpavQ3Fr5/J+LPzVxDmPfH3+M0xEkEWhGvt6Wtqo3z39mBbLAg/+biYqkZkD8IH4R87YZ5DP4u8Soy008VUMRDGn5NfGzsB62LKgnYxVQSUmegRIIs2OS6m0f1L6VcaolVGqHeC06scC2J41OuaCoUjSBEhQpKU7nlni0Hk+iElWjOXua4kn6DWrhB9Po/vNJxZmyioZHQ+QeqgOv6abAOcsuHvwQYVzQu6P1VOPrvfgghymXgGouXhU9e9wiDFpO9Pog0LIhvtvT/gjTX575cn6ylI8PqFne+aOtKejuK6A7ezBbGtmJaTWSm3Ldo1UG47XIf/1Nv9jDsJLU5hvSALoqwo84UJO6l2Z00cSM+YJE7EDVRrC8IzshPYo18lKaEGyrliOVsMIqeCaM5cphWDX8BVOAqi0Rnt7Xcx+d1n/hgG5Bek7gwLQuN/0cxj65cm6OXRBfWEX0EEjXMwq2Ya9yzbdWhXYJAC0fvEA55LoTAVRC7BlI8LqAsEkYt+VtuSxeSns+IiOc9hWhDOs8inxEheMYhI5jm2E1ZBZEFbECMrMt1JldFMl05jxBE6qQQRmSBOxA1cN1Csfvi+MhEiHEWG1EC5yYt/A49/y+uW8FgQOXpUeh5ik0gWC0IHYvWocI+LKZTpL22XBZGHYO0I/hfNbJPu3Qa9aDqWYArFaGnwi+svpx10LhOZQ0FoBdpZJdHzwZPZ1c65AbQw9lQN7SLcnP+dzYIwYxCdbEHo59IFirugvwQhxAnAzUAYuFNK+Vvf+lOBm4AUkAC+L6V8x1m3HKgHkkBCStmO0S7bjo5BjOsjwFf+pyKSqSBaI44wTyYoat1MnRzA/yUu5zvhp1kod1cZK7qgnyakXEwhIRm/biqsA4Ydll5vZtPoH+DZD6hJa178SXrd/P9lXoD2O/sF3H4XwobP4Igfqu8ZLibfcYKykbJaEFEVsH33Ztj9oMz1J/8JUimoD6jjlItcFoTGFChjz1Ivlc6CMq9x4oXBJb5NN4op9E3/vYlrQQS8tMOPhP0ugCOvCt63PZzwu7TVlwuzHUEuoa8/n54AyM+E81ScR7fXPNaX/pp3UzsF14LYRgVxyt/hqcvV5+2h8IIsiP77wqjj1aBBP998DeZPzU8RmqU2tjMFu3NCiDDwD+CLKBE7TQjxlJRyvrHZq8BTUkophBgHPArsbaw/RkppVL7bfmgFsWdPGaAgMrOGUuGY+lHHG4k117Caw1kuB3JN4hIuOnQYrBscqCD2HtQbPjeW6aJ/EGxBjP6S+m8qiCB0wbGg6R/NCdf9ZYX9FkRQbzuXBVE1Qs050JSZCszYryj3ziu/yN12P7liEBpTKJ75L/X/M2fWPfMl3M83+YwmWwwiW1mRXBZEpEjNYdAZHHxpftu1FScYemi6npGfWBmccXv6uymIJhU+U8ZDZ1kQEy9MK4jt0fM2FZpWECU94fxHg7cfvL/6ywfPQLntSyFdTAcCi6WUS6WUrcDDwKnmBlLKrVK63bkeBFZO6xp0mmtVJNOPLFsz4xIyXKwe5JaVCCSDh6kKmv0rYtx4yj7BJYRDYXr28AlbU0HkG4MIQp/PH0PIGOBj1t0Pk/EIMrYX2d1HbbmYtOBpb9AzHwsiiKDR4hnoYL1v5i5NtramdJC6kwaG7Uh0aQxCu7s6M4tpe7iYAhREZ+FmMXWvGMQgwIzKVjvLPAghThdCLACeBb5hrJLAS0KIGUKIrLl+QohLhBDThRDTa2pqOqnpaQsiltyaua45U0G0ElEPsFbNWXzoxAkA7iRBwTXmA0bMmgoiWtrxF6bMGX2cUYXSd5wMF5PPgggKamYLIJqCJSi43VHB408XzPdF0deeT29UZLEgsiFzuJh2drZn1pKfQkx+U8AJddLnCIhBdNqxu2cWU9CvLMNCkFJOlVLuDZyGikdoDpNSTgROBL4nhDgy6CRSyjuklJOklJP69s0yMrUj1K3mB5H/Ur72o4xViZZMBdFCkaMgVGXXsn7DAGiOOwI3qMZ8KJL50E0FIUK50zhzoQV7W72ZkF9B+B5RexSTKViDFIt7DR20IHRPsK35FDR5WRC6SVksiGzkymKydBz9rNoq/96uY24HhRcUg+i0Y3ddDKKQCqIaMKXiYCBrdFJK+RYwQgjRx/m+2vm/HpiKclltN/ovn8qVkamU1MzKWHfcnpl++VYiapCZ43svqxpIj6Iw13/ZqQQ5+ABVC0jP2QzqR+Xv3Zi+e5nKXmto+FHQf2xw40dM9p4jF/6e875neNf7S0oc9O10OeRjfqb+64qmfmFp+u+LytIvqv6frf0AR13jPc5uE9Nlo2VSXeOYU737VI2Ew640rkdbEHn8zP2KsrSPKhOdjVwxiI6y75kqwJ2NPY6GMadlX7/bRDjgm53TlrL+6eebD3udBHuesO3n/eIv1f9ssZ/2MPmGdOHEQuNxrxbIxdSeooOdRCHPOA0YJYQYDqwCzgE8kw0IIUYCS5wg9USgCNgohOgBhKSU9c7n44BfFrCtmeTIYR/bLwbeKSFokV53kQjHmPdL44UZvD/8bK1Kdf2LUxI5FPbVcIl6R1KnktnLH3ztKVXH6DZn8vjJ16v6/37a6j2ZPedYhcr8uXIO3DxOX4j6X7EbXDYtve2NTjG8o34Mtx6mSkX4e97XLIcbHffQT1Zlnnv0l2HsmfDKjd7lw46AY4wgfDgKl7wOb/8JVn6gFOcFT2Qe73L/iOsOWhAAVy8J3k6TK4upo0y5O/f6C5/Mvf6S13Ovbw9XLWrf9uc+1DnnnXCu+usMjvhhOluv0Jhxss4uotcds5iklAkhxGXAi6g017ullPOEEJc6628DzgQuFELEgSbgbEdZ9AemCiXcIsCDUsoXCtXWQIIGn7nrMkfItuCLJ2St3mi6YXz7lFZ5M51k0nAxBQg5c9/25r67xzCO684HUJS5PpclItsbsNVKSwb7a7PFOPTyfCcXalcMop3GdCEsCMvOiyf+1sm/iS7MYiqozSKlfA54zrfsNuPz74DfBey3FBhfyLa1hTQHrEVKvEohoAhbi/S5i/KZQSoU8fYK/AoilwUBmSOEO4JpYcQCFEQ+pQ/aqyD0OaUMNsezncv1T+cbgyj27heEDMhiyofunMVk2TY624IQXWdB2JHU2TAVhL+uUICCaJZR4wGK7H5vT+/cp1T8xfg8FkSAgvDUGOqgBRF0PO3rDMfysyDa7W4xlFKQgmjLgshXQeh73Z7BSPnSnbOYLNtGttH3HUXLki6IQVgFkYXGJiNTyS98A9xPLTrNFXILDU8wqyjTxWTSc2j2ILV/WWdW3NQ9oIkX5Nd7yWVBBPWmdHXUwQcE7zPq+ODz6CB1tgFffqIlKtipU35z0W4LwrqYdnlEKNg67fQ0V12LqZu5mHZmtm41Slz7hVzcUR5GMHfkwCqoyaOolrmuYhDUGYldpoK4bLp3PuGgH2K+LqbrqpUQz1cIRmIqwByrgCcuyWy3n1yVTa9eRkZ288jJ8OOlai7hOc5I032nwIm/V5+zlTUfekh6v3wIheGKjzPm9g6kveNMtAXRlXWLLF3LdQGJF1DALCarIHYItjTGaW1pUqF1yBS+OsPJSMW7+kvj4N48FITp8y/t4922pGf6c1k/59x5uphyTaeZa/rPbOhry6f0gVYQQdsEzdkMaSGve+KhcH6CP1/loGkzXbKjMYhkx/azdB+y/bYLNg7Cuph2CF6ct5YYRrqpX0HogLXxwGKxUsMUzPNBhkLZ4wj6GLmC1J4spgJN6pJP70UriI4Iy23ZtzNpdwwih1K07Np0eppr96zFtFMipeSBD1dQUWQEQjNcTFpB+AZXdaRue7ZUVVdB5Ehz9c+UVgjM+XCz4Qr5DghL7arpzNo7HaG9o21TO0i7LTsenR2XcjMJd1AFIYR4XAhxshBd3c0rPG9/toHZ1VsYUm7OIZDFxeQXDu1REPphu4JdeM+TYUEEBalztLG9BJX1NtuR65r6O+WMO6KkdABZz/7WVbRX0OsSzkU9Or8tlp0TXeK+UAPlduAYxK3A14G/CSH+C/xHSrmgcM3qOqZ+vIrKkii9zWec1cXUQQVxyZtpwThofzU5fe89YJMxelcfO18XUz6ZOlnb8waU7xa8TiuOXK6UKXfBmjntjw8AjDoOzn8cRhzT/n07k/b2fc68S40ezxZQt+x6nPco1CwoQJpr180ol9cZpZSvAK8IISqBc4GXhRArgX8B90sp4zkPsJMQT6Z4ef46Tho7gNA630A5z4ZNTopblonL28pX3m2CsU9YTU4PwRPp5ApSe1xM29Br2W2/7OvymTYxVu6d6Kg9CAGjvtCxfTuT9sYSiivyT7e17BqU9IQhB3f+cbswiynvbpMQogq4CPgm8DFqpriJwMsFaVkXMKd6M1tbEhy1Z7+2B8oFuSS2VdMHuWhyjQbeHgFSXX4jYHBgt6CjI6ktlu3Fjl7NVQjxBPA2UAp8WUp5ipTyESnl5UAW5/XOx4fLNgFwyIgqn4LwC26Z2+XTYQUREEfImea6HUxObUEETJLUrbDBZsuOyk4Qg7hFSvla0IrtPVd0Idm4tZU7YjfTe9FmSJouJjOvWaAURNAISq0gOihsghREUa4sJt9k84XAtSC6u4KwFoRlByXkVGnogomc8lUQo4UQM6WUmwGEEL2Ac6WU/yxYy7qAppYWjhMfwpMf+uaDNm5TURm01qd7nBc+Ba3OqGu39k8HNX3/feGgS9W8Bpo9joGjrk1nCpkIASf9Uc0NUShiu4iCyGfOCIulKxh/LvQc0iWnzldBfEtK+Q/9RUpZK4T4FtCtFES4wZiy1HQxmb3L4gqlIHSPfg9DOLsT1HTQ9ROJwYm+4rbFFXDMddn3OfBbHTtXvhR3dxeTjUFYdnD67qX+uoB834qQEGn7RggRRk3u060oaVqjPoRjzsQ9ASad7lEHuXwi26ggdkR2FQvCKgiLJYN834oXgUeFEJOFEMcCDwHbdwKf7UBZs6MgdE2koJoqukcdFNTU23dBWd6CUezMCNfdFUR758m2WHYB8pVk1wDfBr6DepNeAu4sVKO6iorWdepDrAK2rlMC31/aWxe+29UsiFwz7HUHuiAAaLHs6OQ7UC6FGk19a2Gb07WUxDerD+5kMAEWRC4Xk96+O7krIkWw18kw8cKubklhOPs+eO+W3NVwLZZdlHzHQYwSQjwmhJgvhFiq//LY7wQhxEIhxGIhxLUB608VQswRQswSQkwXQhye776FIJx0eslNm9X/oHK++biY8p3xbGfh3AdhrxO6uhWFYeQX4ML/2SwmiyWAfN+Kf6OshwRwDHAvcF+uHZxA9j+AE4ExwLlCiDG+zV4FxkspJwDfwHFb5blvpxNJOqOFm2rV/6CRzfkEqXWlT4vFYtmJyVdBlEgpXwWElHKFlPJG4Ng29jkQWCylXCqlbAUeBk41N5BSbpVS1zqgB+mpx9rctxBEUjq11WlG0MA1bUEEKQGrICwWSzciXwXR7JT6/kwIcZkQ4nSgXxv7DAJWGt+rnWUehBCnCyEWAM+irIi893X2v8RxT02vqakJ2iQvEskUMdniXRhoQeTI6tF1k6RVEBaLZecnXwXxfVQdpiuA/YGvAl9rY5+gtBCZsUDKqVLKvYHTgJvas6+z/x1SyklSykl9+/Zto0nZaYwnKcanIMY4Rsuo49PL9BSWOk5hErYWhMVi6T60mcXkxAPOklL+GNiKmhciH6qB3Y3vg4GAetYKKeVbQogRQog+7d23M2hsSVIiWr0Lhx4GN27xLqt0DJmkT5mAEaS2CsJisez8tGlBSCmTwP7mSOo8mQaMEkIMF0IUAecAT5kbCCFG6uMKISaiRmdvzGffzqYlkaTEb0EExSAqB2c/iBuDSHRewywWi6WLyHdE18fAk85scg16oZTyiWw7SCkTQojLUKOww8DdUsp5QohLnfW3AWcCFwoh4kATcLYTtA7ct/2Xlz/xZIpifBZEUAyifGD2g7gKopuluVosll2SfBVEb1TP3sxckkBWBQEgpXwOeM637Dbj8++A3/n3y7ZvIWlJpOgl8rAgctVkD1sXk8Vi6T7kO5I637jDTks8KSnJsCB8CqKtEdI2zdVisXQj8lIQQoh/E5yB9I2AzXdK4skUJbSQDBenR1Sb1sJVn6VrLP14CYGJVjYGYbFYuhH5upieMT4XA6dT4Kyi7U08nqBYxGmNVRFuXJu5QZkx7KNHn+CDWBeTxWLpRuTrYnrc/C6EeAh4pSAt6iISrarMRjJWCUEKIh90mW8bpLZYLN2AjlYoGwV0zRx4BSLlzJiWivXs+EG0C8q6mCwWSzcg3xhEPd4YxFrUHBHdBldBlPRWC/REOe1Bl4zuNbSTWmWxWCxdR74upvJCN6SrScVVimvzkKMoH3kI7LZf+w/SowrOvh+GHNrJrbNYLJbtT74WxOnAa1LKLc73nsDRUsr/Fa5p25d4QrmFQiWVcPB5HT/Q6C93UossFoula8k3BnGDVg4AUsrNwA0FaVEXkXIURCSSYyCcxWKx7ELkqyCCtutGEy9DMhkHIBzuVpdlsVgsHSZfBTFdCPFnp9rqHkKIvwAzCtmw7U0y4SiIqLUgLBaLBfJXEJcDrcAjwKOownrfK1SjuoJkUruYrAVhsVgskH8WUwNwbYHb0qXoGEQ4XNTFLbFYLJYdg7wsCCHEy07mkv7eSwjxYsFa1QWknBhEKBzu4pZYLBbLjkG+LqY+TuYSAFLKWtqek3qnQruY3NHQFovFsouTr4JICSHc0hpCiGFkmSN6ZyWZsArCYrFYTPKVhj8F3hFCvOl8PxK4pDBN6hqk42IiZF1MFovFAvkHqV8QQkxCKYVZwJOoTKZuQ8p1MVkFYbFYLJB/qY1vAlcCg1EK4mDgfbxTkAbtdwJwM2pe6TullL/1rT+fdNG/rcB3pJSznXXLgXogCSSklJPyuqIOIm0MwmKxWDzkG4O4EjgAWCGlPAbYD6jJtYMQIgz8AzgRGAOcK4QY49tsGXCUlHIccBNwh2/9MVLKCYVWDgApPU2oVRAWi8UC5K8gmqWUzQBCiJiUcgGwVxv7HAgsllIulVK2Ag8Dp5obSCnfczKiAD5AWShdghuDENbFZLFYLJC/gqh2xkH8D3hZCPEkbU85OghYaR7DWZaNi4Hnje8SeEkIMUMIkTUgLoS4RAgxXQgxvaYmp1GTE2ljEBaLxeIh3yD16c7HG4UQrwOVwAtt7CaCDhW4oRDHoBTE4cbiw6SUq4UQ/VBKaYGU8q2Att2B45qaNGlSh1NvpXUxWSwWi4d2S0Mp5ZttbwUoi2F34/tgAqwOIcQ44E7gRCnlRuM8q53/64UQU1EuqwwF0VmkUjZIbbFYLCYdnZM6H6YBo4QQw4UQRcA5wFPmBs7guyeAC6SUi4zlPYQQ5fozcBwwt4BtTc8jbV1MFovFAhRwTgcpZUIIcRnwIirN9W4p5TwhxKXO+tuA64Eq4J9CCEins/YHpjrLIsCDUsq2XFrbhrUgLBaLxUNBpaGU8jngOd+y24zP3wS+GbDfUmB8IduWQVLHIKwFYbFYLFBYF9POhbUgLBaLxYNVEA5uFpMdB2GxWCyAVRBprAVhsVgsHqyCcBDSKgiLxWIxsQpCk0oiERCyt8RisVjAKog0MklK2NthsVgsGisRASklIpVECuteslgsFo1VEEAyJQmTJGUzmCwWi8XFKgggnpRESCKtgrBYLBYXqyCA1mSKECmrICwWi8XAKgggnkwRsQrCYrFYPFgFgVIQYZJIW4fJYrFYXKyCAOIJSURYC8JisVhMrIIgHYOwo6gtFosljVUQQKK5gTPC74AdKGexWCwuViICfT74NQA9tq7o4pZYLBbLjoNVEEB0i1UMFovF4scqCCAlZVc3wWKxWHY4CqoghBAnCCEWCiEWCyGuDVh/vhBijvP3nhBifL77dirJloIe3mKxWHZGCqYghBBh4B/AicAY4FwhxBjfZsuAo6SU44CbgDvasW+nUdSwtlCHtlgslp2WQloQBwKLpZRLpZStwMPAqeYGUsr3pJS1ztcPgMH57ttpSElx45qCHNpisVh2ZgqpIAYBK43v1c6ybFwMPN/BfTuOlLxzxH2c3PJrll78aUFOYbFYLDsjhRwZJgKWBUaDhRDHoBTE4R3Y9xLgEoAhQ4a0v5WhEBsr92GeTBAqqWz//haLxdJNKaQFUQ3sbnwfDKz2bySEGAfcCZwqpdzYnn0BpJR3SCknSSkn9e3bt0MNTaaU7gmHgvSSxWKx7JoUUkFMA0YJIYYLIYqAc4CnzA2EEEOAJ4ALpJSL2rNvZ6LTXENWQVgsFotLwVxMUsqEEOIy4EUgDNwtpZwnhLjUWX8bcD1QBfxTCAGQcKyBwH0L1dZkSv0PC6sgLBaLRVPQ6nRSyueA53zLbjM+fxP4Zr77Foqka0Fsj7NZLBbLzoEViUDKiUFErIawWCwWFysRgYQOUlsXk8VisbhYBUHagrAGhMVisaSxIpF0DMKmuVosFksaqyBIj4MIWReTxWKxuFgFQdrFZC0Ii8ViSWMVBIaLyVoQFovF4mIVBMrFJIQdSW2xWCwmVkGgFIS1HiwWi8WLVRAoF5O1HiwWi8WLVRCoILW1ICwWi8WLVRCoYn02g8lisVi8WAWBKvdt9YPFYrF4sQoCSKRSRML2VlgsFouJlYooF5MdRW2xWCxeCjofxM5CKiWxBoTFsnMTj8eprq6mubm5q5uyQ1JcXMzgwYOJRqN572MVBCrN1WYxWSw7N9XV1ZSXlzNs2DCEfZ89SCnZuHEj1dXVDB8+PO/9bL8ZZUHYcRAWy85Nc3MzVVVVVjkEIISgqqqq3dZVQRWEEOIEIcRCIcRiIcS1Aev3FkK8L4RoEUJc5Vu3XAjxiRBilhBieiHbmZTSprlaLN0Aqxyy05F7UzAXkxAiDPwD+CJQDUwTQjwlpZxvbLYJuAI4LcthjpFSbihUGzXJlFUQFovF4qeQFsSBwGIp5VIpZSvwMHCquYGUcr2UchoQL2A72sTWYrJYLJZMCqkgBgErje/VzrJ8kcBLQogZQohLOrVlPqwFYbFYLJkUMospSOLKdux/mJRytRCiH/CyEGKBlPKtjJMo5XEJwJAhQzrUUDWS2ioIi6W78Iun5zF/dV2nHnPMbhXc8OV92tzutNNOY+XKlTQ3N3PllVdyySWX8MILL/CTn/yEZDJJnz59ePXVV9m6dSuXX34506dPRwjBDTfcwJlnntmpbd5WCqkgqoHdje+DgdX57iylXO38Xy+EmIpyWWUoCCnlHcAdAJMmTWqPAnKxFoTFYuks7r77bnr37k1TUxMHHHAAp556Kt/61rd46623GD58OJs2bQLgpptuorKykk8++QSA2trarmx2IIVUENOAUUKI4cAq4BzgvHx2FEL0AEJSynrn83HALwvV0KS0kwVZLN2JfHr6heJvf/sbU6dOBWDlypXccccdHHnkke74g969ewPwyiuv8PDDD7v79erVa/s3tg0KpiCklAkhxGXAi0AYuFtKOU8Icamz/jYhxABgOlABpIQQ3wfGAH2AqU5aVgR4UEr5QqHamkpJIlZBWCyWbeSNN97glVde4f3336e0tJSjjz6a8ePHs3DhwoxtpZQ7fFpuQUdSSymfA57zLbvN+LwW5XryUweML2TbTBKplM1islgs28yWLVvo1asXpaWlLFiwgA8++ICWlhbefPNNli1b5rqYevfuzXHHHcctt9zCX//6V0C5mHY0K8KOpAZSKQjZO2GxWLaRE044gUQiwbhx4/j5z3/OwQcfTN++fbnjjjs444wzGD9+PGeffTYAP/vZz6itrWXfffdl/PjxvP76613c+kxsLSbUSOqo1RAWi2UbicViPP/884HrTjzxRM/3srIy7rnnnu3RrA5jpSIqi8mmuVosFosXqyBQ4yBsmqvFYrF4sQoCW2rDYrFYgrAKAjtQzmKxWIKwCgKrICwWiyUIqyBQWUx2JLXFYrF4sQoCZ05qG4OwWCwWD1ZBYGeUs1gsXUNZWVlXNyEndqAczkhqa0FYLN2H56+FtZ907jEHjIUTf9u5x9zBsRYEKkhti/VZLJZt5ZprruGf//yn+/3GG2/kF7/4BZMnT2bixImMHTuWJ598Mq9jbd26Net+9957L+PGjWP8+PFccMEFAKxbt47TTz+d8ePHM378eN57771tvyApZbf523///WVH2P+ml+W1j8/p0L4Wi2XHYP78+V3dBDlz5kx55JFHut9Hjx4tV6xYIbds2SKllLKmpkaOGDFCplIpKaWUPXr0yHqseDweuN/cuXPlnnvuKWtqaqSUUm7cuFFKKeVZZ50l//KXv0gppUwkEnLz5s0Zxwy6R8B0mUWmWhcTeiR1V7fCYrHs7Oy3336sX7+e1atXU1NTQ69evRg4cCA/+MEPeOuttwiFQqxatYp169YxYMCAnMeSUvKTn/wkY7/XXnuNKVOm0KdPHyA9v8Rrr73GvffeC0A4HKaysnKbr8cqCOxIaovF0nlMmTKFxx57jLVr13LOOefwwAMPUFNTw4wZM4hGowwbNozm5uY2j5NtP7kd55Gw/WZUmqsdB2GxWDqDc845h4cffpjHHnuMKVOmsGXLFvr160c0GuX1119nxYoVeR0n236TJ0/m0UcfZePGjQDuFKaTJ0/m1ltvBSCZTFJXt+1zclsFgZPmai0Ii8XSCeyzzz7U19czaNAgBg4cyPnnn8/06dOZNGkSDzzwAHvvvXdex8m23z777MNPf/pTjjrqKMaPH88Pf/hDAG6++WZef/11xo4dy/7778+8efO2+VqEilF0DyZNmiSnT5/e7v1+8MgsjhjVhzMmBk1uZ7FYdgY+/fRTRo8e3dXN2KEJukdCiBlSyklB29sYBPCXsyd0dRMsFotlh6OgLiYhxAlCiIVCiMVCiGsD1u8thHhfCNEihLiqPftaLBZLd+CTTz5hwoQJnr+DDjqoq5sFFNCCEEKEgX8AXwSqgWlCiKeklPONzTYBVwCndWBfi8Vi8bA9M3w6i7FjxzJr1qyCn6cj4YRCWhAHAoullEullK3Aw8Cp5gZSyvVSymlAvL37WiwWi0lxcTEbN27skCDs7kgp2bhxI8XFxe3ar5AxiEHASuN7NZCv3ZT3vkKIS4BLAIYMGdL+Vloslm7B4MGDqa6upqampqubskNSXFzM4MHtS8QppIIIsvPyVe157yulvAO4A1QWU57Ht1gs3YxoNMrw4cO7uhndikK6mKqB3Y3vg4HV22Ffi8VisXQChVQQ04BRQojhQogi4Bzgqe2wr8VisVg6gYK5mKSUCSHEZcCLQBi4W0o5TwhxqbP+NiHEAGA6UAGkhBDfB8ZIKeuC9i1UWy0Wi8WSSbcaSS2EqAHyK3SSSR9gQyc2Z2fAXvOugb3mXYOOXvNQKWXfoBXdSkFsC0KI6dmGm3dX7DXvGthr3jUoxDXbYn0Wi8ViCcQqCIvFYrEEYhVEmju6ugFdgL3mXQN7zbsGnX7NNgZhsVgslkCsBWGxWCyWQKyCsFgsFksgu7yC6K7zTggh7hZCrBdCzDWW9RZCvCyE+Mz538tYd51zDxYKIY7vmlZvG0KI3YUQrwshPhVCzBNCXOks77bXLYQoFkJ8JISY7VzzL5zl3faaNUKIsBDiYyHEM873bn3NQojlQohPhBCzhBDTnWWFvWYp5S77hxqlvQTYAygCZqNGcnd52zrh2o4EJgJzjWW/B651Pl8L/M75PMa59hgw3Lkn4a6+hg5c80BgovO5HFjkXFu3vW5UYcsy53MU+BA4uDtfs3HtPwQeBJ5xvnfrawaWA318ywp6zbu6BdFt552QUr6FmpDJ5FTgHufzPaQnajoVeFhK2SKlXAYsRt2bnQop5Rop5Uzncz3wKap0fLe9bqnY6nyNOn+SbnzNAEKIwcDJwJ3G4m59zVko6DXv6goiaN6JQV3Ulu1BfynlGlDCFOjnLO9290EIMQzYD9Wj7tbX7bhaZgHrgZellN3+moG/AlcDKWNZd79mCbwkhJjhzIMDBb7mQs4HsTOwLXNWdCe61X0QQpQBjwPfl6rwY9ZNA5btdNctpUwCE4QQPYGpQoh9c2y+01+zEOJLwHop5QwhxNH57BKwbKe6ZofDpJSrhRD9gJeFEAtybNsp17yrWxC72rwT64QQAwGc/+ud5d3mPgghoijl8ICU8glncbe/bgAp5WbgDeAEuvc1HwacIoRYjnILHyuEuJ/ufc1IKVc7/9cDU1Euo4Je866uIHa1eSeeAr7mfP4a8KSx/BwhREwIMRwYBXzUBe3bJoQyFe4CPpVS/tlY1W2vWwjR17EcEEKUAF8AFtCNr1lKeZ2UcrCUchjqnX1NSvlVuvE1CyF6CCHK9WfgOGAuhb7mro7Md/UfcBIq22UJ8NOubk8nXtdDwBogjupNXAxUAa8Cnzn/exvb/9S5BwuBE7u6/R285sNRZvQcYJbzd1J3vm5gHPCxc81zgeud5d32mn3XfzTpLKZue82oTMvZzt88LasKfc221IbFYrFYAtnVXUwWi8ViyYJVEBaLxWIJxCoIi8VisQRiFYTFYrFYArEKwmKxWCyBWAVhsewACCGO1lVJLZYdBasgLBaLxRKIVRAWSzsQQnzVmX9hlhDidqdQ3lYhxJ+EEDOFEK8KIfo6204QQnwghJgjhJiqa/ULIUYKIV5x5nCYKYQY4Ry+TAjxmBBigRDiAZGjiJTFsj2wCsJiyRMhxGjgbFTRtAlAEjgf6AHMlFJOBN4EbnB2uRe4Rko5DvjEWP4A8A8p5XjgUNSId1DVZ7+PquW/B6rmkMXSZezq1VwtlvYwGdgfmOZ07ktQxdFSwCPONvcDTwghKoGeUso3neX3AP916ukMklJOBZBSNgM4x/tISlntfJ8FDAPeKfhVWSxZsArCYskfAdwjpbzOs1CIn/u2y1W/JpfbqMX4nMS+n5YuxrqYLJb8eRWY4tTj1/MBD0W9R1Ocbc4D3pFSbgFqhRBHOMsvAN6UUtYB1UKI05xjxIQQpdvzIiyWfLE9FIslT6SU84UQP0PN6hVCVcr9HtAA7COEmAFsQcUpQJVfvs1RAEuBrzvLLwBuF0L80jnGV7bjZVgseWOruVos24gQYquUsqyr22GxdDbWxWSxWCyWQKwFYbFYLJZArAVhsVgslkCsgrBYLBZLIFZBWCwWiyUQqyAsFovFEohVEBaLxWIJ5P8BnPriT95Qp00AAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(history.history['acc'])\n",
    "plt.plot(history.history['val_acc'])\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['acc', 'val_acc'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# weigh save\n",
    "model.save_weights('weights/prediction_model_weighs.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save model weights once\n",
    "model.save('model/prediction_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
